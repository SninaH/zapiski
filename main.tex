\documentclass[a4paper,12pt]{article}

% General document formatting
%\usepackage[margin=0.7in]{geometry}
\usepackage[parfill]{parskip}
\usepackage{url, hyperref}
\usepackage{color}
\usepackage[usestackEOL]{stackengine}[2013-10-15] % formatting Pascal
\usepackage[dvipsnames]{xcolor}

\usepackage{cancel}
\usepackage[export]{adjustbox}

% Related to math
\usepackage{amsmath,amssymb,amsfonts,amsthm}
\usepackage{mathtools}
\usepackage{youngtab} % \young diagram
\usepackage{tikz}

% encoding and language
\usepackage{lmodern}
\usepackage[slovene]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

% multiline comments
\usepackage{verbatim}

% enumerate with letters
\usepackage{enumitem}

% images
\usepackage{graphicx}
\graphicspath{ {./images/} }

% theorems
\theoremstyle{definition}
\newtheorem{counter}{Counter}[section] % not for use
\newtheorem{defn}[counter]{Definicija}
\newtheorem{lemma}[counter]{Lema}
\newtheorem{conseq}[counter]{Posledica}
\newtheorem{claim}[counter]{Trditev}
\newtheorem{theorem}[counter]{Izrek}
%%
\theoremstyle{remark}
\newtheorem*{ex}{Primer}
\newtheorem*{rem}{Opomba}
\newtheorem{rem*}[counter]{Opomba}
\newtheorem{ex*}[counter]{Primer}
\newtheorem{general}[counter]{Posplo"sitev}

% I like my squares DARK
\renewcommand\qedsymbol{$\blacksquare$}

% common commands redefined convenience purposes
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\ch}{\operatorname{char}}

% \cycle{1, 2, 3}
\ExplSyntaxOn
\NewDocumentCommand{\cycle}{ O{\;} m }{(\alec_cycle:nn { #1 } { #2 })}
\seq_new:N \l_alec_cycle_seq
\cs_new_protected:Npn \alec_cycle:nn #1 #2 {
	\seq_set_split:Nnn \l_alec_cycle_seq { , } { #2 }\seq_use:Nn \l_alec_cycle_seq { #1
}}
\ExplSyntaxOff

% Hack za Pascalov trikotnik
% https://newbedev.com/pascal-s-triangle-style
\def\x{\hspace{3ex}}    %BETWEEN TWO 1-DIGIT NUMBERS
\def\y{\hspace{2.45ex}}  %BETWEEN 1 AND 2 DIGIT NUMBERS
\def\z{\hspace{1.9ex}}    %BETWEEN TWO 2-DIGIT NUMBERS
\stackMath

\begin{document}

\title{Verjetnost in statistika - zapiski s predavanj prof. Drnovška}
\author{
	Toma"z Poljan"sek
}
\date{študijsko leto 2022/23}
\maketitle


\pagenumbering{roman}
\tableofcontents
\newpage
\pagenumbering{arabic}




% 7. predavanje: 18.11.

$\sigma$ velik: \\
$\sigma$ majhen: \\
Porazdelitvena funkcija:

\begin{align*}
    &F(X) = \frac{1}{\sigma \sqrt{2\pi}} \int_{-\infty}^{x} e^{-\frac{1}{2} (\frac{t-\mu}{\sigma})^2} dt = \\
    &u = \frac{t-\mu}{\sigma}, du = \frac{dt}{\sigma} \\
    &= \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\frac{x-\mu}{\sigma}} e^{-\frac{1}{2}u^2} du = \\
    &= \frac{1}{\sqrt{2\pi}} (\int_{-\infty}^{0} \cdots + \int_{0}^{\frac{x-\mu}{\sigma}} \cdots) = \\
    &= \frac{1}{2} + \Phi(\frac{x-\mu}{\sigma})
\end{align*}

Laplaceova integralaska formula pravi, da je $Bin(n,p) \approx N(np, \sqrt{npq})$ za velik $n:$

\begin{equation*}
    P_n(k) = \frac{1}{\sqrt{2\pi npq}} - \frac{1}{2} (\frac{k - np}{\sqrt{npq}})^2
\end{equation*}

\begin{ex}
    Sistoli"cni krvni tlak \\
    %slika%
    verjetnost, da ima slu"cajno oseba krvni tlak med $120$ in $130$ mmHg
\end{ex}

\subsubsection{Eksponentna porazdelitev}

\begin{align*}
    &Exp(\lambda), \lambda > 0 \\
    &p(x) = \begin{cases}
            \lambda e^{-\lambda x} \lambda \geq 0 \\
            0 \quad \text{sicer}
        \end{cases}
\end{align*}

\begin{equation*}
    F(x) = \begin{cases}
        1 - e^{-\lambda x} \text{ "ce } x \geq 0 \\
        0 \quad \text{ "ce } x \leq 0
    \end{cases}
\end{equation*}

\begin{ex}
    Radioaktivni razpad \\
    $F(x)$ je verjetnost, da se radioaktivni razpad zgodi pred trenutkom $x \in \R^{+}$
\end{ex}

\subsubsection{Porazdelitev gama}

\begin{align*}
    &\Gamma(b,c), \; b, c > 0 \\
    &p(x) = \begin{cases}
            \frac{c^b}{\Gamma(b)} x^{b-1} e^{-cx} x > 0 \\
            0 \quad \text{sicer}
        \end{cases}
\end{align*}

O"citno je $Exp(\lambda) = \Gamma(1, \lambda)$

\begin{equation*}
    \Gamma(y) = \int_{0}^{\infty} x^{y-1} e^{-x} dx
\end{equation*}

\begin{align*}
    &\int_{-\infty}^{\infty} p(x) dx = \frac{c^b}{\Gamma(b)} \int_{0}^{\infty} x^{b-1} e^{-cx} dx = \\
    &t = cx, dt = c dx \\
    &= \frac{c^b}{\Gamma(b)} \int_{0}^{\infty} (cx)^{b-1} e^{-cx} c dx = \\
    &= \frac{1}{\Gamma(b)} \cdot \Gamma(b) = 1
\end{align*}

- je porazdelitev

\subsubsection{Porazdelitev $\chi^2(n)$}

(hi-kvadrat), $n \in \N$, $n$ je "stevilo prostorskih stopenj

\begin{align*}
    &\chi^2(n) = \Gamma(\frac{n}{2}, \frac{1}{2}) \\
    &p(x) = \begin{cases}
        \frac{1}{2^{\frac{1}{2}} \Gamma(\frac{1}{2})} x^{\frac{n}{2} - 1} e^{-\frac{x}{2}} x > 0 \\
        0 \qquad \text{sicer}
    \end{cases}
\end{align*}

\subsubsection{Cauchyjeva porazdelitev}

\begin{equation*}
    p(x) = \frac{1}{\pi (1+x^2)} \; x \in \R
\end{equation*}

\begin{align*}
    &F(x) = \frac{1}{\pi} \int_{-\infty}^{\infty} \frac{dt}{1+t^2} = \frac{1}{\pi} \arctan t \vert_{-\infty}^x = \\
    &= \frac{1}{\pi} \arctan x - \frac{1}{\pi} \cdot \frac{\pi}{2} = \frac{1}{\pi} \arctan x + \frac{1}{2}
\end{align*}

\begin{ex}
    Slu"cajna spremenljivka, ki ni niti zvezno niti disktretno porazdeljena \\
    Vr"zemo kovanec, "ce pade grbc, postavimo $X=1$, "ce pade cifra, pa naj bo $X$ slu"cajno izbrano stevilo na
    $[0,2]$ \\
    Izra"cunamo porazdelitveno funkcijo:

    \begin{align*}
        &F(x) = P(X \leq x) = \stackrel{x \in [0,2]}{=} P(\text{grb}) \cdot P(X \leq x \mid \text{grb}) +
            P(\text{cifra}) \cdot P(X \leq x \mid \text{cifra})
    \end{align*}

    "Ce je $0 \leq x \leq 1$, potem je

    \begin{equation*}
        F(x) = \frac{1}{2} \cdot 0 + \frac{1}{2} \cdot \frac{x}{2} = \frac{x}{4}
    \end{equation*}

    "Ce je $1 \leq x \leq 2$, potem je

    \begin{equation*}
        F(x) = \frac{1}{2} \cdot 1 + \frac{1}{2} \cdot \frac{x}{2} = \frac{1}{2} + \frac{x}{4}
    \end{equation*}

    \begin{align*}
        F(x) = \begin{cases}
            0 \text{ "ce } x \leq 0 \\
            \frac{x}{4} \text{ "ce } 0 \leq x < 1 \\
            \frac{1}{2} + \frac{x}{4} \text{ "ce } 1 \leq x \leq 2 \\
            1 \text{ "ce } x \geq 2
        \end{cases}
    \end{align*}

    Ker $F$ ni zvezna funkcija, $X$ ni zvezno porazdeljena \\
    Ker $F$ ni odsekoma konstantna, $X$ ni diskretno porazdeljena
\end{ex}

\subsection{Slu"cajni vektorji}

\begin{defn}[Slu"cajni vektor]
    Naj bo $(\Omega, \Phi, P)$ verjetnostni prostor. Slu"cajni vektor je n-terica slu"cajnih spremenljivk
    $x = (x_1 \cdots x_n): \Omega \to \R^n$ z lastnostjo, da je mno"zica
    
    \begin{equation*}
        (X_1 \leq x_1 \cdots X_n \leq x_n) := \{\omega \in \Omega: X_1(\omega) \leq x_1 \cdots X_n(\omega) \leq x_n\}
    \end{equation*}

    dogodek za vse n-terice $x = (x_1 \cdots x_n)$, se pravi v $\Phi$ za $\forall x = (x_1 \cdots x_n) \in \R^n$
\end{defn}

\begin{defn}[Porazdelitvena funkcija]
    Porazdelitvena funkcija slu"cajnega vektorja $X = (X_1 \cdots X_n)$ je funkcija, definirana z

    \begin{equation*}
        F_X(x) = F_{(X_1 \cdots X_n)}(x_1 \cdots x_n) := P(X_1 \leq x_1 \cdots X_n \leq x_n)
    \end{equation*}

    Torej $F_X: \R^n \to \R$
\end{defn}

$F_X$ ima podobne lastnosti kot v primeru $n=1$ \\
O"citno je $0 \leq F_X(x) \leq 1$ za $\forall x \in \R^n$, glede na vsako spremenljivko je $F_X$ nara"s"cajo"ca
in z desne zvezna, velja "se:

\begin{equation*}
    \lim_{\substack{x_1 \to \infty \\ \vdots \\ x_n \to \infty}} F_{(X_1 \cdots X_n)}(x_1 \cdots x_n) = 1
\end{equation*}

\begin{defn}[Robna porazdelitev]
    "Ce po"sljemo v $\infty$ samo nekatere spremenljivke, dobimo porazdelitveno funkcijo slu"cajnega podvektorja, npr.

    \begin{equation*}
        \lim_{\substack{x_2 \to \infty \\ \vdots \\ x_n \to \infty}} F_{(X_1 \cdots X_n)}(x_1 \cdots x_n) = F_{X_1}(x_1)
    \end{equation*}

    ali pa

    \begin{equation*}
        \lim_{x_n \to \infty} F_{(X_1 \cdots X_n)}(x_1 \cdots x_n) =
            F_{X_1 \cdots X_{n-1}}(x_1 \cdots x_{n-1})
    \end{equation*}

    Takim porazdelitvam re"cemo robne (marginalne) porazdelitve
\end{defn}

Oglejmo si dvorazse"zni primer ($n=2$):

\begin{equation*}
    (X,Y): \Omega \to \R^2
\end{equation*}

za $\forall (x,y) \in \R^2$ je 

\begin{equation*}
    (X \leq x, Y \leq y) := \{\omega \in \Omega: X(\omega) \leq x, Y(\omega) \leq y\}
\end{equation*}

dogodek \\
Porazdelitvena funkcija $F_{(X,Y)}: \R^2 \to \R$ je definirana z

\begin{align*}
    &F_{(X,Y)}(x,y) := P(X \leq x, Y \leq y) \\
    &\lim_{x \to \infty} F_{(X,Y)}(x,y) = P(Y \leq y) = F_Y(y) \\
    &\lim_{y \to \infty} F_{(X,Y)}(x,y) = P(X \leq x) = F_X(x) \\
\end{align*}

Izrazimo $P(a < X \leq b, c < Y \leq d)$ s porazdelitveno fukncijo $F(X,Y) = F$. To bo posplo"sitev formule

\begin{equation*}
    P(a < X \leq b) = F_X(b) - F_X(a)
\end{equation*}

ki smo jo imeli v primeru $n=1$



% 8. predavanje: 25.11.

\begin{align*}
    &(X,Y): \Omega \to \R^2 \text{ slu"cajni vektor} \\
    &F_{(X,Y)}(x,y) = P(X \leq x, Y \leq y) = P((x,y) \in (-\infty, x] \times (-\infty, y])
\end{align*}

Izrazimo z $F_{(X,Y)} = F$ verjetnost $P(a < X < b, c < Y < d)$. To bo posplo"sitev formule
$P(a < X < b) = F_X(b) - F_X(a)$ \\
Najprej vzemimo posebni primer:

\begin{align*}
    &P(a < X \leq b, Y \leq d) = P((X \leq b, Y \leq d) \text{\textbackslash} (X \leq a, Y \leq d)) = \\
    &=P (X \leq b, Y \leq d) - P(X \leq a, Y \leq d) = F(b,d) - F(a,b)
\end{align*}

V splo"snem primeru pa imamo

\begin{align*}
    &P(a < X \leq b, c < Y \leq d) = P((a < X \leq b, Y \leq d) \text{\textbackslash}
        (a < X \leq b, Y \leq c)) = \\
    &= P(a < X \leq b, Y \leq d) - P(a < X \leq b, Y \leq c) = \\
    &\stackrel{\text{fiks. y}}{=} (F(b,d) - F(a,d)) - (F(b,c) - F(a,c))
\end{align*}

Torej je

\begin{equation*}
    P(a < X \leq b, c < Y \leq d) = F(b,d) - F(a,d) - F(b,c) + F(a,c)
\end{equation*}

Najpomembnej"sa razreda ve"crazse"znih porazdelitev sta

\subsubsection{Diskretne porazdelitve}

\begin{defn}
    Slu"cajni vektor $X = (X_1 \cdots X_n): \Omega \to \R^n$ je diskretno porazdeljen, "ce je njegova zaloga vrednosti
    kon"cna/"stevna mno"zica to"ck v $\R^n$. Omejimo se na $n=2: \Omega \to \R^2$. \\
    Naj bo $\{x_1, x_2 \cdots\}$ zaloga vrednosti slu"cajne spremenljivke $X$ in $\{y_1, y_2 \cdots\}$ zaloga
    vrednosti slu"cajne spremenljivke $Y$. Potem je zaloga vrednosti vektorja $(X,Y)$ vsebovana v
    $\{(x_i, y_i): i = 1,2 \cdots j = 1,2 \cdots \}$. \\
    Definiramo verjetnostno funkcijo $p_{ij} := P(X = x_i, Y = y_j) i = 1,2 \cdots j = 1,2 \cdots$ \\
    Ker je $\{(X = x_i, Y = y_j)\}_{ij}$ popoln sistem dogodkov, je $\sum_i \sum_j p_{ij} = 1$
\end{defn}

%tabelca
\begin{align*}
    &X: \begin{pmatrix}
            x_1 & x_2 & \cdots \\
            p_1 & p_2 & \cdots
        \end{pmatrix} \\
    &p_i = P(X = x_i) = P(\cup_j (X = x_i, Y = y_j)) = \sum_j P(X = x_i, Y = y_j) = \sum_j p_{ij} \; i = 1,2 \cdots \\
    &\text{"ce je } Y: \begin{pmatrix}
            y_1 & y_2 & \cdots \\
            q_1 & q_2 & \cdots
        \end{pmatrix} \text{, je } \\
    &q_j = P(Y = y_i) = P(\cup_i (X = x_i, Y = y_j)) = \sum_i P(X = x_i, Y = y_j) = \sum_i p_{ij} \; j = 1,2 \cdots
\end{align*}

\begin{ex}
    Met dveh kock: $X$ "stevilo pik na 1. kocki, $Y$ na 2.
\end{ex}

\subsubsection{Zvezne porazdelitve}

\begin{defn}
    Slu"cajni vektor $X = (X_1 \cdots X_n)$ je zvezno porazdeljen, "ce obstaja integrabilna funkcija $p_X: \R^n \to \R$,
    imenovana gostota porazdelitve, da je

    \begin{align*}
        &F_X(x) = F_{(X_1 \cdots X_n)}(x_1 \cdots x_n) = \int_{-\infty}^{x_1} dt_1 \int_{-\infty}^{x_2} dt_2 \cdots
            \int_{-\infty}^{x_n} p_X(t_1 \cdots t_n) dt_n \text{ za } \forall x = (x_1 \cdots x_n) \in \R^n
    \end{align*}
\end{defn}

Ker je $\lim_{\substack{x_1 \to \infty \\ \vdots \\ x_n \to \infty}} F_X(x_1 \cdots x_n) = 1$, je

\begin{equation*}
    \int \cdots_{\R^n} \int p_X(t_1 \cdots t_n) dt_1 \cdots dt_n = 1
\end{equation*}

Za vsako Borelovo mno"zico $A \subseteq \R^n$ (najmanj"sa $\sigma$-algebra z vsemi odprtimi pravokotniki) je

\begin{equation*}
    P(X \in A) \equiv P((x_1 \cdots x_n) \in A) = \int \cdots_{A} \int p_X(t_1 \cdots t_n) dt_1 \cdots dt_n
\end{equation*}

Omejimo se na $n=2: F_{(X,Y)}(x,y) = \int_{-\infty}^{x} du \int_{-\infty}^{y} p_{(X,Y)}(u,v) dv$ \\
Robni porazdelitvi sta:

\begin{align*}
    &F_X(x) = \lim_{y \to \infty} F_{(X,Y)}(x,y) = \text{ (brez utemeljevanja)} \\
    &= \int_{-\infty}^{x} du \int_{-\infty}^{\infty} p_{(X,Y)}(u,v) dv
\end{align*}

ki ima gostoto

\begin{align*}
    p_X(x) = \int_{-\infty}^{\infty} p_{(X,Y)}(x,y) dy
\end{align*}

in

\begin{align*}
    &F_Y(y) = \lim_{x \to \infty} F_{(X,Y)}(x,y) = \\
    &= \int_{-\infty}^{y} dv \int_{-\infty}^{\infty} p_{(X,Y)}(u,v) du
\end{align*}

ki ima gostoto

\begin{align*}
    p_Y(y) = \int_{-\infty}^{\infty} p_{(X,Y)}(x,y) dx
\end{align*}

(ekvivalentno vsoti v diskretnem primeru). \\
Najpomembnej"sa dvorazse"zna zvezna porazdelitev je normalna:

\begin{align*}
    &N(\mu_x, \mu_y, \sigma_x, \sigma_y, \rho), \mu_x, \mu_y \in \R, \sigma_x, \sigma_y > 0, \rho \in (-1,1) \\
    &p(x,y) = \frac{1}{2\pi \sigma_x \sigma_y \sqrt{1-\rho^2}} e^{-\frac{1}{2 (1-\rho^2)}
        ((\frac{x-\mu_x}{\sigma_x})^2 - 2 \rho \frac{x-\mu_x}{\sigma_x} \frac{y-\mu_y}{\sigma_y} +
        (\frac{y-\mu_y}{\sigma_y})^2)} \\
    &(\mu_x, \mu_y) \text{ premik, } (\sigma_x, \sigma_y) \text{ razteg} \\
    &N(0,0,1,1,\rho): p(x,y) = \frac{1}{2\pi \sqrt{1-\rho^2}} e^{-\frac{1}{2 (1-\rho^2)} (x^2 - 2 \rho x y + y^2)}
\end{align*}

Nivojnice, izohipse se: $x^2 - 2 \rho x y + y^2 = c$

\begin{itemize}
    \item $\rho = 0$: kro"znica
    \item $\rho \in (-1,1)$: elipsa
\end{itemize}

Robni porazdelitvi sta

\begin{equation*}
    p_X(x) = \int_{-\infty}^{\infty} p(x,y) dx = \cdots = \frac{1}{\sigma_x \sqrt{2\pi}}
    e^{-\frac{1}{2} (\frac{x-\mu_x}{\sigma_x})^2}
\end{equation*}

torej $X \sim N(\mu_x, \sigma_x)$. Podobno $Y \sim N(\mu_y, \sigma_y)$

\begin{ex}
    Krvni tlak, $X$ je sistoli"cni, $Y$ je diastoli"cni krvni tlak \\
    $\mu_x = 120, \mu_y = 75, \rho \doteq 0.7$
\end{ex}

Dvorazse"zna normalna porazdelitev je posebni primer ve"crazse"zne normalne porazdelitve $N(\mu, A)$, kjer je
$\mu = (\mu_1 \cdots \mu_n)^T$ in $A$ pozitivno definitna matrika. \\
Gostota v to"cki $x = (x_1 \cdots x_n)^T$ je

\begin{align*}
    &p(X) = \sqrt{\frac{det A}{(2\pi)^n}} e^{-\frac{1}{2} (x-\mu)^T A (x-\mu)} \\
    &(x-\mu)^T A (x-\mu) = \langle A(x-\mu), x-\mu \rangle
\end{align*}

Za dokaz enakosti

\begin{equation*}
    \int \cdots_{\R^n} \int p(x) dx_1 \cdots dx_n = 1
\end{equation*}

izra"cunajmo integral

\begin{equation*}
    \int \cdots_{\R^n} \int e^{-\frac{1}{2} (x-\mu)^T A (x-\mu)} dx_1 \cdots dx_n =
    \sqrt{\frac{(2\pi)^n}{det A}}
\end{equation*}



% 9. predavanje: 2.12.

$N(\mu, A), \mu \in \R^n, A \in \R^{m \times n}$ pozitivna definitna matrika, t.j. sebi adjungirana matrika,
za katero velja

\begin{equation*}
    x^T A x = \langle Ax, x \rangle > 0 \; \forall x \in \R^n \text{\textbackslash} \{0^n\}
\end{equation*}

V to"cki $x = (x_1 \cdots x_n)^T$ je

\begin{equation*}
    p(x) = \sqrt{\frac{detA}{(2\pi)^n}} \cdot e^{-\frac{1}{2} (x-\mu)^T A (x-\mu)}
\end{equation*}

Izra"cunajmo integral

\begin{align*}
    &\int \underbrace{\cdots}_{\R^n} \int e^{-\frac{1}{2} (x-\mu)^T A (x-\mu)} dx = \\
    &y = x - \mu \implies dy = dx \\
    &= \int \underbrace{\cdots}_{\R^n} \int e^{-\frac{1}{2} y^T A y} dy
\end{align*}

Ker je $A$ pozitivna definitna matrika, obstaja ortogonalna matrika $U$ in diagonalna matrika
$D = diag(\lambda_1 \cdots \lambda_n)$, da je $A = U^T D U$

\begin{align*}
    &= \int \underbrace{\cdots}_{\R^n} \int e^{-\frac{1}{2} y^T U^T D U y} dy = \\
    &z = U y, y = U^T z, dy = |det U^T| dz = dz \\
    &= \int \underbrace{\cdots}_{\R^n} \int e^{-\frac{1}{2} z^T D z} dz = \\
    &= \int \underbrace{\cdots}_{\R^n} \int e^{-\frac{1}{2} (\lambda_1 z_1^2 + \cdots + \lambda_n z_n^2)}
        dz_1 \cdots dz_n = \\
    &= \int_{\R} e^{-\frac{1}{2} \lambda_1 z_1^2} dz_1 \cdots \int_{\R} e^{-\frac{1}{2} \lambda_1 z_n^2} dz_n = \\
\end{align*}

Ker je $\int_{\R} e^{-\frac{1}{2} \lambda z^2} dz = \sqrt{\frac{2\pi}{\lambda}}$ - $z \in \R$ - s pomo"cjo
$\Gamma$ funkcije, Bronsterin, sledi iz

\begin{equation*}
    \frac{1}{\sqrt{2\pi}\sigma} = \int_{\R} e^{-\frac{1}{2} (\frac{x}{\sigma})^2} dx = 1
\end{equation*}

Gostota za $N(0, \sigma), \lambda := \frac{1}{\sigma^2}, \sigma = \frac{1}{\sqrt{\lambda}}$

\begin{align*}
    &= \sqrt{\frac{2\pi}{\lambda_1}} \cdot \cdots \cdot \sqrt{\frac{2\pi}{\lambda_1}} =
        \sqrt{\frac{(2\pi)^n}{det A}}
\end{align*}

Torej je $\int \underbrace{\cdots}_{\R^n} \int p(x) dx = 1$ \\
Dvoraz"se"zni primer je posebni primer

\begin{align*}
    &A = \frac{1}{1-\rho^2} \begin{bmatrix}
            \frac{1}{\sigma_x^2} & -\frac{\rho}{\sigma_x \sigma_y} \\
            -\frac{\rho}{\sigma_x \sigma_y} & \frac{q}{\sigma_y^2} \\
        \end{bmatrix}, \mu = \begin{bmatrix}
            \mu_x \\
            \mu_y
        \end{bmatrix} \\
    &det A = \frac{1}{1-\rho^2} (\frac{1}{\sigma_x^2 \sigma_y^2} - \frac{\rho^2}{\sigma_x^2 \sigma_y^2})
        \stackrel{\text{?}}{=} \frac{1}{\sigma_x^2 \sigma_y^2}
\end{align*}

$K = A^{-1} = \begin{bmatrix}
    \sigma_x^2 & \rho \sigma_x \sigma_y \\
    -\rho \sigma_x \sigma_y & \sigma_y^2
\end{bmatrix}$ kovarian"cna matrika (slu"cajnemu vektorju $X,Y$)

\subsection{Neovdisnost slu"cajnih spremenljivk}

\begin{defn}[Neodvisnost]
    Slu"cjane spremenljivke $x_1, x_2 \cdots x_n$ v slu"cjanem vektorju $x = (x_1 \cdots x_n)$ so neodvisne,
    "ce je

    \begin{equation*}
        F_X(x_1 \cdots x_n) = F_{X_1}(x_1) \cdots F_{X_n}(x_n) \text{ za } \forall x \in \R^n
    \end{equation*}
    
    oziroma

    \begin{equation*}
        P(X_1 \leq x_1, X_2 \leq x_2 \cdots X_n \leq x_n) = P(X_1 \leq x_1) \cdots P(X_n \leq x_n)
    \end{equation*}

    oziroma dogodki $(X_1 \leq x_1) \cdots (X_n \leq x_n)$ so neodvisni
\end{defn}

Oglejmo si dvorazse"zni diskretni primer

\begin{claim}
    Naj bo $(X,Y)$ diskretno porazdeljen vektor:

    \begin{equation*}
        p_{ij} = P(X = x_i, Y = y_j), p_i = P(X = x_i), q_j = P(Y = y_j)
    \end{equation*}

    Potem sta $X$ in $Y$ neodvisni $\iff p_{ij} = p_i \cdot q_j \; \forall i,j$
\end{claim}

\begin{proof}
    $F \equiv F_{(X,Y)}$ porazdelitvena funkcija vektorja $(x,y)$ \\
    $(\Rightarrow)$

    \begin{align*}
        &p_{ij} \stackrel{\text{def}}{=} P(X = x_i, Y = y_j) =
            \lim_{h \to 0} P(x_i - h < X \leq x_i, y_j - h < Y \leq y_j) = \\
        &= \lim_{h \to 0} (F_X(x_i) F_Y(y_j) - F_X(x_i - h) F_Y(y_j) -
            F_X(x_i) F_Y(y_j - h) - F_X(x_i - h) F_Y(y_j - h)) = \\
        &\stackrel{\text{neodv.}}{=} \lim_{h \to 0} (F_X(x_i) - F_X(x_i - h)) (F_Y(y_j) - F_Y(y_j - h)) = \\
        &= \lim_{h \to 0} P(x_i - h < X \leq x_i) \cdot P(y_j - h < Y \leq y_j) = \\
        &= \lim_{h \to 0} P(x_i - h < X \leq x_i) \cdot \lim_{h \to 0} P(y_j - h < Y \leq y_j) = \\
        &= P(X = x_i) \cdot P(Y = y_j) = p_i \cdot q_j
    \end{align*}

    $(\Leftarrow)$

    \begin{align*}
        &F_{(X,Y)}(x,y) = P(X \leq x, Y \leq y) = P(\cup_{i: x_i \leq x} \cup_{j: y_j \leq y} (X = x_i, Y = y_j)) = \\
        &\stackrel{\text{disjunktni}}{=} \sum_{i: x_i \leq x} \sum_{j: y_j \leq y} P(X = x_i, Y = y_j) = \\
        &\stackrel{\text{predpostavka}}{=} \sum_{i: x_i \leq x} \sum_{j: y_j \leq y} p_i q_j = \\
        &= (\sum_{i: x_i \leq x} p_i) (\sum_{j: y_j \leq y} q_j) = \\
        &= P(X \leq x_i) \cdot P(Y \leq y_j) = F_X(x) \cdot F_Y(y)
    \end{align*}
\end{proof}

Torej sta $X$ in $Y$ neodvisni slu"cajni spremenljivki
%tabelca

\begin{claim}
    Naj bo $(X,Y)$ zvezno porazdeljen slu"cajni vektore z gostoto $p(x,y)$. Potem sta $X$ in $Y$ neodvisni
    slu"cajni spremenljivki $\iff$ $p_{(X,Y)}(x,y) = p_X(x) \cdot p_Y(y)$ za (skoraj) vse $x,y \in \R$
\end{claim}

\begin{proof}
    (ideja): $X$ in $Y$ sta neodvisni, "ce $F_{(X,Y)}(x,y) = F_X(x) \cdot F_Y(y) \forall x, y \in \R$. "Ce parcialno
    odvajamo po $x$ in po $y$, dobimo $p_{(X,Y)}(x,y) = p_X(x) \cdot p_Y(y)$. Obratno dobimo z integriranjem po
    $x$ in po $y$
\end{proof}

\begin{ex}
    $(X,Y) \sim N(\mu_x, \mu_y, \sigma_x, \sigma_y, \rho)$. Tedaj je

    \begin{align*}
        &X \sim N(\mu_x, \sigma), Y \sim N(\mu_y, \sigma_y) \\
        &X \text{ in } Y \text{ sta neodvisni } \iff \rho = 0 \\
        &p_{(X,Y)}(x,y) = \frac{1}{2\pi \sigma_x \sigma_y} e^{-\frac{1}{2} ((\frac{x-\mu_x}{\sigma_x})^2 +
            (\frac{y-\mu_y}{\sigma_y})^2)} = \\
        &= \frac{1}{\sqrt{2\pi} \sigma_x} e^{-\frac{1}{2} (\frac{x-\mu_x}{\sigma_x})^2} +
            \frac{1}{\sqrt{2\pi} \sigma_y} e^{-\frac{1}{2} (\frac{y-\mu_y}{\sigma_y})^2} = p_X(x) \cdot p_Y(y)
    \end{align*}

    \begin{align*}
        &N(0,0,1,1,\rho): x^2 - 2\rho x y + y^2 = c \text{ - ovojnica} \\
        &\quad \rho = 0: x^2 + y^2 = c \text{ - kro"znica}
    \end{align*}
\end{ex}

\begin{claim}
    Naj bo $(X,Y)$ zvezno porazdeljen slu"cajni vektor. Potem sta $X$ in $Y$ neodvisni $\iff p_{(X,Y)}(x,y) =
    f(x) \cdot g(y)$ za neki integrabilni funkciji $f$ in $g$
\end{claim}

\begin{proof}
    ($\Rightarrow$) jasno na zadnji trditvi \\
    ($\Leftarrow$)

    \begin{align*}
        &p_X(x) = \int_{-\infty}^{\infty} p_{(X,Y)}(x,y) dy \stackrel{\text{predpostavka}}{=}
            f(x) \int_{-\infty}^{\infty} g(y) dy \text{ in } \\
        &p_Y(y) = \int_{-\infty}^{\infty} p_{(X,Y)}(x,y) dx \stackrel{\text{predpostavka}}{=}
            g(y) \int_{-\infty}^{\infty} f(x) dx \\
    \end{align*}

    Ker je $\iint\limits_{\R^2} p_{(X,Y)}(x,y) dx dy = 1$, je

    \begin{equation*}
        \int_{-\infty}^{\infty} f(x) dx \cdot \int_{-\infty}^{\infty} g(y) dy = 1 \text{ predpostavka}
    \end{equation*}

    Zato je $p_X(x) \cdot p_Y(y) = f(x) \cdot g(y) = p_{(X,Y)}(x,y)$, kar pomeni neodvisnost po prej"snji trditvi
\end{proof}

\begin{theorem}
    Slu"cajni spremenljivki $X$ in $Y$ sta neodvisni $\iff$ za vsaki Borelovi mno"zici $A, B \subseteq \R$ velja

    \begin{equation*}
        P(X \in A, Y \in B) = P(X \in A) \cdot P(Y \in B)
    \end{equation*}

    t.j. dogodka $(X \in A)$ in $(Y \in B)$ sta neodvisna \\
    (Borelova $\sigma$-algebra: najmanj"sa $\sigma$-algebra z odprtimi mno"zicami)
\end{theorem}

\begin{proof}
    ($\Leftarrow$)

    \begin{align*}
        &A = (-\infty, x], B = (-\infty, y] \\
        &P(X \leq x, Y \leq y) = P(X \in (-\infty, x], Y \in (-\infty, y]) = \\
        &= P(x \in (-\infty, x]) \cdot P(Y \in (-\infty, y]) = P(X \leq x) P(Y \leq y) \\
        &\implies F_{(X,Y)}(x,y) = F_X(x) \cdot F_Y(y)
    \end{align*}

    ($\Rightarrow$) izpustimo
\end{proof}

\subsection{Funkcije slu"cajnih spremenljivk in slu"cajnih vektorjev}

Naj bo $X: \Omega \to \R$ slu"cajna spremenljivka in $f: \R \to \R$ zvezna. Potem je $Y := f \circ X: \omega \to \R$
tudi slu"cajna spremenljivka.



% 10. predavanje: 9.12.

$f \circ X = f(X)$ \\
saj je mno"zica

\begin{align*}
    &(Y \leq y) \equiv \{\omega \in \Omega: f(X(\omega)) \leq y\} = \\
    &= \{\omega \in \Omega: f(X(\omega)) \in (-\infty, y]\} = \\
    &= \{\omega \in \Omega: X(\omega) \in f^{-1}((-\infty, y])\} = \\
    &= \{X \in f^{-1}((-\infty, y])\} 
\end{align*}

dogodek, ker je $f^{-1}((-\infty, y])$ zaprta mno"zica, torej Borelova. \\
$y$ je funkcija slu"cajne spremenljivke X. \\
Naj bo $f$ strogo nara"s"cajo"ca funkcija z zalogo vrednosti $(a,b)$, kjer je $-\infty \leq a < b \leq \infty$ \\
Vzemimo $y \in (a,b)$. Potem je

\begin{align*}
    &F_Y(y) \stackrel{\text{def}}{=} P(Y \leq y) = P(f \circ X \leq y) = \\
    &\text{f nara"s"cajo"ca} \to \text{obrnljiva} \\
    &= P(X \leq f^{-1}(y)) = F_X(f^{-1}(y))
\end{align*}

kjer je $f^{-1}: (a,b) \to \R$ inverzna funkcija k funkciji $f$ \\
"ce je $y \geq b$ je $F_Y(y) = 1$ \\
"ce je $y \leq a$ je $F_Y(y) = 0$ \\
"Ce je "se $f$ zvezno odvedljiva in $X$ zvezno porazdeljena slu"cajna spremenljivka, potem je $y$ tudi zvezno
porazdeljena z gostoto $\Phi$

\begin{equation*}
    \Phi_Y(y) = F_Y^{'}(y) = F_X^{'}(f^{-1}(y)) \cdot (f^{-1}(y))^{'}
\end{equation*}

za $y \in (a,b)$, "ce je $y \notin (a,b)$, je $p_Y(y) = 0$ \\
Podobno ravnamo v primeru, ko je f strogo padajo"ca ($(a,b)$ zaloga vrednosti)

\begin{align*}
    &F_Y(y) = P(Y \leq y) = P(f \circ X \leq y) = P(X \geq f^{-1}(y)) = \\
    &= 1 - P(X \leq f^{-1}(y)) = 1 - F_X(f^{-1}(y)-)
\end{align*}

\begin{ex}
    $X \sim N(0,1)$, $f(x) = kx + n, k \neq 0, n \in \R$ \\
    Vzemimo, da je $k > 0$. Definiramo $Y = f(X)$. Tedaj je

    \begin{equation*}
        p_Y(y) = p_X(\frac{y - n}{k}) \cdot \frac{1}{k}
    \end{equation*}

    po formuli (prej).
    \begin{align*}
        &y = kx + n \implies x = \frac{y-n}{k} = f^{-1}(y)
    \end{align*}

    To je enako

    \begin{align*}
        p_Y(y) = \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2} (\frac{y-n}{k})^2} \frac{1}{k}
    \end{align*}

    torej je $Y \sim N(n, k)$ \\
    "Ce je $k < 0$, potem je $p_Y(y) = p_X(\frac{y-n}{k}) \cdot \frac{1}{-k}$, torej za poljuben
    $k \in \R \textbackslash \{0\}$ je $Y \sim N(n, |k|)$
\end{ex}

\begin{ex}
    $X \sim N(0,1), f(x) = x^2$. Tedaj ima $Y = f(X) = X^2$ porazdelitveno funkcijo
    
    \begin{equation*}
        F_Y(y) = P(Y \leq y) = P(X^2 \leq y) = 0
    \end{equation*}

    za $y \leq 0$ in

    \begin{align*}
        &F_Y(y) = P(|X| \leq \sqrt{y}) = P(-\sqrt{y} \leq X \leq \sqrt{y}) = F_X(\sqrt{y}) - F_X(-\sqrt{y}) = \\
        &= \frac{1}{\sqrt{2\pi} \int_{-\sqrt{y}}^{\sqrt{y}}} e^{-\frac{x^2}{2}} dx
            \stackrel{e^{-\frac{x^2}{2}}\text{ soda}}{under}\text{=} \frac{2}{\sqrt{2\pi}}
            \int_{0}^{\sqrt{y}} e^{-\frac{x^2}{2}} dx
    \end{align*}

    za $y \geq 0$ \\
    Gostota za $Y$ pa je
    
    \begin{align*}
        &p_Y(y) = F_Y^{'}(y) = \frac{2}{\sqrt{2\pi}} \cdot e^{-\frac{1}{2}(\sqrt{y})^2} \cdot \frac{1}{2\sqrt{y}} = \\
        &= \frac{1}{\sqrt{2\pi}} y^{-\frac{1}{2}} e^{-\frac{y}{2}}
    \end{align*}

    kar je $\chi^2(1)$, saj je

    \begin{equation*}
        \chi^2(n): p_X(x) = \frac{1}{2^{\frac{n}{2}} \gamma(\frac{n}{2})} x^{\frac{n}{2} - 1} e^{-\frac{x}{2}}
    \end{equation*}

    za $x > 0$, sicer $p_X(x) = 0$
\end{ex}

\begin{claim}
    "Ce sta $X$ in $Y$ neodvisni slu"cajni spremenljivki, $f$ in $g: \R \to \R$ zvezni funkciji, potem sta tudi
    $U = f(X)$ in $V = g(Y)$ neodvisni slu"cajni spremenljivki
\end{claim}

\begin{proof}
    \begin{align*}
        &F_{(U,V)}(u,v) = P(f(x) \leq u, g(y) \leq v) = P(X \in f^{-1}((-\infty, u]), Y \in g^{-1}((-\infty, v])) =\\
        &f^{-1}((-\infty, u]) \text{ in } g^{-1}((-\infty, v]) \text{ zaprti } \implies \text{ Borelovi} \\
        &\stackrel{\text{Borelov izrek}}{=} P(X \in f^{-1}((-\infty, u])) \cdot P(Y \in g^{-1}((-\infty, v])) = \\
        &= P(f(X) \leq u) \cdot P(g(Y) \leq v) = F_U(u) \cdot F_V(v) \; \forall u, v \in \R
    \end{align*}
\end{proof}

\begin{theorem}
    Naj bo $X = (X_1 \cdots X_n): \Omega \to \R^n$ slu"cajni vektor in $f = (f_1 \cdots f_m): \R^n \to \R^m$ zvezna
    preslikava. Potem je $Y = f \circ X \equiv f(X): \Omega \to \R^m$ tudi slu"cajni vektor (brez dokaza).
\end{theorem}

$Y$ je funkcija slu"cajnega vektorja $X$ in ima $m$ komponent $Y = (Y_1 \cdots Y_m)$. \\
Porazdelitvena funkcija za $Y_j, (j = 1, 2 \cdots m)$ je

\begin{equation*}
    F_{Y_j}(y) = P(f_i(x_1 \cdots x_n) \leq y) = P((x_1 \cdots x_n) \in f_j^{-1}((-\infty, y])) \text{ mno"zica v} \R^n
\end{equation*}

"Ce je $X = (X_1 \cdots X_n)$ zvezno porazdeljena, je torej

\begin{equation*}
    F_{Y_j}(y) = \int \underbrace{\cdots}_{f^{-1}((-\infty, y])} \int p_X(x_1 \cdots x_n) dx_1 \cdots dx_n
\end{equation*}

\begin{ex}
    $n = 2, m = 1$, $(x,y): \Omega \to \R^2$ zvezno porazdeljen

    \begin{align*}
        &F_Z(z) = P(Z \leq z) = P(f(x,y) \leq z) = P((X,Y) \in f^{-1}((-\infty, z])) = \\
        &= \iint\limits_{x + y \leq z} p_{(X,Y)}(x,y) dx dy = \int_{-\infty}^{\infty} dx
            \int_{-\infty}^{z-x} p_{(X,Y)}(x,y) dy
    \end{align*}

    od tod sledi, da je gostota slu"cajne spremenljivke $Z$

    \begin{equation*}
        p_Z(z) = F_Z^{'}(z) = \int_{-\infty}^{\infty} p_{(X,Y)}(x,z-x) dx
    \end{equation*}

    "Ce sta "se $X$ in $Y$ neodvisni slu"cajni spremenljivki, potem je

    \begin{equation*}
        p_Z(z) = \int_{-\infty}^{\infty} p_X(x) \cdot p_Y(z-x) dx \text{ - konvolucija funkcij } p_X \text{ in } p_Y
    \end{equation*}
\end{ex}

Vzemimo posebni primer $X \sim \chi^2(m)$, $Y \sim \chi^2(n)$, torej

\begin{equation*}
    p_X(x) = \frac{1}{2^{\frac{m}{2}} \Gamma(\frac{m}{2})} x^{\frac{m}{2} - 1} e^{-\frac{x}{2}} \text{ za } x > 0
    \text{ in 0 sicer}
\end{equation*}

za $p_Y(y)$ podobno. \\
Po zadnji formuli je $p_Z(z) = \int_{-\infty}^{\infty} p_X(x) \cdot p_Y(z-x) dx = 0$ za $z \leq 0$, \\
sicer je za $z > 0$

\begin{align*}
    &p_Z(z) = \frac{1}{2^{\frac{m}{2}} \Gamma(\frac{m}{2}) 2^{\frac{m}{2}} \Gamma(\frac{n}{2})} e^{-\frac{z}{2}}
        \int_{0}^{z} x^{\frac{m}{2} - 1} (z-x)^{\frac{m}{2} - 1 + \frac{n}{2} - 1 + 1} e^{-\frac{x}{2}}
        e^{-\frac{z-x}{2}} dx = \\
    &= \frac{1}{2^{\frac{m+n}{2}} \Gamma(\frac{m}{2}) \Gamma(\frac{n}{2})} e^{-\frac{z}{2}}
        \int_0^z x^{\frac{m}{2} - 1} (z-x)^{\frac{n}{2} - 1} dx =
\end{align*}

\begin{align*}
    &B(p,q) = \int_0^1 t^{p-1} (1-t)^{q-1} dt \\
    &x = tz \; dx = z dt
\end{align*}

\begin{align*}
    &= \frac{1}{2^{\frac{m+n}{2}} \Gamma(\frac{m}{2}) \Gamma(\frac{n}{2})} e^{-\frac{z}{2}}
        z^{\frac{m}{2} - 1 + \frac{n}{2} - 1 + 1} \int_0^1 t^{\frac{m}{2} - 1} (1-t)^{\frac{n}{2} - 1} dt =
\end{align*}

\begin{align*}
    &B(p,q) = \frac{\Gamma(p) \Gamma(q)}{\Gamma(p+q)} \\
    &\to B(\frac{m}{2}, \frac{n}{2}) = \frac{\Gamma(\frac{m}{2}) \Gamma(\frac{n}{2})}{\Gamma(\frac{m+n}{2})}
\end{align*}

\begin{align*}
    &= \frac{1}{2^{\frac{m+n}{2}} \Gamma(\frac{m+n}{2})} e^{-\frac{z}{2}} z^{\frac{m+n}{2} - 1}
\end{align*}

Torej $X + Y \sim \chi^n(m+n)$ \\
Dokazali smo

\begin{claim}
    Naj bosta neodvisni slu"cajni spremenljivki $X \sim \chi^2(m), Y \sim \chi^2(n)$z. Potem je
    $X + Y \sim \chi^2(m+n)$
\end{claim}



% 11. predavanje: 16.12.

\begin{conseq}
    "Ce so $X_1, X_2 \cdots X_n$ neodvisne slu"cajne spremenljivke, porazdeljene $N(0,1)$, potem je
    $Y := X_1^2 + \cdots + X_n^2$ porazdeljena po $\chi^2(n)$
\end{conseq}

\begin{proof}
    Vemo, da je $X_i^2 \sim \chi^2(1)$ in da so $X_1^2 \cdots X_n^2$ neodvisne spremenljivke. Potem je po trditvi
    + indukciji $Y \sim \chi^2(1 + \cdots + 1) = \chi^2(n)$
\end{proof}

Oglejmo si transformacijo $f: \R^2 \to \R, (x,y) \to (u,v)$, ki preslika zvezno porazdeljen slu"cajni vektor $(x,y)$ v
zvezno porazdeljen slu"cajni vektor $(u,v)$, torej $U = u(x,y), V = v(x,y)$ \\
Ozna"cimo "se $A_{u,v} = (-\infty,u] \times (-\infty,v]$ \\
Potem je

\begin{equation*}
    F_{(U,V)}(u,v) = \underset{A_{u,v}}{\iint} p_{(U,V)}(s,t) ds dt
\end{equation*}\label{eqn:Auv}

Pot drugi strani pa je

\begin{equation*}
    F_{(U,V)}(u,v) = P((U,V) \in A_{u,v}) = P((X,Y) \in f^{-1}(A_{u,v})) = \underset{f^{-1}(A_{u,v})}{\iint}
    p_{(X,Y)}(x,y) dx dy
\end{equation*}

Privzemimo "se, da je f zvezno odvedljiva bijekcija. Potem je $f: \R^2 \to \R^2, (u,v) \to (x,y)$ tudi zvezno
odvedljiva. Z zamenjavo spremenljivk $x = X(u,v), y = Y(u,v)$ v zadnjem intergalu dobimo

\begin{equation*}
    F_{(U,V)}(u,v) = \underset{A_{u,v}}{\iint} p_{(X,Y)}(x(s,t),y(s,t)) \cdot |J(s,t)| dx ds
\end{equation*}

kjer je

\begin{equation*}
    J(u,v) = \begin{bmatrix}
        \frac{\partial x}{\partial u} & \frac{\partial x}{\partial v} \\
        \frac{\partial y}{\partial u} & \frac{\partial y}{\partial v}
    \end{bmatrix}(u,v)
\end{equation*}

Jacobijeva determinanta.

Zaradi \ref{eqn:Auv} imamo torej $p_{(U,V)}(u,v) = p_{(X,Y)}(x(u,v), y(u,v)) |J(u,v)|$

Oglejmo si poseben primer

\begin{ex}
    $U = X, V = v(x,y)$ oz $X = U, Y = y(u,v)$ \\
    Tedaj je $p_{(U,V)}(u,v) = p_{(X,Y)}(u, y(u,v)) |\frac{\partial y}{\partial v}(u,v)|$ \\
    Gostota spremenljivke $V$ je $\int_{-\infty}^{\infty}  p_{(X,Y)}(u, y(u,v)) |\frac{\partial y}{\partial v}(u,v)| dv =
    p_V(v)$ \\
    Pi"simo $Z = V$, torej je $Y = y(x,z)$, saj je $U=X$ \\
    Potem prepi"semo $p_Z(z) = \int_{-\infty}^{\infty}  p_{(X,Y)}(u, y(x,z)) |\frac{\partial y}{\partial z}(x,z)| dx$
\end{ex}

\begin{ex} \text{} \\
    \begin{enumerate}
        \item $Z = X + Y \implies Y = Z - X$, torej je $y(x,z) = z - x, \frac{\partial y}{\partial z}(x,z) = 1$
            \begin{equation*}
                p_{X+Y}(z) = \int_{-\infty}^{\infty} p_{(X,Y)}(x,z-x) \cdot 1 dx
            \end{equation*}
        \item $Z = X \cdot Y \implies Y = \frac{Z}{X}$, torej je $y(x,z) = \frac{z}{x},
            \frac{\partial y}{\partial z}(x,z) = \frac{1}{x}$
            \begin{equation*}
                p_{X \cdot Y}(z) = \int_{-\infty}^{\infty} p_{(X,Y)}(x,\frac{z}{x}) \frac{1}{|x|} dx
            \end{equation*}
            "Ce sta "se $X$ in $Y$ neodvisni slu"cajni spremenljivki, potem je
            \begin{equation*}
                p_{X \cdot Y}(z) = \int_{-\infty}^{\infty} p_X(x) \cdot p_Y(\frac{z}{x}) \cdot \frac{1}{|x|} dx
            \end{equation*}
    \end{enumerate}
\end{ex}

\subsection{Matemati"cno upanje oz. pri"cakovana vrednost}

V primeru $X : \begin{pmatrix}x_1 & \cdots & x_n \\ p_1 & \cdots & p_n
\end{pmatrix}$ je matemati"cno upanje oz. pricakovana vrednost vsota $E(X) := \sum_{k=1}^{n} x_k \cdot p_k$ \\

V posebnem primeru $p_1 = \cdots = p_n = \frac{1}{n}$ je $E(X) = \frac{1}{n} \sum_{k=1}^{n} x_k =
\frac{x_1 + \cdots + x_n}{n}$ - povpre"cje "stevil $x_1 \cdots x_n$ \\

expected value, expectation, mean value \\

Naj bo $X$ diskretno porazdeljena slu"cajna spremenljivka z neskon"cno zalogo vrednosti:

\begin{equation*}
    X: \begin{pmatrix}
        x_1 & x_2 & x_3 & \cdots \\
        p_1 & p_2 & p_3 & \cdots \\
    \end{pmatrix}
\end{equation*}

$X$ ima matemati"cno upanje oz. pri"cakovano vrednost, "ce je $\sum_{k=1}^{\infty} |x_k| p_k < \infty$ \\
Tedaj je matemati"cno upanje definirano kot $E(X) = \sum_{k=1}^{\infty} x_k \cdot p_k$ \\
Naj bo sedaj $X$ zvezno porazdeljena slu"cajna spremenljivka z gostoto $p_X$. Potem ima $X$ matemati"cno
upanje, "ce je $\int_{-\infty}^{\infty} |x| \cdot p_X(x) dx < \infty$. Tedaje je matemati"cno upanje
slu"cajne spremenljivke $X$ enako $E(X) = \int_{-\infty}^{\infty} x \cdot p_X(x) dx$

\begin{ex} \text{} \\
    \begin{enumerate}
        \item $X \sim Ber(p)$ oz. $X: \begin{pmatrix}0 & 1 \\ q & p
            \end{pmatrix} q = 1-p, E(X) = 0 \cdot q + 1 \cdot p = p$
        \item $X \sim Poi(\lambda)$, torej $p_k = P(X=k) = \frac{\lambda^k}{k!} e^{-\lambda} k = 0, 1 \cdots$
            \begin{equation*}
                E(X) = \sum_{k=0}^{\infty} k \cdot p_k = \sum_{k=0}^{\infty} k \cdot \frac{\lambda^k}{k!} e^{-\lambda} =
                e^{-\lambda} \cdot \lambda \sum_{k=1}^{\infty} \frac{\lambda^{k-1}}{(k-1)!} = \lambda
            \end{equation*}
        \item Enakomerna porazdelitev na $[a,b]$
            \begin{align*}
                &p(X) = \begin{cases}
                    \frac{1}{b-a} \; \text{ "ce } a \leq x \leq b \\
                    0 \quad \text{sicer}
                \end{cases} \\
                &E(X) = \int_{a}^{b} x \cdot \frac{1}{b-a} dx = \frac{1}{b-a} \cdot \frac{x^2}{2} \vert_{a}^b =
                    \frac{b^2 - a^2}{2(b-a)} = \frac{b+a}{2}
            \end{align*}
        \item $X \sim N(\mu, \sigma)$ %skica
            \begin{align*}
                &E(X) = \frac{1}{\sigma \sqrt{2\pi}} \cdot \frac{-\infty}{\infty} x \cdot
                e^{-\frac{1}{2} (\frac{x-\mu}{\sigma})^2} dx =
            \end{align*}
            \begin{equation*}
                U = \frac{x - \mu}{\sigma} \implies du = \frac{dx}{\sigma}
            \end{equation*}
            \begin{align}
                &= \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} (\sigma u + \mu) e^{-\frac{1}{2} u^2} du =
                &= \frac{1}{\sqrt{2\pi}} \sigma \int_{-\infty}^{\infty} u \cdot e^{-\frac{1}{2} u^2} du +
                    \mu \cdot \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} u \cdot e^{-\frac{1}{2} u^2} du =
                &= \mu
            \end{align}
            Ker je v predzadnjem koraku 1. funkcija (v integralu) liha, 2. pa je gostota porazdelitve $N(0,1)$
        \item Cauchyjeva porazdelitev $p(x) = \frac{1}{\pi (1+x^2)}$ \\ % skica
            Nima matemati"cnega upanja, saj je $\int_{-\infty}^{\infty} |x| \cdot \frac{1}{\pi (1+x^2)} dx =
            \frac{2}{\pi} \int_{0}^{\infty} \frac{x}{1+x^2} dx = \frac{1}{\pi} ln(1+x^2) \vert_{0}^{\infty} = \infty$
        \item $1 - \frac{1}{2} + \frac{1}{3} - \cdots$ je pogojno konvergentna vrsta, t.j. konvergira, a ne absolutno
            \begin{align*}
                &X: \begin{pmatrix}
                    x_1 & x_2 & \cdots \\
                    p_1 & p_2 & \cdots \\
                \end{pmatrix}, \sum_{k=1}^{\infty} x_k \cdot p_k = \sum_{k=1}^{\infty} \frac{(-1)^{k-1}}{k} \\
                &x_k \cdot p_k = \frac{(-1)^{k-1}}{k} \\
                &\sum_{k=1}^{\infty} p_k = 1
                &p_k := \frac{1}{2^k} \text{ npr. ker je vsota 1} \\
                &x_k := \frac{(-1)^{k-1}}{k} = 2^k
            \end{align*}
            Ta porazdelitev nima matemati"cnega upanja, ker vrsta ne konvergira absolutno
    \end{enumerate}
\end{ex}

\begin{claim}
    Naj bo $f: \R \to \R$ zvezna funkcija
    \begin{enumerate}[label=(\alph*)]
        \item "Ce je $X: \begin{pmatrix} x_1 & x_2 & \cdots \\ p_1 & p_2 & \cdots \end{pmatrix}$ \\
            potem je $E(f \circ X) \equiv E(f(X)) = \sum_{k=1}^{\infty} f(x_k) \cdot p_k$
            ("ce le to matema"ticno upanje obstaja)
        \item "Ce je $X$ zvezno porazdeljena z gostoto $p_X$, potem je $E(f \circ X) =
            \int_{-\infty}^{\infty} f(x) \cdot p_X(x) dx$
    \end{enumerate}
\end{claim}

\begin{proof}
    (samo (a)): \\
    \begin{equation*}
        f \circ X: \begin{pmatrix}
            f(x_1) & f(x_2) & \cdots \\
            p_1 & p_2 & \cdots \\
        \end{pmatrix}
    \end{equation*}
    npr "ce $f(x_1) = f(x_3) \implies \begin{pmatrix}
        f(x_1) & f(x_2) & \cdots \\
        p_1 + p_3 & p_2 & \cdots \\
    \end{pmatrix}$ \\
    ($E(f \circ X) = \int_{-\infty}^{\infty} x \cdot p_{f(x)}(x) dx = \cdots =
    \int_{-\infty}^{\infty} f(x) \cdot p_X(x) dx$ - substitucija $y=f(x)$ v integralu)
\end{proof}



% 12. predavanje: 23.12.

\begin{conseq}
    Slu"cajna spremenljivka $X$ ima matemati"cno upanje $\iff$ $X$ ima matemati"cno upanje. Tedaj velja
    $|E(X)| = E(|X|)$
\end{conseq}

\begin{proof}
    (samo diskreten primer): \\
    \begin{equation*}
        E(|X|) \stackrel{\text{trd.} f(x)=|x|}{=} \sum_i |x_i| \cdot p_i \geq |\sum_i x_i \cdot p_i| = |E(X)|
    \end{equation*}
\end{proof}

\begin{conseq}
    Za $\forall a \in \R$ in vsako slu"cjano spremenljivko $X$ z matemati"cnim upanjem velja $E(a \cdot X) = a \cdot E(X)$
    (homogenost)
\end{conseq}

\begin{proof}
    $f(x) = a \cdot x$, trditev (od prej)
\end{proof}

Podobno kot zadnjo trditev se doka"ze

\begin{claim}
    Naj bo $f: \R^2 \to \R$ zvezna funkcija in $(X,Y)$ slu"cajni vektor
    \begin{enumerate}[label=(\alph*)]
        \item Naj bo $(X,Y)$ diskretno porazdeljen $p_{ij} := P(X=x_i, Y=y_j)$. Potem je $E(f(X,Y)) = \sum_i \sum_i
            f(x_i,y_j) \cdot p_{ij}$ ("ce le vrsta (oz. kon"cna vsota) absolutno konvergira)
        \item Naj bo $(X,Y)$ zvezno porazdeljen z gostoto $p(X,Y)$. Potem je $E(f(X,Y)) = \int_{-\infty}^{\infty} dx
            \int_{-\infty}^{\infty} f(x,y) p_{(X,Y)}(x,y) dy$ ("ce le integral absolutno konvergira)
    \end{enumerate}
\end{claim}

\begin{conseq}
    "Ce slu"cajni spremenljivki $X$ in $Y$ imata matamati"cno upanje, potem ga ima tudi $X+Y$ in velja $E(X+Y) = E(X) + E(Y)$
    (aditivnost)
\end{conseq}

\begin{proof}
    (samo zvezen primer): \\
    \begin{align*}
        &E(X,Y) \stackrel{f(x,y)=x+y}{=} \int_{-\infty}^{\infty} dx \int_{-\infty}^{\infty} (x+y) p_{(X,Y)}(x,y) dy =\\
        &= \int_{-\infty}^{\infty} x dx \int_{-\infty}^{\infty}p_{(X,Y)}(x,y) dy +
        \int_{-\infty}^{\infty} y dy \int_{-\infty}^{\infty}p_{(X,Y)}(x,y) dx =\\
        &= \int_{-\infty}^{\infty} x p_X(x) dx + \int_{-\infty}^{\infty} y p_{Y}(y) dy = E(X) + E(Y)
    \end{align*}
\end{proof}

\begin{conseq}
    Za slu"cajne spremenljivke $X_1 \cdots X_n$, ki imajo matemati"cno upanje, velja $E(a_1 X_1 + \cdots + a_n X_n) =
    a_1 E(X_1) + \cdots + a_n E(X_n)$ z $\forall a_1 \cdots a_n \in \R$
\end{conseq}

\begin{equation*}
    E(X+Y) = \int_{-\infty}^{\infty} x \cdot p_{X+Y}(x) dx \stackrel{\text{?}}{=} E(X) + E(Y) \text{ ni o"citno iz tega}
\end{equation*}

\begin{ex}
    \begin{enumerate}
        \item "Ce ima $X$ matemati"cno upanje, potem $E(X-E(X)) = E(X) - E(E(X)) = E(X) - E(X) = 0$
        \item $X_k \sim Ber(p)$, t.j. $X_k \sim \begin{pmatrix}0 & 1 \\ q & p\end{pmatrix}, q = 1 - p$
            \begin{equation*}
                X = X_1 + \cdots + X_n \implies E(X) = E(X_1) + \cdots + E(X_n) = n \cdot p
            \end{equation*}
    \end{enumerate}
\end{ex}

Posebej to (v 2. zgledu) velja v primeru, ko so $\{X_k\}_{i=1}^n$ neodvisne. To velja tudi za Bernoullijevo zaporedje
ponovitev poskusa: opazujemo dogodek A s $P(A) = p$. $X$ je frekvenca dogodka A v n ponovitvah poskusa. Potem je
$X \sim Bin(n,p)$ in $X = X_1 + \cdots + X_n$, kjer je $(X_k=1)$ dogodek, da se A zgodi v k-ti ponovitvi poskusa,
sicer je $(X_k=0)$. Po zgornjem je $E(X) = n \cdot p$. Do tega lahko pridemo tudi direktno:
\begin{align*}
    &E(X) = \sum_{k=0}^{n} k \cdot p_k = \sum_{k=0}^{n} k \cdot \binom{n}{k} p^k q^{n-k} = \\
    &= \sum_{k=1}^{n} k \cdot \frac{n}{k} \binom{n-1}{k-1} p^k q^{n-k} =
        np \sum_{k=1}^{n} \binom{n-1}{k-1} p^{k-1} q^{n-k} \stackrel{j=k-1}{=} \\
    &= np (\sum_{j=0}^{n-1} \binom{n-1}{j} p^j q^{n-1-j}) = np (p+q)^{n-1} = np
\end{align*}

\begin{claim}[Cauchy-Schwartzova neenakost]
    "Ce obstajata $E(X^2)$ in $E(Y^2)$, potem obstaja tudi $E(X,Y)$ in velja $E(|X \cdot Y|) \leq \sqrt{E(X^2) \cdot E(Y^2)}$.
    Ena"caj velja samo v primeru $|Y| = \sqrt{\frac{E(Y^2)}{E(X^2)}}|X|$ z verjetnostjo 1
\end{claim}

\begin{proof}
    Ker za nenegativa realna "stevila velja neenakost
    \begin{equation*}
        u \cdot v \leq \frac{1}{2}(u^2 + v^2) \; \iff \; (u-v)^2 \geq 0
    \end{equation*}
    za nenegativni slu"cajni spremenljivki $U$ in $V$ velja neenakost
    \begin{equation*}
        U \cdot V \leq \frac{1}{2}(U^2 + V^2)
    \end{equation*}
    Enakost velja samo v to"ckah $\omega \in \Omega$, za katere je $U(\omega) = V(\omega)$ \\
    "Ce vstavimo $U = a \cdot |X|$ in $V = \frac{1}{a}|Y|$ za $a > 0$, dobimo
    $|X \cdot Y| \leq \frac{1}{2} (a^2 Y^2 + \frac{1}{a^2}Y^2)$ in zato je
    \begin{equation}
        E(|X \cdot Y|) \leq \frac{1}{2} (a^2 E(X^2) + \frac{1}{a^2} E(Y^2)) \text{ za } \forall a > 0
    \end{equation}
    "Ce vstavimo $a^2 = \sqrt{\frac{E(Y^2)}{E(X^2)}}$ na desni strani dobimo
    \begin{equation*}
        \frac{1}{2} (\sqrt{E(Y^2) + E(X^2)} + \sqrt{E(X^2 + E(Y^2))}) = \sqrt{E(X^2) + E(Y^2)}
    \end{equation*}
    Torej je
    \begin{equation*}
        E(|X \cdot Y|) \leq \sqrt{E(X^2) \cdot E(Y^2)}
    \end{equation*}
    Enakost v neenakosti velja $\iff a |X| = \frac{1}{a} |Y|$, torej $|Y| = a^2 |X| = \frac{E(Y^2)}{E(X^2)} |X|$
    z verjetnostjo 1
\end{proof}

\begin{conseq}
    "Ce obstaja $E(X^2)$, potem obstaja $E(X)$ in velja $(E(X))^2 \leq E(X^2)$
\end{conseq}

\begin{proof}
    $Y=1$, t.j. $Y: \begin{pmatrix}1 \\ 1\end{pmatrix} \implies$
    \begin{align*}
        &E(|X \cdot 1|) \leq \sqrt{E(X^2) \cdot 1}  /^2
        &(E(|X|))^2 \leq E(X^2)
    \end{align*}
\end{proof}

\begin{claim}
    Naj bosta $X$ in $Y$ neodvisni slu"cajni spremenljivki, ki imata matemati"cni upanji. Potem ima matemati"cno
    upanje tudi $X \cdot Y$ in velja $E(X \cdot Y) = E(X) \cdot E(Y)$
\end{claim}

\begin{proof}
    (samo zvezem primer): \\
    \begin{align*}
        &E(X \cdot Y) \stackrel{\text{trd}}{=} \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} x \cdot y
            \cdot p_{(X,Y)}(x,y) dx dx \stackrel{\text{neodvisnost}}{=} &
        &=\underset{\R}{\iint} x \cdot y \cdot p_X(x) \cdot p_Y(y) dx dy =
            \int_{-\infty}^{\infty} x p_X(x) dx \cdot \int_{-\infty}^{\infty} x p_Y(y) dy = E(X) \cdot E(Y)
    \end{align*}
\end{proof}

\begin{defn}[Nekoreliranost]
    Slu"cajni spremenljivki $X$ in $Y$ sta nekorelirani, "ce velja $E(X \cdot Y) = E(X) \cdot E(Y)$, sicer sta
    korelirani.
\end{defn}

Po trditvi iz neodvisnosti sledi nekoreliranost. Obratno pa ne velja:

\begin{ex}
    \begin{align*}
        &U = \begin{pmatrix}0 & \frac{\pi}{2} & \pi \\ \frac{1}{3} & \frac{1}{3} & \frac{1}{3}\end{pmatrix} \\
        &X = cos(U): \begin{pmatrix}1 & 0 & -1 \\ \frac{1}{3} & \frac{1}{3} & \frac{1}{3}\end{pmatrix} \\
        &Y = sin(U): \begin{pmatrix}0 & 1 & 0 \\ \frac{1}{3} & \frac{1}{3} & \frac{1}{3}\end{pmatrix} =
            \begin{pmatrix}0 & 1 \\ \frac{2}{3} & \frac{1}{3}\end{pmatrix} \\
        &E(X) = 0, E(Y) = \frac{1}{3} \\
        &X \cdot Y = sin(U) \cdot cos(U) = 0 \implies E(X \cdot Y) = 0 \implies \text{ X in Y sta nekorelirani
            slu"cajni spremenljivki}
    \end{align*}
    \begin{center}
        \begin{tabular}{ c | c c | c}
            X \textbackslash Y & 0 & 1 & $\Sigma$ \\
            \hline
            -1 & $\frac{1}{3}$ & 0 & $\frac{1}{3}$ \\
            0 & 0 & $\frac{1}{3}$ & $\frac{1}{3}$ \\
            -1 & $\frac{1}{3}$ & 0 & $\frac{1}{3}$ \\
            \hline
            $\sum$ & $\frac{2}{3}$ & $\frac{1}{3}$ & 1
        \end{tabular}
        $\implies$ nista neodvisni, npr $\frac{1}{3} = P(X=1,Y=0) \neq P(X=1) \cdot P(Y=0) = \frac{1}{3} \cdot \frac{2}{3}$
    \end{center}
\end{ex}

\begin{claim}
    $X: \begin{pmatrix}x_1 & x_2 \\ p_1 & p_2\end{pmatrix}$,
    $Y: \begin{pmatrix}y_1 & y_2 \\ q_1 & q_2\end{pmatrix}$ \\
    Potem sta $X$ in $Y$ neodvisni $\iff$ nekorelirani \\
    $\iff E(X \cdot Y) = E(X) \cdot E(Y)$   % D.N?
\end{claim}

\subsection{Disperzija, kovarianco in korelacijski koeficient}

\begin{defn}[Disperzija]
    Naj obstaja $E(X^2)$. Disperzija oz. varianca slu"cajne spremenljivke $X$ je $D(X) \equiv var(X) := E((X-E(X))^2)$
\end{defn}

Disperzija meri razpr"senost slu"cajne spremenljivke $X$ okoli $E(X)$ \\
Ker je $E((X-E(X))^2) = E(X^2 - 2E(X)X + (E(X))^2) = E(X^2) - 2E(X)E(X) + (E(X))^2 = E(X^2) - (E(X))^2$, je
$D(X) = E(X^2) - (E(X))^2$



% 13. predavanje: 6.1.

Lastnosti disperzije:

\begin{itemize}
    \item $D(X) \geq 0$ in $D(X) = 0 \iff P(X=E(X)) = 1$, t.j. X je izrojena slu"cajna spremenljivka
    \item $D(a \cdot X) = a^2 D(X) \; a \in \R$
    \item $\forall a \in \R$ velja: $E((X-a)^2) \geq D(X)$. Enakost velja le v primeru $a = E(X)$
        \begin{proof}
            \begin{align*}
                &E((X-a)^2) = E(X^2 - 2aX + a^2) = E(X^2) - 2E(x)|a| + a^2 =
                &= (a-E(X))^2 + E(X^2) - (E(X))^2 = D(X) + (a-E(X))^2 \geq D(X)
            \end{align*}
            Enakost velja samo za $a=E(X)$
        \end{proof}
\end{itemize}

\begin{defn}[Standardna deviacija]
    Standardna deviacija ali standardni odklon slu"cajne spremenljivke $X$ je $\sigma(X) := \sqrt{D(X)}$
\end{defn}

Zanjo velja $\sigma(aX) = |a| \cdot \sigma(X)$ za $\forall a \in \R$ \\
Primeri nekaterih $E(X)$ in $D(X)$

\begin{enumerate}
    \item enakomerna diskretna porazdelitev: $\begin{pmatrix}x_1 & \cdots & x_n \\ \frac{1}{n} & \cdots & \frac{1}{n}\end{pmatrix}$
        \begin{align*}
            E(X) = \frac{x_1 + \cdots + x_n}{n}, D(X) = E(X^2) - (E(X))^2 = \frac{x_1^2 + \cdots + x_n^2}{2} -
            (\frac{x_1 + \cdots + x_n}{2})^2
        \end{align*}
    \item Binomska porazdelitev $Bin(n,p), n \in \N, p \in (0,1), q = 1-p$
        \begin{align*}
            E(X) = n \cdot p, D(X) = npq, \sigma(X) = \sqrt{npq}
        \end{align*}
    \item Poissonova porazdelitev $Poi(\lambda), \lambda > 0$
        \begin{align*}
            E(X) = \lambda, D(X) = \lambda
        \end{align*}
    \item Geometrijska porazdelitev $geo(p), p \in (0,1), q = 1-p$
        \begin{align*}
            E(X) = \frac{1}{p}, D(X) = \frac{q}{p^2}
        \end{align*}
    \item Pascalova porazdelitev $Pas(m,p), m \in \N, p \in (0,1)$
        \begin{align*}
            E(X) = \frac{m}{p}, D(X) = \frac{mq}{p^2}
        \end{align*}
    \item Enakomerna zvezna porazdelitev $Ed$ na $[a,b]$    % ed oznaka?
        \begin{align*}
            E(X) = \frac{a+b}{2}, D(X) = \frac{(b-a)^2}{12}
        \end{align*}
    \item Normalna porazdelitev $N(\mu, \sigma)$
        \begin{align*}
            E(X) = \mu, D(X) = \sigma^2, \sigma(X) = \sigma
        \end{align*}
    \item Porazdelitev gama $\gamma(b,c)$
        \begin{align*}
            E(X) = \frac{b}{c}, D(X) = \frac{b}{c^2}
        \end{align*}
    \item Porazdelitev $\chi^2(n) = \gamma(\frac{n}{2}, \frac{1}{2})$
        \begin{align*}
            E(X) = n, D(X) = 2n
        \end{align*}
    \item Eksponentna porazdelitev $Exp(\lambda), \lambda > 0$ $= \gamma(1,\lambda)$
        \begin{align*}
            E(X) = \frac{1}{\lambda}, D(X) = \frac{1}{\lambda^2}, \sigma(X) = \frac{1}{\lambda}
        \end{align*}
\end{enumerate}

Preverimo, da je $D(X) = \sigma^2$ za $X \sim N(\mu, \sigma)$

\begin{align*}
    &D(X) = E((X-E(X))^2) = \frac{1}{\sigma\sqrt{2\pi}} \int_{-\infty}^{\infty} (x-\mu)^2 \cdot
        e^{-\frac{1}{2} (\frac{x-\mu}{\sigma})^2} dx
\end{align*}
\begin{equation*}
    t = \frac{x-\mu}{\sigma} \implies x - \mu = \sigma t, dx = \sigma dt
\end{equation*}
\begin{align*}
    &= \frac{\sigma^2}{\sqrt{2\pi}} \int_{-\infty}^{\infty} t^2 e^{-\frac{1}{2}t^2} =
\end{align*}
\begin{align*}
    &u = t, dv = t \cdot e^{-\frac{1}{2}t^2} \\
    &du = dt, v = -e^{-\frac{1}{2}t^2}
\end{align*}   
\begin{align*}
    &\frac{\sigma^2}{\sqrt{2\pi}} (-t e^{-\frac{1}{2}t^2} \vert_{-\infty}^{\infty}) +
        \int_{-\infty}^{\infty} e^{-\frac{1}{2}t^2} dt=
    &= \frac{\sigma^2}{\sqrt{2\pi}} (0 + \sqrt{2\pi}) = \sigma^2
\end{align*}

\begin{defn}[Kovarianca]
    Kovarianca slu"cajnih spremnljivk $K(X,Y) \equiv Cov(X,Y) := E((X-E(X))(Y-E(Y)))$
\end{defn}

% ??
Ker je
\begin{equation*}
    E((X-E(X))(Y-E(Y))) = E(XY - E(Y)X - E(X)Y + E(X)E(Y)) = E(XY) - E(X)E(X) - E(X)E(Y)+ E(X)E(Y)
\end{equation*}
je $cov(X,Y) = E(XY) - E(X)E(Y)$ \\

Lastnosti:

\begin{enumerate}
    \item $K(X,X) = D(X)$
    \item $K(X,Y) = 0 \iff$ $X$ in $Y$ sta neodvisni
    \item $K$ je simetri"cna in bilinearna funkcija:
        \begin{itemize}
            \item $K(X,Y) = K(Y,X)$
            \item $K(aX+bY,Z) = aK(X,Z) + bK(Y,Z) \forall a,b \in \R$
        \end{itemize}
    \item "Ce obstajata $D(X)$ in $D(Y)$, potem obstaja tudi $K(X,Y)$. Tedaj velja $|K(X,Y)| \leq
        \sqrt{D(X) \cdot D(Y)} = \sigma(X) \cdot \sigma(Y)$ \\
        To sledi iz Cauchy-Schwartzove neenakosti ($|E(U \cdot V)| \leq \sqrt{E(U^2) \cdot E(V^2)}$) za
        slu"cajni spremenljivki $X-E(X)$ in $Y-E(Y)$. Ena"caj v neenakosti velja $\iff$ $Y - E(Y) \pm
        \frac{\sigma(Y)}{\sigma(X)} (X - E(X))$ z verjetnostjo 1
    \item "Ce X in Y imata disperziji, potem jo ima tudi $X+Y$ in valja $D(X+Y) = D(X) + D(Y) + 2K(X,Y)$ \\
        "ce sta $X$ in $Y$ nekorelirani (posebej neodvisni), potem je $D(X+Y) = D(X) + D(Y)$
        \begin{proof}
            Sledi iz enakosti
            \begin{align*}
                &(X+Y-E(X+Y))^2 ? ((X-E(X))+(Y-E(Y)))^2 = (X-E(X))^2 + (Y-E(Y))^2 + 2(X-E(X))(Y-E(Y)) \quad E() \\
                &D(X+Y) = E((X-E(X))^2) + E((Y-E(Y))^2) + E(2(X-E(X))(Y-E(Y))) = D(X) + D(Y) + 2K(X,Y)
            \end{align*}
        \end{proof}
    \item Posplo"sitev: $D(X_1 + \cdots + X_n) = D(X_1) + \cdots + D(X_n) + 2 \sum_{i<j} K(X_i,X_j)$ \\
        "Ce so $X_1 \cdots X_n$ paroma nekorelirani (posebej neodvisni), potem je $D(X_1 + \cdots + X_n) =
        D(X_1) + \cdots + D(X_n)$
\end{enumerate}

\begin{ex}
    $Bin(n,p)$ je vsota $X = X_1 + \cdots + X_n$, kjer je $X_i \sim Ber(p)$, t.j. $X_i \sim
    \begin{pmatrix}0 & 1 \\ q & p\end{pmatrix}$, ki so neodvisne \\
    Zato je $D(X) = D(X_1 + \cdots + X_n) = n \cdot D(X_1) = n \cdot p \cdot q$, saj je
    $D(X_n) = E(X_n^2) - (E(X_n))^2 = p - p^2 = pq$
\end{ex}

\begin{defn}[Standardizacija slu"cajne spremenljivke]
    Standardizacija sku"cajne spremenljivke $X$ je slu"cajna spremenljivka $X_s = \frac{X-E(X)}{\sigma(X)}$ 
\end{defn}

Zanjo velja:
\begin{itemize}
    \item $E(X_s) = 0$
    \item $D(X_s) = \frac{1}{\sigma(x)^2} \cdot D(X-E(X)) = \frac{1}{\sigma(X)^2} D(X) = 1$
\end{itemize}

\begin{ex}
    \begin{equation*}
        X \sim N(\mu, \sigma) \implies X_s = \frac{X-E(X)}{\sigma(X)} = \frac{X-\mu}{\sigma} \sim N(0,1)
    \end{equation*}
\end{ex}

\begin{defn}[Korelacijski koeficient]
    Korelacijski koeficient slu"cajnih spremenljivk $X$ in $Y$ je
    \begin{equation*}
        r(X,Y) = \frac{K(X,Y)}{\sigma(X) \sigma(Y)} = \frac{E((X-E(X))(Y-E(Y)))}{\sigma(X)\sigma(Y)} = E(X_s \cdot Y_s)
    \end{equation*}
\end{defn}

Lastnosti:

\begin{enumerate}
    \item $r(X,Y) = 0 \iff X$ in $Y$ sta nekorelirani
    \item $r(X,Y) \in [-1,1]$, kar sledi iz lastnosti (4) za kovarianco
    \item \begin{itemize}
        \item $r(X,Y) = 1 \iff Y = \frac{\sigma(Y)}{\sigma(X)} (X-E(X)) + E(Y)$ z verjetnostjo 1
        \item $r(X,Y) = -1 \iff Y = -\frac{\sigma(Y)}{\sigma(X)} (X-E(X)) + E(Y)$ z verjetnostjo 1
    \end{itemize}
        Tedaj imamo linearno zvezo med $X$ in $Y$
\end{enumerate}

\begin{ex}
    \begin{equation*}
        (X,Y) \sim N(\mu_x, \mu_y, \sigma_x, \sigma_y, \rho) \; \mu_x, \mu_y \in \R, \sigma_x, \sigma_y \in [0,\infty], \rho \in [-1,1]
    \end{equation*}
    Trdimo, da je $r(X,Y) = \rho$
    \begin{align*}
        &(X_s,Y_s) \sim N(0,0,1,1,\rho) \\
        &r(X,Y) = E(X_s \cdot Y_s) = \frac{1}{2\pi \sqrt{1-p^2}} \underset{\R}{\iint} x y 
            e^{-\frac{1}{2(1-\rho^2)} (x^2 - 2\rho xy ' y^2)} dx dy \\
    \end{align*}
    \begin{equation*}
        x^2 - 2 \rho xy + y^2 = (x-\rho y)^2 + (1-\rho^2)y^2
    \end{equation*}
    \begin{align*}
        %
        %
        %
% 14. predavanje: 13.1.
        %
        &= \int_{-\infty}^{\infty} y e^{-\frac{1}{2}y^2} dy = \frac{1}{\sqrt{2\pi (1-\rho^2)}}
        \int_{-\infty}^{\infty} x e^{-\frac{1}{2(1-\rho^2)} (x - \rho y)^2} dx = \\
        &=E(N(\rho y, \sqrt{1-\rho^2})), \text{ ker je } p(x) = \frac{1}{\sigma \sqrt{2\pi}}
        e^{-\frac{1}{2} (\frac{x - \mu}{\sigma})^2} = \\
        &= \rho \frac{1}{\sqrt(2\pi)} \int_{-\infty}^{\infty} y^2 e^{-\frac{1}{2} y^2} dy = \\
        &= (\frac{1}{\sqrt(2\pi)} \int_{-\infty}^{\infty} y^2 e^{-\frac{1}{2} y^2} = D(N(0,1)) = 1) \implies = \rho
    \end{align*}
\end{ex}
Torej sta X in Y nekorelirani $\stackrel{\text{v splo"snem}}{\iff} \rho = 0 \stackrel{\text{ta primer}}{\iff}
X, Y$ neodvisni \\
Kak"sna je gostota, "ce je $\rho$ blizu 1?
$\rho \uparrow 1:$ %skica
$\rho \downarrow -1:$ \\ %skica
gostota je \"skoraj skoncentrirana\" na neki premici, torej med X in Y obstaja skoraj linearna zveza

\subsection{Pogojna porazdelitev in pogojno matemati"cno upanje}

Izberimo si dogodek B s $P(B) > 0$

\begin{defn}
    Pogojna porazdelitvena funkcija slu"cajne spremnljivke X glede na B je $F_X(X \mid B) := P(X \leq x \mid B) =
    \frac{P(X \leq x \land B)}{P(B)}$ \\
\end{defn}

Ima enake lastnosti kot porazdelitvena funkcija

\begin{enumerate}[label=\Alph*]
    \item Diskreten primer \\
        Naj bo (X,Y) diskretno porazdeljen slu"cajni vektor z verjetnostno funkcijo $p_{ij} = P(X=x_i, Y=y_i) i,j = 1, 2 \cdots$ \\
        Za pogoj B vzemimo $B = (Y=y_j)$ pri nekem j, torej $q_j = P(Y=Y_j)$ \\
        Potem je pogojna porazdelitvena funkcija slu"cajne spremenljivke X glede $F_X(X \mid Y=y) :=
        \frac{P(X \leq x \mid Y=y_j)}{P(Y=y_j)} = \frac{1}{q_{j}} \sum_{j: x_j \leq x} p_{ij}$ \\
        "Ce vpeljemo pogojno verjetnostno funkcije $P_{i \mid j} = P(X=x_i \mid Y=y_j) = \frac{p_{ij}}{q_j}$,
        $F_X(X \mid Y=y_j) = \sum_{i: x_i \leq X} p_{i \mid j}$ \\
        Pogojno matemati"cno upanje slu"cajne spremenljivke X glede na $Y=y_j$ je matemati"cno upanje te porazdelitve:
        \[E(X \mid Y=y_j) := \sum_{i} x_i \cdot p_{i \mid j} = \frac{1}{q_j} \sum_{i} x_j \cdot p_{ij} \]    % e(x_i)?
        Regresijska funkcija $\ell (y_j) = \sum (X \mid Y=y_j)$, ki je definirana na zalogi vrednoti slu"cajne
        spremenljivke Y \\
        Definirajmo novo slu"cajno spremenljivko $E(X \mid Y) = \ell (y)$, ki ji re"cemo pogojno matemati"cno upanje
        slu"cajne spremenljivke X glede slu"cajne spremenljivke Y \\
        Ta ima shemo $E(X \mid Y) = \begin{pmatrix} \ell (y_1) & \ell (y_2) & \cdots \\ q_1 & q_2 & \cdots
        \end{pmatrix} = \begin{pmatrix} E(X \mid Y=y_1) & \cdots \\ q_1 & \cdots
        \end{pmatrix}$ \\
        Zanjo velja
        \[E(X \mid Y) = \sum_j \ell (y_j) \cdot q_j ? \sum_j \sum_i x_i \cdot p_{ij} = \sim_i x_i (\sum_j p_{ij}) =
        \sum_i x_i \cdot p_i = E(X)\]
        kjer je $p_i = P(X=x_i)$ \\
        Kaj dobimo, "ce sta X in Y neodvisni slu"cajni spremenljivki? \\
        Tedaj je $p_{i \mid j} = \frac{p_{ij}}{q_j} = \frac{p_i \cdot q_j}{q_j} = p_i$ in
        $\ell (y_j) = E(E(X \mid Y=y_j)) = \sum_i x_i \cdot p_{i \mid j} = \sum_i x_i \cdot p_i = E(X)$, torej je
        regresijska funkcija kar konstanta $E(X)$ oz. je $E(X \mid Y)$ izrojena slu"cajna spremenljivka z vrednostjo $E(X)$

        \begin{ex}
            Koko"s znese N jajc, kjer je $N \sim Poi(\lambda)$ z $\lambda > 0$. Iz vsakega jajca se z verjetnostjo
            $p \in (0,1)$ izvali pi"s"canec, neodvisno od drugih jajc. Naj bo K "stevilo pi"s"cancev Dolocino $E(K \mid N),
            E(K) in E(N \mid K)$ \\
            \[P(N=n) = \frac{\lambda^n}{n!} e^{-\lambda} \; n = 0, 1, 2 \cdots \]
            \[P(K=k \mid N=n) = \binom{n}{k} p^k q^{n-k} \; k = 0, 1 \cdots n \]
            \[\ell(n) = E(K \mid N=n) = E(Bin(n,p)) = n \cdot p\]
            torej je $E(K \mid N) = \ell(n) = p \cdot N$
            \[E(K \mid N) = \begin{pmatrix}p \cdot 0 & p \cdot 1 & p \cdot 2 & \cdots \\ P(N=0) & P(N=1) & P(N=2) & \cdots
            \end{pmatrix} \]
            \[E(K) = E(E(K \mid N)) = E(p \cdot N) = p \cdot E(N) = p \cdot \lambda \]
            \[P(K=k) = \sum_{n=k}^{\infty} P(K=k \mid N=n) \cdot P(N \leq n) = \sum_{n=k}^{\infty} \frac{n!}{k!(n-k)!}
            p^k q^{n-k} \cdot \frac{\lambda^n}{n!} e^{-\lambda} = \]
            \[= \frac{1}{k!} e^{-\lambda} p^k \lambda^k
            \sum_{n=k}^{\infty} \frac{(qk)^{n-k}}{(n-k)!} = \frac{(p\lambda)^k}{k!} e^{-\lambda} e^{q\lambda} =
            \frac{(p\lambda)^k}{k!} e^{-p\lambda} \; k = 0, 1 \cdots n\]
            Torej je $K \sim Poi(p \cdot \lambda)$ \\
            \[P(N=n \mid K=k) = \frac{P(N=n, K=k)}{P(K=k)} = \frac{P(K=k \mid N=n) \cdot P(N=n)}{P(K=k)} =\]
            \[= \frac{n! p^k q^{n-k}}{k! (n-k)!} \cdot \frac{\lambda^n e^{-\lambda}}{n!} \cdot
            \frac{p k! e^{p\lambda}}{(p \lambda)^k} = \frac{(q\lambda)^{n-k}}{(n-k)!} \cdot e^{-q\lambda} n = k, k+1 \cdots\]
            To je za k premaknjena Poissonova porazdelitev: $k + Poi(q \lambda)$ \\
            Potem je $\psi(k) = E(N \mid K=k) = E(k + Poi(q k)) = k + q \cdot \lambda$ in zato je $E(N \mid K) = \psi(k) =
            k \cdot q + \lambda$ \\
            Preizkus: $E(E(N \mid K)) = E(k + q \cdot \lambda) = p \lambda + q \lambda = \lambda = E(N)$ (ok) \\
            Regresijsko premico je vpeljal Golten (1822-1911)
        \end{ex}
    \item Zvezni primer \\
        Naj bo $(X,Y)$ zvezno porazdeljen slu"cajni vektor z gostoto $p_{(X,Y)}(x,y)$. Vzemimo $B = (y < Y \leq y+k)$ za
        nek $y \in \R, k > 0$. \\
        Potem je $F_X(X \mid y < Y \leq y+k) = P(x \leq x \mid y < Y \leq y+k) =
        \frac{P(X \leq x, y < Y \leq y+k)}{P(y < Y \leq y+k)} = \frac{F_{(X,Y)}(x,y+k) - F_{(X,Y)}(x,y)}{F_Y(y+k) - F_Y(y)}$ \\
        Pogojna porazdelitvena funkcija slu"cajne spremenljivke X glede na dogodek $(Y=y)$ je limita, "ce obstaja:
        \[F_X(x \mid Y=y) = \lim_{h \downarrow 0} F_X(x \mid y < Y \leq y+h) = \lim_{h \downarrow 0}
        \frac{F_{(X,Y)}(x,y+h) - F_{(X,Y)}(x,y)}{F_Y(y+h) - F_Y(y)}\]



% 15. predavanje: 14.2.

        Denimo sedaj, da sta $p_{X,Y}$ in $p_Y$ zvezni funkciji. Tedaj je $F_X(X \mid Y=y) = 
        \frac{\frac{\partial}{\partial y} F_{(X,Y)}(x,y)}{F_Y^{'}(y)} = \frac{1}{p_Y(y)}
        \int_{-\infty}^x p_{(X,Y)}(x,v) dv$ \\
        "Ce vpeljemo pogojno pogojno gostoto $p_X(x \mid Y=y) := \frac{p_{(X,Y)}(x,y)}{p_Y(y)}$, je torej
        \[F_{(X,Y)}(x \mid Y=y) = \int_{-\infty}^x p_X(u \mid y) du \]
        Pogojno matemati"cno upanje slu"cajne spremenljivke X glede na dogodek $(Y=y)$ je 
        \[E(X \mid Y=y) :=
        \int_{-\infty}^{\infty} x \cdot p_X(x|y) dx = \frac{1}{p_Y(y)} \cdot \int_{-\infty}^{\infty}
        x p_{(X,Y)}(x,y) dx\]
        Vpeljimo regresijsko funkcijo $l(y) := E(X \mid Y=y)$, definirano na zalogi vrednosti slu"cajne spremenljivke Y.
        Tako dobimo novo slu"cajno spremenljivko $E(X \mid Y) := l(y)$: pogojno matemati"cno upanje slu"cajne spremenljivke
        X glede na slucajno spremenljivko Y. \\
        Kot v diskretnem primeru se poka"ze enakost $E(E(X \mid Y)) = E(X)$

        \begin{ex}
            $(X,Y) \sim N(\mu_x,\mu_y,\sigma_x,\sigma_y,\rho)$ \\
            Robna gostota za Y je $N(\mu_y,\sigma_y)$ \\
            Zato je pogojna gostota
            \[p_X(x \mid y) = \frac{p_{(X,Y)}(x,y)}{p_y(x)} = \stackrel{\text{D.N.}}{\cdots} = \frac{1}{\sigma_x \sqrt(2\pi) (1-\rho^2)}
            exp(-\frac{1}{2 (1-\rho)^2} (\frac{x-\mu_x}{\sigma_x} - \rho \frac{y-\mu_y}{\sigma_y})^2)\]
            torej je $N(\mu_x + \rho \frac{\sigma_x}{\sigma_y}(y-\mu_y), \sigma_x \sqrt{1 - \rho^2})$ \\
            Eksponent: $\frac{1}{2 (1 -\rho^2)} \sigma_x^2 (x - (\mu_x + \rho \frac{\sigma_x}{\sigma_y} (y-\mu_y)))^2$ \\
            $\implies l(y) = E(X \mid Y=y) = \mu_x + \rho \frac{\sigma_x}{\sigma_y} (y - \mu_y)$ - 1. parameter \\
            $= \alpha + \beta y: \beta = \rho \frac{\sigma_x}{\sigma_y}, \alpha = \mu_x - \frac{\sigma_x}{\sigma_y} \cdot \mu_y$ \\
            Torej je $E(x \mid y) = \alpha + \beta y$
        \end{ex}

        \begin{ex}
            Meritev onesna"zenosti zraka \\
            Slu"cajna spremenljivka X meri koncentracijo ogljikovih delcev (v $\mu g / m^3$), Y pa koncentracijo ozona (v $\mu l/l = ppm$) \\
            Podatki ka"zejo, da ima (X,Y) pribli"zno dvorazse"zno normalno porazdelitev, $\mu_x = 10.7, \sigma_x^2 = 29, \mu_y = 0.1,
            \sigma_y^2 = 0.02, \rho = 0.72$ \\
            Koncentracija ozona je "skodljiva zdravju, "ce je $\geq 0.3$ \\
            Denimo, da naprava za merjenje ozona odpove, koncentracija "skodljivih delcev je $X = 200$
            \begin{enumerate}[label=\alph*]
                \item kolik"sna je pri"cakovana koncentracija ozona?
                \item kolik"sna je verjetnost, da je stopnja ozona zdravju skodljiva
            \end{enumerate}
            \begin{enumerate}[label=\alph*]
                \item \[E(Y \mid X=x) = \mu_y + \rho \frac{\sigma_y}{\sigma_x} (x - \mu_x) =
                    0.1 + 0.72 \sqrt{\frac{0.02}{29} (20 - 10.7)} \dot{=} 0.28 \]
                    % skica
                \item Pogojna porazdelitev $Y \mid X=x$ je $N(\mu_y + \rho \frac{\sigma_y}{\sigma_x} (x - \mu_x), \sigma_x \sqrt{1 - \rho^2}) =
                    N(0.28, 0.1)$ \\
                    \[P(Y>0.3 \mid X=20) = 1 - P(Y \leq 0.3 \mid X=20) = 1 - F_{N(0,1)} (\frac{0.3 - 0.28}{0.1}) \dot{=} 0.42 \]
            \end{enumerate}
        \end{ex}
\end{enumerate}

\subsection{Vi"sji momenti in vrstilne karakteristike}

\begin{defn}[Momenti]
    Naj bo $k \in \N$ in $a \in \R$. Moment reda k glede na to"cko a je $m_k(a) := E((X-a)^k)$ ("ce obstaja)
\end{defn}

Za a obicajno vzamemo
\begin{enumerate}
    \item $a=0$: $z_k := m_k(0) = E(X^k)$ za"cetni moment reda k
    \item $a=E(X)$: $m_k := m_k(E(X))$ cenralni moment reda k
\end{enumerate}

Ocitno je $z_1 = E(X), m_2 = D(X)$

\begin{claim}
    "Ce $\exists m_n(a)$, potem obstajaj tudi moment $m_k(a)$ za vse $k < n$
\end{claim}

\begin{proof}
    (V zveznem primeru): \\
    \[E((X-a)^k) = \int_{-\infty}^{\infty} (x-a)^k p_X(x) dx = \int{a-1}^{a+1} (X-a)^k p_X(x) dx +
    \int_{(-\infty,a-1) \cup (a+1,\infty)} (x-a)^k p_X(x) dx \leq \]
    \[\leq \int_{-\infty}^{\infty} p_X(x) dx + \int_{(-\infty,a-1) \cup (a+1,\infty)} (x-a)^k p_X(x) dx \leq\]
    \[\leq 1 + E((X-a)^k) < \infty\]
\end{proof}

\begin{claim}
    "Ce obstaja zacetni moment $z_n$, potem obstaja $m_n(a)$ glede na poljubno to"cko $a \in \R$
\end{claim}

\begin{proof}
    \[E((X-a)^n) \leq E((|X| + |a|)^n) = \sum_{k=0}^n \binom{n}{k} E(a)^{n-k} \cdot E(|X|^k) < \infty\]
\end{proof}

Centralne momente lahko izrazimo z za"cetnimi:
\[m_n(a) = E((X-a)^n) = \sum_{k=0}^n \binom{n}{k} (-a)^{n-k} E(X^k)\]
\[a = E(X) \; \implies \; m_k = \sum_{k=0}^n \binom{n}{k} (-1)^{n-k} z_1^{n-k} z_k \]

Asimetrija slu"cajne spremenljivke X je $A(X) := E(X_s^3) = E((\frac{X-E(X)}{\sigma_x})^3) =
\frac{m_3}{m_{2}^{\frac{3}{2}}}$ $m_2 = \sigma^2 = D(X)$ \\
$A(N(\mu,\sigma)) = 0, $ ker $A(X) = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} x^3 e^{-\frac{1}{2}x^2} dx$ \\
Splo"s"cenost (kurtozis) $K(X) := E(X_s^4) = \frac{m_4}{m_2^2}$ \\
$K(N(\mu,\sigma)) = 3$ \\
Ce momenti ne obstajajo (npr. "ze $E(X)$ ne), potem si lahko pomagamo z vrstilnimi karakteristikami

\begin{defn}[Mediana]
    Mediana slu"cajne spremenljivke X je vsaka vrednost $x \in \R$, za katero velja $P(X \leq x) \leq \frac{1}{2}$
    in $P(Y \geq x) \geq \frac{1}{2} (1-P(X < x) = 1 - F(x-))$
\end{defn}

"Ce je F porazdelitvena funkcija za X, je to ekvivalentno s pogojem $F(x-) \leq \frac{1}{2} \leq F(x)$ \\
"Ce je X zvezno porazdeljena slu"cajna spremenljivka, dobimo $F(X) = \frac{1}{2}$ oz.
$\int_{-\infty}^{\infty} p(t) dx = \frac{1}{2}$     % skica grafa



% 16. predavanje: 21.2.

Te vrednosti (lahko jih je ve"c) ozna"cimo z $X_{\frac{1}{2}}$

\begin{ex} \text{} \\
    \begin{itemize}
        \item
            $X \sim \begin{pmatrix}0 & 1 \\ \frac{1}{5} & \frac{4}{5} \end{pmatrix}$ \\
            % skica
            $x_{\frac{1}{2}} = 1, E(X) = \frac{4}{5}$
        \item $X: \begin{pmatrix}-1 & 0 & 1 \\ \frac{1}{4} & \frac{1}{4} & \frac{2}{4} \end{pmatrix}$ \\
            % skica
            Mediane so $[0,1]$
        \item % skica
        \item $X \sim N(0,1)$ \\
            % skica
            $x_{\frac{1}{2}} = \mu = E(X)$
    \end{itemize}
\end{ex}

\begin{defn}[Kvantil]
    Kvantil reda p $(p \in (0,1))$ je vsaka vrednost $x_p$, za katero velja $P(X \leq x_p) \geq p$ in $P(X \geq x_p) \geq 1-p$ \\
    Ekvivalentno je $F(x_p-) \leq p \leq F(x_p)$
\end{defn}

"Ce je X zvezno porazdeljena, je pogoj $F(x_p) = p$ t.j. $\int_{-\infty}^{\infty} p(t) dt = p$

\begin{itemize}
    \item Kvartili: $X_{\frac{1}{4}}, X_{\frac{2}{4}}, X_{\frac{3}{4}}$
    \item Percentili: $X_{\frac{1}{100}}, X_{\frac{2}{100}}, \cdots X_{\frac{99}{100}}$
\end{itemize}

\begin{ex}
    Telesna vi"sina odraslih mo"skih
    % skica
\end{ex}

\begin{defn}[(Semiinter)kvartilni razmik]
    $s := \frac{1}{2} (x_{\frac{3}{4}} - x_{\frac{1}{4}})$
\end{defn}

je nadomestek (analog) za standardno deviacijo

\begin{ex} \text{} \\
    \begin{itemize}
        \item $X \sim N(0,1)$ \\
            $X_{\frac{1}{2}} = 0$ \\
            $\int_{-\infty}^{\frac{1}{4}} p(t) dt = \frac{1}{4} \xRightarrow{\text{tabelca}} x_{\frac{1}{4}} \doteq -0.67$ \\
            $\xRightarrow{\text{simetrija}} x_{\frac{3}{4}} \doteq 0.67 \implies s = 0.67, \sigma(x) = 1$ \\
        \item $X$ naj ima Cauchyjevo porazdelitev \\
            $p(x) = \frac{1}{\pi(1 + x^2)}$ \\
            $x_{\frac{1}{2}} = 0$ \\
            % skica
            Momenti ne obstajajo \\
            \[\int_{-\infty}^{x_{\frac{1}{4}}} \frac{1}{\pi} \frac{1}{1 + x^2} dx = \frac{1}{4} \]
            \[\frac{1}{\pi} \arctan x \vert_{x=-\infty}^{x_{\frac{1}{4}}} = \frac{1}{4} \]
            \[\frac{1}{\pi} arctan x_{\frac{1}{4}} + \frac{1}{2} = \frac{1}{4}\]
            \[arctan x_{\frac{1}{4}} = \frac{1}{4} \implies x_{\frac{1}{4}} = -1\]
            \[\xRightarrow{\text{simetrija}} x_{\frac{3}{4}} = 1, s = 1\]
    \end{itemize}
\end{ex}

\subsection{Rodovne funkcije}

\begin{defn}
    Naj bo X slu"cajna spremenljivka z vrednostmi v $\N \cup \{0\}: p_k = P(X = k) k = 0, 1, 2 \cdots \;
    p_k \geq 0, \sum_{k = 0}^{\infty} = 1$ \\
    Rodovna funkcija sku"cajne spremenljivke X je
    \[G_X(s) = p_0 + p_1 s + p_2 s^2 + \cdots = \sum_{k = 0}^{\infty} p_k \cdots s^k\]
    za $\forall s \in \R$, za katere vrsta absolutno konvergira.
\end{defn}

O"citno je $G_X(0) = p_0, G_X(1) = \sum_{k = 0}^{\infty} p_k = 1$ \\
Ker je $s^X: \begin{pmatrix}s^0 & s^1 & s^2 & \cdots \\ p_0 & p_1 & p_2 & \cdots\end{pmatrix}$, je $G_X(s) = E(s^X)$ \\
Za $s \in [-1,1]$ velja $|p_k \cdot s^k| \leq P_k$ in $\sum_{k = 0}^{\infty} p_k = 1$. Zato je vrsta
konvergentna, "ce je $|s| \leq 1$. Torej je konvergen"cni radij vrste vsaj 1

\begin{ex} \text{} \\
    \begin{itemize}
        \item $X \sim geo(p)$, $p \in (0,1)$
            \begin{align*}
                &p_k = P(X = k) = p \cdot q^{k-1} \; k = 1,2,3 \cdots \\
                &G_X(s) = \sum_{k = 1}^{\infty} p \cdot q^{k - 1} s^k = ps \sum_{k = 0}^{\infty} (qs)^{k-1} \\
                &= ps \frac{1}{1 - qs}
            \end{align*}
            konvergira, ko $|qs| < 1 \Leftrightarrow |s| < \frac{1}{|q|} =: R$
        \item $p_k = P(X = k) = \frac{\lambda^k}{k!} e^{-\lambda}$
            \[G_X(s) = \sum_{k = 0}^{\infty} \frac{\lambda^k}{k!} e^{-\lambda} s^k =
            e^{-\lambda} \sum_{k = 0}^{\infty} \frac{(\lambda s)^k}{k!} = \]
            \[= e^{-\lambda} \cdot e^{\lambda s} = e^{\lambda(s - 1)} \]
            $R = \infty \; \forall s \in \R$
    \end{itemize}
\end{ex}

Iz teorije Taylorjevih vrst sledi

\begin{theorem}[O enili"cnosti]
    Naj imata X in Y rodovni funkciji $G_X$ in $G_Y$. Potem je $G_X(s) = G_Y(s)$ za $\forall s \in [-1,1] \leftrightarrow
    P(X = k) = P(Y = k)$ za vse $k = 0, 1, 2 \cdots$ \\
    Tedaj velja $P(X = k) = \frac{1}{k!} G_X^{k}(0)$
\end{theorem}

$G_X(s) = \sum_{k = 0}^{\infty} p_k s^k$, $p_k = P(X = k)$ \\
Naj ima rodovna funkcija $G_X$ slu"cajne spremenljivke X konvergen"cni radij R > 1. Potem za $\forall s \in (-R,R)$ velja
$G_X^{'}(s) = \sum_{k = 1}^{\infty} k \cdot p_k s^{k-1}$ \\
"Ce postavimo $s=1$, dobimo $G^{'}(1) = \sum_{k = 1}^{\infty} k \cdot p_k = E(X)$

\begin{theorem}
    Naj ima X rodovno funkcijo $G_X(s)$ in naj bo $n \in \N$. Potem je
    \[G_X^{n}(1-) \equiv \lim_{s \nearrow 1} G_X^{n}(s) = E(X (X-1) (X-2) \cdots (X-N+1))\]
\end{theorem}

\begin{proof}
    Za $\forall s \in [0,1)$ je $G_X^{n}(s) = \sum_{k = n}^{\infty} k(k-1)(k-2) \cdots (k-n+1) p_k s^{k-n+1} =$
    \[= E(X(X-1)(X-2) \cdots (X-n+1) \cdot s^{X-n}) \]
    Ko gre $s \uparrow 1$, z uporabo Abelove leme dobimo
    \[\lim_{s \nearrow 1} G_X^{n}(s) = \lim_{s \nearrow 1} \sum_{k = n}^{\infty} k(k-1) \cdot (k-n+1) =\]
    \[\stackrel{\text{Abelova lema}}{=} \sum_{k = n}^{\infty} lim_{s \nearrow 1} k(k-1) \cdot (k-n+1) =
    \sum_{k = n}^{\infty} k(k-1) \cdot (k-n+1) p_k = E(X(X-1) \cdots (X-n+1))\]
\end{proof}

\begin{conseq}
    \[E(X) = G_{X}^{'}(1-)\]
    \[D(X) = E(X^2) - (E(X))^2 = E(X(X-1)) + E(X) - (E(X))^2 = G_X^{(2)}(1-) + G_X^{(1)}(1) - (G_X^{(1)}(1-))^2\]
\end{conseq}

\begin{theorem}
    Naj bosta X in Y neodvisni slu"cajni spremenljivki z rodovnima funkcijama $G_X$ in $G_Y$. Potem je $G_{X+Y}(s) =
    G_X(s) \cdot G_Y(s)$ za $s \in [-1,1]$
\end{theorem}

\begin{proof}
    $G_{X+Y}(s) = E(s^{X+Y}) = E(s^X \cdot s^Y) \stackrel{\text{izrek}}{=} E(s^X) \cdot E(s^Y) = G_X(s) \cdot G_Y(s)$,
    saj sta $s^X$ in $s^Y$ neodvisni slu"cajni spremenljivki
\end{proof}

\begin{general}
    "Ce so $X_1, X_2 \cdots X_n$ neodvisne slu"cajne spremenljivke, potem je za vse $s \in [-1,1] G_{X_1 + \cdots + X_n}(s) =
    G_{X_1}(s) \cdot \cdots \cdot G_{X_n}(s).$ \\
    "Ce so $X_1, X_2 \cdots X_n$ enako porazdeljene in neodvisne, potem je
    \begin{align*}
        G_{X_1 + \cdots + X_n}(s) = (G_X(s))^n    % (*)
    \end{align*}
\end{general}

\begin{theorem}
    Naj bodo za $\forall n \in \N$ slu"cajne spremenljivke $N, X_1, X_2 \cdots X_n$ neodvisne. Naj ima N rodovno
    funkcijo $G_N, X_n$ pa rodovno funkcijo $G_X$. Potem ima slu"cajna spemenljivka $S := X_1 + X_2 + \cdots + X_n$
    rodovno funkcijo enako $G_S = G_N \circ G_X$ oz. $G_S(s) = G_N(G_X(s))$ za $s \in [-1,1]$
\end{theorem}

To je posplo"sitev formule dd: $P(N = n) = 1, G_N(s) = 1 \cdot s^n = s^n$%(*)

\begin{proof}
    Zaradi neodvisnosti imamo $P(S = k) = \sum_{n=0}^{\infty} P(S = k, N = n) =$
    \[= \sum_{n=0}^{\infty} P(N = n, X_1 + \cdots + X_n = k) \stackrel{\text{neodvisnost}}{=}
    \sum_{n=0}^{\infty} P(N = n) \cdot P(X_1 + \cdots + X_n = k)\]
    Zato je
    \[G_S(s) = \sum_{k=0}^{\infty} P(S = k) \cdot s^k =
    \sum_{k=0}^{\infty} \sum_{n=1}^{\infty} P(N = n) \cdot P(X_1 + \cdots + X_n = k) \cdot s^k =\]
    \[= \sum_{n=1}^{\infty} P(N = n) (\sum_{k=0}^{\infty} P(X_1 + \cdots + X_n = k) \cdot s^k) =\]
    \[\stackrel{G_{X_1 + \cdots + X_n}(s) \stackrel{\text{neodvisnost}}{\*} (G_X(s)^n)}{=}
    \sum_{n=1}^{\infty} P(N = n) \cdot (G_X(s))^n = G_N(G_X(s))\]
    za vse $s \in [-1,1]$
\end{proof}



% 17. predavanje: 28.2.

\begin{conseq}
	Pri predpostavkah iz izreka velja Waldova enakost: \[E(S) = E(N) \cdot E(X)\]
\end{conseq}

\begin{proof}
    \begin{align}
    &G_S(s) = G_N(G_X(s)) \forall s \in [-1,1] \\
    &E(S) = G_s^{'}(1-) = G_N^{'}(G_X(1-)) \cdot G_X^{'}(1-) = E(N) \cdot E(X)
    \end{align}
\end{proof}

\begin{ex}
    Koko"s, jajca, pi"s"canci \\
    N jajc, $N \sim Poi(\lambda)$ \\
    K je "stevilo pi"s"cancev \\
    Definiramo $X_i = 1$ dogodek, da se iz i-tega jajca izvali pi"s"canec, sicer $X_i = 0$. Potem je
    $X_i: \begin{pmatrix}
        0 & 1 \\
        q & p
    \end{pmatrix}, q = 1 - p$ in $X_i$ so neodvisne slu"cajne spremenljivke. \\
    O"citno je $K = X_1 + X_2 + \cdots + X_n$ \\
    Ker je $G_N(s) = e^{\lambda(s-1)}$ in $G_X(s) = q \cdot s^0 + p \cdot s = q + ps$, je po
    izreku $G_K(s) = G_N(G_X(s)) = e^{\lambda(q + ps - 1)} = e^{\lambda(ps - p)} = e^{\lambda p(s-1)} \forall s \in [-1,1]$,
    zato je $K \sim Poi(\lambda p)$
\end{ex}

\subsection{Momentno rodovna funkcija}

\begin{defn}[Momentno rodovna funkcija]
    Momentno rodovna funkcija je $M_X(t) = E(e^{tX})$ za $t \in \R$, za katere obstaja matemati"cno upanje
\end{defn}

V primeru zvezne porazdelitve je $M_X(t) = \int_{-\infty}^{\infty} e^{tx} p_X(x) dx$ \\
To je Laplaceova transformacija funkcije $p_X$ \\
V diskretnem primeru $X: \begin{pmatrix}x_1 & x_2 & \cdots \\ p_1 & p_2 & \cdots \end{pmatrix}$ je
$M_X(t) = \sum_i e^{tx} p_i$ \\

V posebnem primeru, ko ima X nenegative celo"stevilske vrednosti, je $M_X(t) = \sum_{i=0}^{\infty} e^{it} p_i =$
\[= \sum_{i=0}^{\infty} p_i (e^{t})^{i} = G_X(e^t) \; (M_X(t) = E((e^t)^X) = G_X(e^t))\]
\[G_X(s) = E(s^X)\]

O"citno je $M_X(0) = E(e^0) = E(1) = 1$

\begin{ex}
    \[X \sim N(0,1)\]
    \[M_X(t) = \int_{-\infty}^{\infty} e^{tx} \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}} dx =\]
    \[= \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} e^{-\frac{(x-t)^2}{2}} dx \cdot e^{-\frac{t^2}{2}} =\]
    \[= e^{\frac{t^2}{2}} \forall t \in \R\]
    ker je $\frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} e^{-\frac{(x-t)^2}{2}}$ gostota za $N(0,1)$
\end{ex}

\begin{theorem}
    Naj bo $M_X(t) < \infty$ (obstaja, $< \infty$ zato, ker je $e^t > 0$) za $\forall t \in (-\delta, \delta)$ pri
    nekem $\delta > 0$. Potem je porazdelitev za X natanko dolo"cena z $M_X$, vsi za"cetni momenti obstajajo,
    $z_k = E(X^k) = M_X^{k}(0)$ za $\forall k \in \N$ in velja $M_X(t) = \sum_{k=0}^{\infty} \frac{z_k}{k!} t^k$ % M_X^{(k)} ?? 
    za $\forall t \in (-\delta, \delta)$
\end{theorem}

\begin{proof} (bistvo)
    \[M_X(t) = E(e^{t \cdot X}) = E(\sum_{k=0}^{\infty} t^k \frac{x^k}{k!}) =\]
    \[\sum_{k=0}^{\infty} \frac{E(X^k)}{k!} t^k = \sum_{k=0}^{\infty} \frac{z^k}{k!} t^k\]
\end{proof}

\begin{claim}
    $M_{aX+b}(t) = e^{bt} M_X(at), a \neq 0, b \in R$
\end{claim}

\begin{proof}
    $M_{aX+b}(t) = E(e^{t(aX+b)}) = E(e^{(at)X} \cdot e^{bt}) = e^{bt} M_X(at)$
\end{proof}

\begin{theorem}
    "Ce sta X in Y neodvisni slu"cajni spremenljivki, potem je $M_{X+Y}(t) = M_X(t) \cdot M_Y(t)$
\end{theorem}

\begin{proof}
    $M_{X+Y}(t) = E(e^{t(X+Y)}) = E(e^{t^X} \cdot e^{tY}) \stackrel{\text{$e^{tX}$, $e^{tY}$ neodvisni}}{=} $ \\
    $= E(e^{t^X}) \cdot E(e^{tY}) = M_X(t) \cdot M_Y(t)$
\end{proof}

\begin{claim}
    Naj bosta X in Y neodvisni slu"cajni spremenljivki in $X \sim N(\mu_x, \sigma_x), Y \sim N(\mu_y, \sigma_y)$.
    Potem je $X + Y \sim N(\mu_x + \mu_y, \sqrt{\sigma_x^2 + \sigma_y^2})$
\end{claim}

\begin{proof}
    Ker je
    \[U := \frac{X-\mu_x}{\sigma_x} = \frac{X-E(X)}{\sigma(X)} \sim N(0,1)\]
    (standardizacija), je
    \[X = \sigma_x \cdot U + \mu_x\]
    in zato je
    \[M_X(t) = e^{\mu_x t} \cdot M_U(\sigma_x t)\]
    po zadnji trditvi. Potem je
    \[M_U(t) = e^{\frac{t^2}{2}}\]
    je
    \[M_X(t) = e^{\mu_x t} \cdot e^{\frac{\sigma_x^2 t^2}{2}} = e^{\frac{\sigma_x^2 t^2}{2} + \mu_x t} \; \forall t \ in \R\]
    za $Y$ velja podobno. Po zadnjem izreku je
    \[M_{X+Y}(t) = M_X(t) \cdot M_Y(t) = e^{\frac{\sigma_x^2 t^2}{2} + \mu_x t} \cdot e^{\frac{\sigma_y^2 t^2}{2} + \mu_y t} =\]
    \[= e^{\frac{(\sigma_x^2 + \sigma_y^2) t^2}{2} + (\mu_x + \mu_y) t}\]
    Po izreku je
    \[X + Y \sim N(\mu_x+\mu_y, \sqrt{\sigma_x^2 + \sigma_y^2})\]
\end{proof}

\begin{rem}
    Če bi vedeli, da je $X + Y$ porazdeljena normalno, bi ``samo'' izra"cunali parametra %integral?
\end{rem}

\begin{ex}
    \[X \sim N(0,1), M_X(t) = e^{\frac{t^2}{2}} = \sum_{k=0}^{\infty} \frac{(\frac{t^2}{2})^k}{k!} =
    \sum_{k=0}^{\infty} \frac{1}{2^k \cdot k!} t^{2k}\]
    Po drugi strani je $M_X(t) = \sum_{j=0}^{\infty} \frac{z_j}{j!} t^j \; \forall t \in \R$ \\
    Primerjamo koeficiente:
    \begin{itemize}
        \item lihi koeficienti: $z_{2k-1} = 0 \; k \in \N$
        \item sodi koeficienti:
        \[\frac{z_{2k}}{(2k)!} = \frac{1}{k! 2^k} \implies z_{2k} = \frac{(2k)!}{k! 2^k} =\]
        \[= \frac{1 \cdot 2 \cdot 3 \cdot \cdots \cdot (2k)}{2 \cdot 4 \cdot 5 \cdot \cdots \cdot (2k)} =
        1 \cdot 3 \cdot 5 \cdot \cdots \cdot (2k-1) = (2k-1)!! \; k \in \N\]
    \end{itemize}
\end{ex}

\subsection{"Sibki in krepki zakon velikih "stevil}

\begin{defn}[Verjetnostna konvergenca]
    Zaporedje slu"cajnih spremenljivk $\{X_n\}_{n \in \N}$ verjetnostno konvergira proti sku"cajni spremenljivki
    X, "ce za $\forall \epsilon > 0$ velja $\lim_{n \to \infty} P(|X_n-X| \geq \epsilon) = 0$ \\
    oz. $\lim_{n \to \infty} P(|X_n-X| < \epsilon) = 1$
\end{defn}

\begin{defn}[Skoraj gotova konvergenca]     % izraz ?
    Zaporedje slu"cajnih spremenljivk $\{X_n\}_{n \in \N}$ skoraj gotovo konvergira proti sku"cajni spremenljivki
    X, "ce velja P(p $\lim_{n \to \infty} X_n = X) = 1$ \\
    Tukaj je $(\lim_{n \to \infty} X_n = X) = \{\omega \in \Omega: \lim_{n \to \infty} X_n(\omega) = X(\omega)\} =$
    \[= \{\omega \in \Omega: \forall k (\in \N) \exists m \in \N \forall n \geq m: |X_n(\omega) - X(\omega)| < \frac{1}{k}\} =\]
    \begin{align}
        = \{\cap_{k \in \N} \cup_{m \in \N} \cap_{n \geq m} \omega \in \Omega: |X_n(\omega) - X(\omega)| < \frac{1}{k}\} \*
    \end{align}
\end{defn}

\begin{rem}
    Števne unije in preseki $\implies$ smo v $\sigma-$algebri, torej je to res dogodek
\end{rem}

\begin{claim}
    Če $X_n \xrightarrow[]{n \to \infty} X$ skoraj gotovo, potem za $\forall \epsilon > 0
    \lim_{n \to \infty} P(|X_n - X| < \epsilon \text{ za } n \geq m) = 1$
\end{claim}

\begin{proof}
    Ozna"cimo $c_m := (|X_n - X| < \epsilon \text{ za } n \geq m) = \cap_{n=m}^{\infty} (|x_n - X| < \epsilon)$. \\
    Potem je $c_1 \subseteq c_2 \subseteq \cdots$ \\
    \* je $c_m$ za $\epsilon = \frac{1}{k}$ in $(\lim_{n \to \infty} X_n = X) \subseteq \cup_{n=1}^{\infty} c_m$ (presek) \\
    Torej je $1 = P(\lim_{n \to \infty} X_n = X) \subseteq =(\cup_{m=1}^{\infty} c_m) = \lim_{m \to \infty} P(c_m)$ \\
    Od tod sledi $\lim_{m \to \infty} P(c_m) = 1$
\end{proof}

\begin{conseq}
    "Ce $X_n \xrightarrow[]{n \to \infty} X$ skoraj gotovo, potem $X_n \xrightarrow[]{n \to \infty} X$ verjetnostno konvergira.
\end{conseq}

\begin{proof}
    Izberemo $\epsilon > 0$. Potem velja
    \[P(|X_n - X| < \epsilon \text{ za } \forall n \geq m) \leq P(|X_m - X| < \epsilon)\]
    "Ce uporabimo trditev, dobimo $\lim_{n \to \infty} P(|X_n - X| < \epsilon) = 1$ (leva stran). % verjetnostna konvergenca \\
\end{proof}

\begin{rem}
    Obratna implikacija ne velja
\end{rem}



% 18. predavanje: 7.3.

\begin{defn}
    Naj bo $X_1, X_2, X_3 \cdots$ zaporedje slu"cajnih spremenljivk, ki imajo matemati"cno upanje.
    Definirajmo $Y_n = \frac{S_n - E(S_n)}{n} = \frac{X_1 + \cdots + X_n}{n} - \frac{E(X_1) + \cdots + E(X_n)}{n}$ \\
    Potem je $E(Y_n) = 0$ \\
    Za $\{Y_n\}_{n \in \N}$ velja "sibki zakon velikih "stevil ("SZV"S), kadar $Y_n \stackrel{n \to \infty}{\rightarrow} 0$
    verjetnostno, torej za $\forall \epsilon > 0 \lim_{n \to \infty} (|y| < \epsilon) = 1 =
    \lim_{n \to \infty} (|\frac{S_n - E(S_n)}{n}| < \epsilon)$
    Za $\{Y_n\}_{n \in \N}$ velja krepki zakon velikih "stevil (KZV"S), kadar $Y_n \stackrel{n \to \infty}{\rightarrow} 0$
    skoraj gotovo, torej $P(\lim_{n \to \infty} \frac{S_n - E(S_n)}{n} = 0) = 1$ \\
    "Ce velja KVZ"S, potem velja "SVZ"S
\end{defn}

\begin{ex}
    Me"cemo kocko, $X_k$ je $\#$ pik v k-tem metu. Potem je $E(X_k) = \frac{7}{2}$ in $Y_n = \frac{X_1 + \cdots + X_n}{n} - \frac{7}{2}$ \\
    Ali konvergira $\frac{X_1 + \cdots + X_n}{n} \stackrel{n \to \infty}{\rightarrow} \frac{7}{2}$ skoraj gotovo? (Da)
\end{ex}

\begin{theorem} \text{} \\
    \begin{enumerate}[label=\alph*]
        \item Neenakost Markova: "ce slu"cajna spremenljivka X ima matemati"cno upanje, potem je
            $P(|X| \geq a) \leq \frac{E(|X|)}{a}$ za $\forall a > 0$
        \item Neenakost "Cebi"seva: "ce slu"cajna spremenljivka X ima disperzijo, potem je
            $P(|X - E(X)| \geq a \cdot \sigma(x)) \leq \frac{1}{a^2}$ za $\forall a > 0$ (pomembno za
            $a \geq 1$, ker je verjetnost $\leq 1$) \\
            oz. "ce pi"semo $\epsilon = a \cdot \sigma(x) \implies P(|X - E(X)| \geq \epsilon) \leq \frac{D(X)}{\epsilon^2}$
            za $\forall \epsilon > 0$
    \end{enumerate}
\end{theorem}

\begin{proof}
    (samo zvezni primer)
    \begin{enumerate}[label=\alph*]
        \item \[E(X) = \int_{-\infty}^{\infty} |x| p_x(x) dx \geq \int_{\{x: |x| \geq a\}} |x| p_x(x) dx \geq\]
            \[|a| \int_{\{x: |x| \geq a\}} p_x(x) dx = a \cdot P(|X| \geq a)\]
        \item \[P((X - E(X)) \geq \epsilon) = P((X - E(X))^2 \geq \epsilon^2)
            \stackrel{\text{(a) za X-E(X)}}{\leq} \frac{E((X-E(X))^2)}{\epsilon^2} = \frac{D(X)}{\epsilon^2}\]
    \end{enumerate}
\end{proof}

\begin{theorem}[Markov]
    "Ce za zaporedje slu"cajnih spremenljivk $\{X_n\}_{n \in \N}$ velja $\frac{D(S_n)}{n^2} \stackrel{n \to \infty}{\rightarrow} 0$,
    potem velja "SZV"S. Tukaj je $S_n := X_1 + \cdots + X_n$
\end{theorem}

\begin{proof}
    V neenakosti "Cebi"seva vzamemo $X = \frac{S_n}{n}$
    \[P(\frac{|S_n - E(S_n)|}{n} \geq \epsilon) \leq \frac{P(S_n)}{n^2 \epsilon^2} \stackrel{n \to \infty}{\rightarrow} 0\]
    "Ce vzamemo $Y_n = \frac{|S_n - E(S_n)|}{n}$, je $P(|Y_n| \geq \epsilon) \stackrel{n \to \infty}{\rightarrow} 0$ \\
    oz. $P(|Y_n| < \epsilon) \stackrel{n \to \infty}{\rightarrow} 1$ \\
    Zato $Y_n \stackrel{n \to \infty}{\rightarrow} 0$ verjetnostno, torej velja "SZV"S za zaporedje $\{X_n\}_{n \in \N}$
\end{proof}

\begin{conseq}[Izrek "Cebi"sev]
    "Ce so $X_1, X_2 \cdots X_n$ paroma nekorelirane slu"cajne spremenljivke in $\sup_{n \in \N} D(X_n) < \infty$, potem
    za $\{X_n\}_{n \in \infty}$ velja "SVZ"S
\end{conseq}

\begin{proof}
    Ker je $D(S_n) = D(X_1) + \cdots + D(X_n) \leq n \cdot c$, je $\frac{D(S_n)}{n^2} \leq \frac{n \cdot c}{n^2}
    = \frac{c}{n} \stackrel{n \to \infty}{\rightarrow} 0$, zato po izreku Markova velja "SZV"S
\end{proof}

\begin{ex}
    $X_n: \begin{pmatrix}0 & 1 \\ q & p\end{pmatrix}$ neodvisne slu"cajne spremenljivke, $D(X_n) = pq, E(X_n) = p,
    E(S_n) = n \cdot p$ \\
    Po izreku "Cebi"seva velja "SZV"S: $P(\frac{|S_n - E(S_n)|}{n} \geq \epsilon) \stackrel{n \to \infty}{\rightarrow} 0$
    \[\implies P(|\frac{S_n}{n} - p| \geq \epsilon) \stackrel{n \to \infty}{\rightarrow} 0\]
    $S_n$ je frekvenca dogodka, $\frac{S_n}{n}$ je relativna frekvenca, $\frac{S_n}{n} = \frac{X_1 + \cdots + X_n}{n}
    \stackrel{n \to \infty}{\rightarrow} p$ verjetnostno \\
    To je Bernoulijev zakon velikih "stevil iz 1713
\end{ex}

\begin{theorem}[Kolmogorov]
    "Ce za neodvisne slu"cajne spremenljivke $\{X_n\}_{n \in \N}$ velja $\sum_{n=1}^{\infty} \frac{D_n}{n^2} < \infty$,
    potem velja KZV"S, t.j. $P(\lim_{n \to \infty} \frac{S-n - E(S_n)}{n} = 0) = 1$. \\
    Posebej je pogoj za vrsto izpolnjen, "ce je $\sup_n D(X_n) < \infty$ 
\end{theorem}

\begin{ex}
    $X_n: \begin{pmatrix}0 & 1 \\ q & p\end{pmatrix}$ neodvisne slu"cajne spremenljivke, $D(X_n) = pq$ \\
    Po izreku Kolmogorova velja KVZ"S, t.j. $\frac{S_n}{n} = \frac{X_1 + \cdots + X_n}{n}
    \stackrel{n \to \infty}{\rightarrow} p$ skoraj gotovo. \\
    To posplo"suje Bernoullijev zakon
\end{ex}

\subsection{Centralni limitni izrek}

\begin{defn}
    Naj bo $\{X_n\}_{n \in \N}$ zaporedje slu"cajnih spremenljivk s kon"cnimi disperzijami. Definiramo
    $S_n := X_1 + \cdots + X_n$ in standardizirajmo: $Z_n = \frac{S_n - E(S_n)}{\sigma(S_n)}$, torej
    $E(Z_n) = 0, D(Z_n) = 1$ \\
    Za $\{X_n\}_{n \in \N}$ velja centralni limitni izrek, "ce je $F_{Z_n}(x) = P(Z_n \leq x)
    \stackrel{n \to \infty}{\rightarrow} F_{N(0,1)} \forall x \in \R$, t.j.
    \[P(\frac{S_n - E(S_n)}{\sigma(S_n)} \leq x) \stackrel{n \to \infty}{\rightarrow}
    \frac{1}{2 \pi} \int_{-\infty}^x e^{-\frac{t^2}{2}} dx \text{ za } \forall x \in \R\]
    Pracimo, da $\{Z_n\}_{n \in \N}$ po porazdelitvi konvergira proti standardizirani normalni porazdelitvi.
\end{defn}

\begin{theorem}[Centralni limitni izrek (CLI, osnovna verzija)]
    Naj bodo $X_1, X_2 \cdots$ neodvisne in enako porazdeljene slu"cajne spremenljivke. Potem zanje velja centralni limitni
    zakon, t.j
    \[P(\frac{S_n - E(S_n)}{\sigma(S_n)} \leq x) \stackrel{n \to \infty}{\rightarrow}
    \int_{-\infty}^x e^{\frac{t^2}{2}} dx \text{ za } \forall x \in \R\]
\end{theorem}

Dokazal je Ljapunov (1900), s tem je posplo"sil Laplaceov izrek iz leta 1812. V dokazu bomo uporabili

\begin{theorem}[O zveznosti rodovne funkcije]
    Naj za zaporedje $\{Z_n\}_{n \in \N}$ slu"cajnih spremenljivk velja: \\
    $M_{Z_n}(t) \rightarrow M_{N(0,1)}(t) = e^{\frac{t^2}{2}}$ za vse $t \in (-\delta,\delta)$ pri nekem $\delta > 0$ \\
    Potem $F_{Z_n}(x) \rightarrow F_{N(0,1)}(x)$ za $\forall x \in \R$
\end{theorem}

\begin{proof}
    CLI v primeru, ko $X_n$ imajo momentno rodovno funkcijo \\
    $M_X(t) = E(e^{t X_n})$ na neki okolici to"cke 0 \\
    Naj bo $E(X_n) = \mu, D(X_n) = \sigma^2$ in $U_n := X_n - \mu = X_n - E(X_n)$. Torej je $E(U_n) = 0$ in
    $D(U_n) = \sigma^2$ ter $M_{U}(t) = 1 + t E(U_n) + \frac{t^2}{2!} E(U_n^2) + o(t^2) =$ \\
    $= 1 + \frac{t^2}{2} \sigma^2 + o(t^2)$ ($\lim_{n \to \infty} \frac{o(n)}{n} = 0$) \\
    Ker je $D(S_n) \stackrel{\text{neodvisne}}{=} D(X_1) + \cdots + D(X_n) = n \cdot \sigma^2$ in
    $E(S_n) = n \cdot \mu = E(X_1) + \cdots + E(X_n)$, je $Z_n = \frac{S_n - E(S_n)}{\sigma(S_n)} =$ \\
    $= \frac{1}{\sigma \sqrt{n}}$ ($\sum_{n=0}^{n} U_i$) \\
    Potem je $M_{Z_n}(t) = E(e^{t Z_n}) = E(e^{\frac{t}{\sigma \sqrt{n}}(U_1 + \cdots + U_n)}) =
    E(e^{\frac{t}{\sigma \sqrt{n}} U_1}) \cdot \cdots \cdot E(e^{\frac{t}{\sigma \sqrt{n}} U_n}) =$ \\
    $\stackrel{\text{enaki}}{=} (M_U(\frac{t}{\sigma \sqrt{n}}))^n = (1 + \frac{t^2}{2n} + o(\frac{1}{n}))^n$ \\
    $\stackrel{n \to \infty \equiv o(\frac{1}{n} \to 0)}{\rightarrow} e^{\frac{t^2}{2}}$
    \begin{lemma}
        "Ce $X_n \to X$, potem $(1 + \frac{X_n}{n})^n \stackrel{n \to \infty}{\rightarrow} e^x$
    \end{lemma}
    Po prej"snjem izreku: $F_{Z_n}(x) \stackrel{n \to \infty}{\rightarrow} F_{N(0,1)}(x)$



% 19. predavanje: 14.3.

    \begin{equation*}
        \epsilon > 0: x - \epsilon \leq x_n \leq x + \epsilon \text{ za dovolj velik n}
    \end{equation*}

    \begin{align*}
        &\implies (1 + \frac{x-\epsilon}{n})^n \leq (1 + \frac{x_n}{n})^n \leq (1 + \frac{x+\epsilon}{n})^n \\
        &\implies (1 + \frac{x-\epsilon}{n})^n \to e^{x-\epsilon} \\
        &\implies (1 + \frac{x_n}{n})^n \to e^{x} \\
        &\implies (1 + \frac{x+\epsilon}{n})^n \to e^{x+\epsilon} \\
    \end{align*}
\end{proof}

V splo"snem se CLI doka"ze s pomo"cjo karakteristi"cnih funkcij: \\
naj bo X slu"cajna spremenljivka, $\ell_X(t) := E(e^{itX}) = E(cos(tX)) + iE(sin(tX)) t \in \R$ \\
za razliko od momentno rodovnih funkcij karakteristi"cne funkcije vedno odstajajo \\
v zveznem primeru je $\int_{-\infty}^{\infty} e^{itx}p(x)dx$ - Fourierova transformacija funkcije $p_X(x)$ \\
$X_1, X_2 \cdots X_n$ neodvisne, enako porazdeljene

\begin{align*}
    &\mu := E(X_n), \sigma := \sigma(X_n) \\
    &E(S_n) \stackrel{\text{neodvisnost}}{=} E(X_1) + \cdots + E(X_n) = n \mu \\
    &D(S_n) \stackrel{\text{neodvisnost}}{=} D(X_1) + \cdots + D(X_n) = n \sigma^2
\end{align*}

$X_1, X_2 \cdots X_n$ neodvisne slu"cajne spremenljivke

\begin{align*}
    &Z_n = \frac{S_n - E(S_n)}{\sigma(S_n)} = \frac{S_n - n \mu}{\sqrt{n} \sigma} =
        \frac{\frac{S_n}{n} - \mu}{\frac{\sigma}{\sqrt{n}}} \\
    &\overline{Z_n} := \frac{S_n}{n} = \frac{X_1 + \cdots + X_n}{n} \implies
        Z_n = \frac{\overline{X} - \mu}{\frac{\sigma}{\sqrt{n}}}
\end{align*}

Po CLI za velike n velja $Z_n \approx N(0,1)$, zato je $\overline{X} \approx N(\mu, \frac{\sigma}{\sqrt{n}})$ oz.
$S_n \approx N(n \mu, \sigma \sqrt{n})$ \\
"Ce so $X_1, X_2 \cdots$ porazdeljene normalno $N(\mu, \sigma)$, potem je $Z_n \sim N(0,1)$, torej
$F_{Z_n}(x) = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{x} e^{-\frac{t^2}{2}} dt$ \\

\begin{ex}
    Laplaceova formula je poseben primer CLI: \\
    $X_n: \begin{pmatrix}0 & 1 \\ q & p\end{pmatrix}, X_n = 1$ je dogodek, da se dogodek A (s $P(A) = p$) zgodi
    v n-ti ponovitvi poskusa, sicer je $X_n = 0$ \\
    $E(X_n) = p, S_n = X_1 + \cdots + X_n$ frekvenca dogodka A v prvih n ponovitvah \\
    $S_n \sim Bin(n,p), E(S_n) = np, D(S_n) = npq$, ker je $D(X_1) = pq$ \\
    $Z_n = \frac{S_n - E(S_n)}{\sigma(S_n)} = \frac{S_n - np}{\sqrt{npq}} \stackrel{\text{CLI}}{\approx} N(0,1)$,
    "ce je n velik \\
    \begin{align*}
        &P(S_n \leq X) = P(\frac{S_n - np}{\sqrt{npq}} \leq \frac{X - np}{\sqrt{npq}}) \approx \\
        &\approx \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\frac{x-np}{\sqrt{npq}}} e^{-\frac{t^2}{2}} dt = \\
        &= \frac{1}{2} + \Phi(\frac{x-np}{\sqrt{npq}})
    \end{align*}
    kjer je
    \begin{equation*}
        \Phi(x) := \frac{1}{\sqrt{2\pi}} \int_0^x e^{-\frac{t^2}{2}} dt
    \end{equation*}
    verjetnostni integral \\
    \begin{align*}
        &P(\alpha < S_n \leq \beta) = \\
        &= P(S_n \leq \beta) - P(S_n \leq \alpha) \approx \\
        &\approx \frac{1}{2} + \Phi(\frac{\beta - np}{\sqrt{npq}}) - \frac{1}{2} - \Phi(\frac{\alpha - no}{\sqrt{npq}}) = \\
        &= \Phi(\frac{\beta - np}{\sqrt{npq}}) - \Phi(\frac{\alpha - np}{\sqrt{npq}})
    \end{align*}
    Laplaceova aproksimacijska formula
\end{ex}

\begin{ex}
    Te"za vre"cke kostanja je porazdeljena pribli"zno normalno, saj je vsota te"z posameznih kostanjev, ki so neodvisne,
    enako porazdeljene slu"cajne spremenljivke \\
    $X_n \cdots$ te"za n-tega kostanja, $S_n = X_1 + \cdots + X_n \approx$ normalno - aditiven efekt
\end{ex}

\begin{ex}
    \begin{align*}
        &p_{X_n}(x) = \begin{cases}
            \frac{1}{2}; x \in [-1,1] \\
            0 \text{ sicer}
        \end{cases} \\
        &E(X_1) = 0, D(X_1) = \frac{(b-a)^2}{12} = \frac{1}{3} \\
        &S_1 = X_1, Z_1 = \frac{X_1 - E(X_1)}{\sigma(X_1)} = \frac{X_1}{\sqrt{\frac{1}{3}}} = x_1 \sqrt{3} \\
        &S_2 = X_1 + X_2 , Z_2 = \frac{S_2 - E(S_2)}{\sigma(S_2)} =
            \frac{X_1 + X_2 - E(X_1 + X_2)}{\sigma(X_1 + X_2)} \\
        &S_3 = X_1 + X_2 + X_3, Z_3 = \frac{S_3 - E(S_3)}{\sigma(S_3)}
    \end{align*}
\end{ex}

\section{Statistika}

\subsection{Osnovni pojmi}

Kot vedo statistiko razdelimo na:
\begin{enumerate}
    \item opisno statistiko: zbiranje, razvr"s"canje, prikazovanje podatkov, ra"cunanje osnovnih koli"cin
    \item analiti"cno statistiko: upraba podatkov pri sklepanju glede zakonitosti danega podro"cja
\end{enumerate}

\begin{defn}[Populacija]
    Populacija je kon"cna ali neskon"cna mno"zica elementov, pri katerih merimo ali opazujemo neko koli"cino
\end{defn}

\begin{ex} \text{} \\
    \begin{enumerate}[label=(\alph*)]
        \item kontrole kvalitete: populacija je mno"zica (serija) izdelka, npr. dnevna proizvodnja, merimo
            lastnosti izdelkov, npr. "zivljensko dobo
        \item testiranje seb: populacija je mno"zica vseh zaposlenih v dr"zavi, merimo npr. starost,
            vi"sino place $\cdots$
    \end{enumerate}
\end{ex}

Matemati"cni pogled: na verjetnostnem prostoru $(\Omega, \mathcal{F})$ imamo slu"cajno spremenljivko X. \\
Praviloma ne moremo izmeriti cele populacije, ampak meritve opravimo na relativno majhnem delu populacije,
na vzorcu. Le-ta mora biti reprezentativen, izbran nepristransko in dovolj velik. \\
Matemati"cni pogled: vzorec velikosti n je slu"cajni vektor $(x_1 \cdots x_n)$, kjer so komponente enako porazdeljene
kot slu"cajna spremenljivka X in med seboj neodvisne. \\
Vrednost tega slu"cajnega vektorja pri enem naboru n meritev je realizacija vzorca: $(x_1 \cdots x_n)$: to so
konkretni podatki, ki jih analiziramo. Pri opisni statistiki predstavimo in obdelamo te podatke. \\
Iz teh vzor"cnih podatkov "zelimo oceniti nekatere lastnosti populacije, kot sta:

\begin{enumerate}
    \item sredina populacije $\mu$, t.i. matemati"cno upanje slu"cajne spremenljivke X
    \item povpre"cni odklon $\sigma$ od sredine populacije, t.i. Standardna deviacija slu"cajne spremenljivke X
\end{enumerate}



% 20. predavanje: 21.3.

Ocene za $\mu$ so:

\begin{itemize}
    \item vzor"cno povpre"cje: $\overline{x} = \frac{x_1 + \cdots + x_n}{n}$
    \item vzor"cni modus: najpogostej"sa vrednost v vzorcu
    \item vzor"cna mediana: srednja vrednost v vzorcu, urejenem po velikosti
\end{itemize}

Ocene za $\sigma$ so:

\begin{itemize}
    \item vzor"cni razmak: razlika med najve"cjo in najmanj"so vrednostjo v vzorcu
    \item vzor"cna disperzija: $s_0^2 ? \frac{1}{n} \sum_{i=1}^n (x_i - \overline{x})^2$
    \item popravljena vzor"cna disperzija: $s^2 ? \frac{1}{n-1} \sum_{i=1}^n (x_i - \overline{x})^2 =
        \frac{n}{n-1} s_0^2$
\end{itemize}

\subsection{Vzor"cne statistike in cenilke}

\begin{defn}[Vzor"cna statistika]
    Naj bo $(X_1, X_2 \cdots X_n)$ vzorec t.i. slu"cajni vektor, kjer so $X_1 \cdots X_n$ enako porazdeljene
    kot slucajna spremenljivka $X$ in med seboj neodvisne. \\
    Vzor"cna statistika je simetri"cna funkcija vzorca $y = g(X_1, X_2 \cdots X_n)$, kjer je $g$ simetricna
    funkcije n spremenljivk
\end{defn}

Praviloma vzor"cna statistika ocenjuje vrednost nekega parametra $\xi$. Tedaj je y cenilka za parameter. \\
y je odvisna od n, zato pi"semo tudi $y_n = g(X_1 \cdots X_n)$. \\

\begin{defn}[Nepristranskost, doslednost]
    "Ce je $E(Y) = \xi$, je $Y$ nepristranska cenilka za parameter $xi$ \\
    Cenilka $Y=Y_n$ je dosledna, "ce $Y_n \stackrel{n \to \infty}{\to} \xi$
    verjetnostno, t.i. $\forall \epsilon > 0$ je $\lim_{n \to \infty} P(|Y_n - \xi| \geq \epsilon) = 0$ oz.
    $\lim_{n \to \infty} P(|Y_n - \xi| < \epsilon) = 1$
\end{defn}

\begin{defn}[Standardna napaka]
    Standardna napaka vzor"cne statistike $Y$ je standardna deviacija slu"cajne spremenljivke $Y$:
    $SE(Y) := \sigma(Y)$
\end{defn}

\begin{defn}[Vzor"cno povprecje]
    Naj bo $X$ slu"cajna spremenljivka na populaciji, ki ima matemati"cno upanje $E(X) = \mu$ in standardno
    deviacijo $\sigma(X) = \sigma$. Naj bo $(X_1 \cdots X_n)$ vzorec. Definirajmo vzor"cno povprecje
    \begin{equation*}
        \overline{X} = \frac{X_1 + \cdots + X_n}{n}
    \end{equation*}
    ki je vzor"cna statistika. \\
\end{defn}

Je cenilka za $\overline{X}$, ki je nepristranska:

\begin{equation*}
    E(\overline{X}) = \frac{1}{n} (E(X_1) + \cdots + E(X_n)) = \frac{1}{n} n \cdot \mu = \mu
\end{equation*}

Po "SZV"S (izreku "Cebi"seva) je to dosledna cenilka za $\mu$. \\
Ker je
\begin{equation*}
    D(\overline{X}) \stackrel{\text{neodv}}{=} \frac{1}{n^2} \sum_{i=1}^n D(X_i) =
    \frac{1}{n^2} n \cdot \sigma^2 = \frac{\sigma^2}{n}
\end{equation*}

je standardna napaka

\begin{equation*}
    SE(Y) = \frac{\sigma}{\sqrt{n}}
\end{equation*}

- "cim vecji n, bolje oceni parameter $\mu$ \\
Po CLI je pri velikem n slu"cajna spremenljivka $Z_n := \frac{S - n \mu}{\sigma \sqrt{n}} =
\frac{\overline{X} - \mu}{\frac{\sigma}{\sqrt{n}}} = \frac{\overline{X} - \mu}{\sigma} \sqrt{n}$
porazdeljena pribli"zno $N(0,1)$ oz. $\overline{X}$ je porazdeljen pribli"zno $N(\mu, \frac{\sigma}{\sqrt{n}})$ \\
"Ce je $X$ normalno porazdeljena $N(\mu, \sigma)$, potem je $\overline{X}$ porazdeljen
$N(\mu, \frac{\sigma}{\sqrt{n}})$ za vsak $n$

\begin{claim}
    Naj bo $Y_n$ cenilka za $\xi$. "Ce je $E(Y_n) \stackrel{n \to \infty}{\to} \xi$ in
    $D(Y_n) \stackrel{n \to \infty}{\to} 0$, potem je $Y=Y_n$ dosledna cenilka za $\xi$
\end{claim}

\begin{proof}
    Fiksirajmo $\epsilon > 0$. Dokazati moramo $\lim_{n \to \infty} P(|Y_n - \xi| \geq \epsilon) = 0$ \\
    Ker je $E(Y_n) \stackrel{n \to \infty}{\xi}$, obstaja $n_0 \in \N$: $|E(Y_n) - \xi| < \frac{\epsilon}{2}$
    zato je dogodek
    \begin{align*}
        &(|Y_n - \xi| \geq \epsilon) \subseteq (|Y_n - E(Y_n)| + |E(Y_n) - \xi| \geq \epsilon) \text{ za }
            \forall n \subseteq\\
        &\stackrel{n \geq n_0}{\subseteq} (|Y_{n_0} - E(Y_{n_0})| + |E(Y_{n_0}) - \xi| \geq \epsilon)
    \end{align*}
    Torej je za $n \geq n_0$
    \begin{equation*}
        P(|Y_n - \xi| \geq \epsilon) \leq P(|Y_n - E(Y_n)| \geq  \frac{\epsilon}{2})
        \leq \frac{D(Y_n)}{\epsilon^2} \cdot 4 \stackrel{n \to \infty}{\to} 0 \text{ (doslednost)}
    \end{equation*}
    Neenakost "Cebi"seva: $P(|X - E(X)| \geq \epsilon) \leq \frac{D(X)}{\epsilon^2}$ \\
    Tako imamo doslednost cenilke: $P(|Y_n - \xi| \geq \epsilon) \stackrel{n \to \infty}{\to} 0$
\end{proof}

\begin{ex}
    Porazdelitev $\chi^2$, n "stevilo prostorskih stopenj \\
    \begin{equation*}
        p(X) = \begin{cases}
            \frac{1}{2^{\frac{n}{2}} \gamma(\frac{n}{2})} x^{\frac{n}{2}-1} e^{-\frac{x}{2}} x > 0 \\
            0 \text{ sicer}
        \end{cases}
    \end{equation*}
    Modus = $n-2$, $E(X) = n$, $D(X) = 2n$ \\
    Mediana $\approx n \cdot (1 - \frac{2}{9n})^3$
\end{ex}

\begin{defn}[Vzorcna disperzija]
    Naj bo X slu"cajna spremenljivka na populaciji. Vzor"cna disperzija je definirana s
    \begin{equation*}
        s_0^2 = \frac{1}{n} \sum_{i=1}^n (x_i - \overline{x})^2
    \end{equation*}
    popravljena vzor"cna disperzija pa je
    \begin{equation*}
        s^2 = \frac{1}{n-1} \sum_{i=1}^{n} (x_i - \overline{x})^2
    \end{equation*}
\end{defn}

Kako sta porazdeljeni, "ce je $X \sim N(\mu, \sigma)$? \\
Raje vzemimo vzor"cno statistiko: $\chi^2 := \frac{1}{\sigma^2} \sum_{i=1}^n (x_i - \overline{x})^2 =
\frac{n}{\sigma^2} s_0^2 = \frac{n-1}{\sigma^2} s^2$ \\
Ni lahko izra"cunati, da je $\chi^2 \sim \chi^2(n-1)$ \\
Ideja izpeljave je $\chi^2 = Z_1^2 + \cdots + Z_{n-1}^2$ za $Z_i \sim N(0,1)$ in med seboj neodvisne.
Potem uporabimo trditev iz verjetnosti: $Z_i^2 \sim \chi^2(1)$, torej $E(\chi^2) = n-1$, $D(\chi^2) = 2(n-1)$.
Od tod sledi
\begin{equation*}
    E(s_0^2) = E(\frac{\sigma^2}{n} \chi^2) = \frac{\sigma^2}{n} E(\chi^2) = \frac{n-1}{n} \sigma^2
\end{equation*}
torej $s_0^2$ ni nepristranska za $\sigma^2$, je pa asimptoti"cno nepristranska, t.i.
$E(s_0^2) \stackrel{n \to \infty}{\to} \sigma^2$ \\
Podobno je $E(s^2) = \frac{\sigma^2}{n-1} E(\chi^2) = \sigma^2$, torej je $s^2$ nepristranska cenilka za
$\sigma^2$ \\
Ker je $D(s_0^2) = \frac{\sigma^4}{n^2} D(\chi^2) = \frac{\sigma^4 2(n-1)}{n^4} \stackrel{n \to \infty}{\to} 0$
in $D(s^2) = \frac{2 \sigma^4}{(n-1)^2} \stackrel{n \to \infty}{\to} 0$, iz trditve sledi, da sta $s_0^2$ in
$s^2$ dosledni cenilki za $\sigma^2$



% 21. predavanje: 28.3.

\subsubsection*{Studentova t-porazdelitev}

\begin{equation*}
    p(x) = \frac{1}{\sqrt{n} B(\frac{n}{2},\frac{1}{2})} (1 + \frac{x^2}{n})^{-\frac{n+1}{2}}
\end{equation*}
kjer je $B(p,q) = \frac{\Gamma(p) \Gamma(q)}{\Gamma(p+q)}$ Beta funkcija

\begin{align*}
    n = 1: &\quad \frac{1}{\pi} (1+x^2)^{-1} = \frac{1}{\pi (1+x^2)} \text{Cauchyjeva porazdelitev} \\
    &\text{ko gre } n \to \infty, \text{ gre } \sqrt{n} B(\frac{n}{2},\frac{1}{n}) \to \sqrt{2 \pi}
        \text{ in } (1 + \frac{x^2}{n})^{-\frac{n-1}{2}} = ((1 + \frac{x^2}{n})^n)^{-\frac{n+1}{2n}}
        \to e^{-\frac{x^2}{2}} \\
    &\text{torej je pri velikih n gostota pribli"zno } N(0,1) \\
    n = 2: &\quad \frac{1}{\sqrt{2} B(1,\frac{1}{2})} (1 + \frac{x^2}{2})^{-\frac{3}{2}} \\
    &\text{za } n \geq 2 \text{ je } E(X) = 0 \\
    n = 3: &\quad c \cdot (1 + \frac{x^2}{2})^{-2} \approx \frac{1}{x^4} \text{ za velike } x \\
    &\text{za } n \geq 3 \text{ je } D(X) = \frac{n}{n-2} > 1
\end{align*}

Leta 1908 jo je odkril W.S. Gosset, statistik v pivovarni guiness v Dublinu. Student je njegov prevdonim.

\subsubsection*{}

Pri normalni porazdelitvi slu"cajne spremenljivke $X \sim N(\mu, \sigma)$ je vzor"cno povpre"cje  $\overline{X}$
porazdeljeno $N(\mu, \frac{\sigma}{\sqrt{n}}), \overline{X} = \frac{X_1 + \cdots + X_n}{n}$, torej je
$Z := \frac{\overline{X} - \mu}{\frac{\sigma}{\sqrt{n}}} = \frac{\overline{X} - \mu}{\sigma} \sqrt{n}$
porazdeljena N(0,1). "Ce poznamo $\sigma$, potem bomo znali povedati, kako dobra ocena za $\mu$ je $\overline{X}$
($\to$ intervali zaupanja). \\

Kako ravnati, "ce $\sigma$ ne poznamo? \\
Lahko jo ocenimo s $s = \sqrt{\frac{1}{n-1} \sum_{i=1}^n (X_i - \overline{X})^2}$, tako da potem vzor"cna
statistika $T = \frac{\overline{X} - \mu}{s} \sqrt{n}$ ni ve"c porazdeljena po $N(0,1)$, niti pribli"zno
normalna, razen "ce je n velik in je s potem skoraj konstanta $\sigma$. \\

Kako je porazdeljena vzor"cna statistika T? \\
Ker je $\chi^2 = \frac{1}{\sigma^2} \sum_{i=1}^n (X_i - \overline{X})^2 = \frac{(n-1) S^2}{\sigma^2}$, je
$\frac{Z}{T} = \frac{S}{\sigma} = \sqrt{\frac{\chi^2}{n-1}}$, torej je $T = \frac{Z}{\sqrt{\frac{\chi^2}{n-1}}}$ \\
Izka"ze se, da sta $Z \sim N(0,1)$ in $\chi^2 \sim \chi^2(n-1)$ neodvisni slu"cajni spremenljivki. Od tod lahko
izra"cunamo, da ima $T$ Studentovo porazdelitev z $n-1$ prostorskimi stopnjami:
\begin{equation*}
    p_T(t) = \frac{1}{(n-1) B(\frac{n-1}{2}, \frac{1}{2})} \cdot \frac{1}{(1 + \frac{x^2}{n-1})^{\frac{n}{2}}}
\end{equation*}

\subsection{Metode za pridobivanje cenilk}

\subsubsection{Metoda momentov}

\begin{defn}[Vzro"cni moment]
    Naj bo $(X_1, X_2 \cdots X_n)$ vzorec velikosti n, torej $X_1 \cdots X_n$ neodvisne slu"cajne spremenljivke,
    porazdeljene kot slu"cajna spremenljivka $X$. Zacetni moment reda $k$ je $z_k = E(X^k)$. Definiramo k-ti
    vzro"cni moment $z_k := \frac{X_1^k + \cdots + X_n^k}{n}$. Le ta je nepristranska cenilka za $z_k:
    E(Z_k) = \frac{1}{n} (E(X_1^k) + \cdots + E(X_n^k)) = z_k$. $Z_k$ je tudi dosledna cenilka za $z_k$.
\end{defn}

Naj bo gostota slu"cajne spremenljivke $X$ odvisna od parametrov $\xi_1 \cdots \xi_n: p(X; \xi_1 \cdots \xi_m)$.
Naj odstajajo za"cetni momenti $z_k = E(X^k) = \int_{-\infty}^{\infty} p(x; \xi_1 \cdots \xi_n) dx, k = 1, 2 \cdots m$.
Denimo, da iz teh m ena"cb lahko izrazimo parametre: $\xi_k = \phi_k(z_1, z_2 \cdots z_m), k = 1 \cdots m$ za neko
funkcijo $\phi_k$. Potem je $c_k := \phi_k(z_1 \cdots z_m)$ cenilka za parameter $\xi_k, k = 1 \cdots n$

\begin{ex}
    Naj bo $X \sim N(\mu, \sigma)$, kjer sta $\mu$ in $\sigma$ neznana parametra. Potem je $z_1 = E(X) = \mu,
    z_2 = E(X^2) = E(X^2) - (E(X))^2 + (E(X))^2 = D(X) + (E(X))^2 = \sigma^2 + \mu^2$ (m = 2) \\
    Iz teh dveh ena"cb izrazimo parametra $\mu$ in $\sigma$: $\mu = z_1, \sigma^2 = z_2 - \mu^2 = z_2 - z_1^2$. \\
    Cenilka za $\mu$ je $Z_1 = \overline{X} = \frac{X_1 + \cdots + X_n}{n}$, cenilka za $\sigma^2$ je
    $Z_2 - Z_1^2 = \frac{X_1^2 + \cdots + X_n^2}{n} - \overline{X}^2$. To je enako
    \begin{align*}
        &S_0^2 = \frac{1}{n} \sum_{i=1}^n (X_i - \overline{X})^2 = \\
        &= \frac{1}{n} \sum_{i=1}^n (X_i^2 - 2 X_i \overline{X} + \overline{X}^2) = \\
        &= \frac{1}{n} \sum_{i=1}^n X_i^2 - 2 \overline{X} \overline{X} + \overline{X}^2 = \\
        &= \frac{1}{n} \sum_{i=1}^2 X_i^2 - \overline{X}^2
    \end{align*}
    Torej bodimo "ze znani cenilki za parametra $\mu$ in $\sigma^2$
\end{ex}

\begin{ex}
    Naj bo $X$ porazdeljena enakomerno na $[a,b]$, kjer sta $a$ in $b$ neznana parametra. I"s"cemo cenilki za $a$
    in $b$. Po metodi momentov moramo izra"cunati 2 za"cetna momenta
    \begin{align*}
        &z_1 = E(X) = \frac{a+b}{2} \\
        &z_2 = E(X^2) = \int_{-\infty}^{\infty} x^2 p(x; a, b) dx = \frac{1}{b-a} \int_{a}^{b} x^2 dx = \\
        &= \frac{1}{b-a} \frac{x^3}{3} \vert_a^b = \frac{b^3-a^3}{3(b-a)} = \frac{b^2 + ab + a^2}{3}
    \end{align*}
    Iz 1. ena"cbe dobimo $b = 2z_1 - a$, kar vstavimo v 2. ena"cbo
    \begin{align*}
        &3z_2^2 = b^2 + ab + a^2 = 4z_1^2 - 4z_1 a + a^2 + 2a z_1 - a^2 + a^2 \\
        &\implies 3z_2 = 4z_1^2 - 2z_1 a + a^2 \\
        &a^2 - 2a z_1 + (4z_1^2 - 3z_2) = 0 \\
        &\quad D = 4z_1^2 - 4(4z_1^2 - 3z_2) = 12(z_2 - z_1^2) \\
        &a_{1,2} = \frac{1}{2} (2z_1 \pm \sqrt{D}) = z_1 \pm \frac{1}{2} 2\sqrt{3} \sqrt{z_2 - z_1^2} =
            z_1 \pm \sqrt{3} \sqrt{z_2 - z_1^2}
    \end{align*}
    Ker je $a < b$, je torej
    \begin{align*}
        &a = z_1 - \frac{1}{2} 2\sqrt{3} \sqrt{z_2 - z_1^2} \\
        &b = z_1 + \frac{1}{2} 2\sqrt{3} \sqrt{z_2 - z_1^2}
    \end{align*}
    Cenilka za a je
    \begin{align*}
        &A := Z_1 \pm \frac{1}{2} 2\sqrt{3} \sqrt{Z_2 - Z_1^2}
        &A := Z_1 \pm \frac{1}{2} 2\sqrt{3} \sqrt{Z_2 - Z_1^2} = Z_1 - S_0 \sqrt{3} \text{ po prej"snjem primeru}
            = \overline{X} - S_0 \sqrt{3}
    \end{align*}
    Cenilka za b je $B = \overline{X} + S_0 \sqrt{3}$
    Denimo da imamo konkreten vzorec $-2, 0, 1, 2, 4 (n=5)$ \\
    $\overline{X} = \frac{-2 + 0 + 1 + 2 + 4}{5} = 1$ \\
    $S_0^2 = \frac{1}{n} \sum_{i=1}^n (X_i - \overline{X})^2 = \frac{1}{5} ((-3)^2 + (-1)^2 + 0^2 + 1^2 + 3^2) = 4$
    Vzor"cna vrednost za $A$ je $\overline{X} - S_0 \sqrt{3} = 1 - 2\sqrt{3} = \doteq -2.46$, vzor"cna
    vrednost z $B$ je $\overline{X} + S_0 \sqrt{3} = 1 + 2\sqrt{3} \doteq 4.46$
\end{ex}



% 22. predavanje: 4.4.

\subsubsection{Metoda maksimalne zanesljivosti (oz. najve"cjega verjetja)}

\begin{defn}[Funkcija zanesljivosti]
    Naj bo gostota slu"cajne spremenljivke $X$ odvisna od parametra $\xi$, torej $p(x; \xi)$. Funkcija
    zanesljivosti (likelihood function) je
    \begin{equation*}
        L(x_1 \cdots x_n; \xi) = p(x_1; \xi) \cdot \cdots \cdot p(x_n; \xi)
    \end{equation*}
\end{defn}

Pri danih $x_1 \cdots x_n$ izberimo tak $\xi_{max}$, da ima $L$ tam maksimum. Ta vrednost parametra je odvisna od
$x_1 \cdots x_n$, torej $\xi_{max} = \phi(x_1, x_2 \cdots x_n)$ za neko funkcijo $\phi$. Tako dobimo cenilko
$c := \phi(x_1 \cdots x_n)$ za parameter $\xi$

\begin{ex}
    \begin{equation*}
        p(x; \lambda) := \begin{cases}
            \lambda e^{-\lambda x} \; x > 0 \\
            0 \qquad x < 0
        \end{cases}
    \end{equation*}
    $\lambda$ je neznan parameter, ki ga ocenjujemo
    \begin{equation*}
        L(x_1 \cdots x_n; \lambda) = \lambda e^{-\lambda x_1} \cdot \cdots \cdot \lambda e^{-\lambda x_n} =
        \lambda^n e^{-(x_1 + \cdots + x_n)}
    \end{equation*}
    Poiskati moramo $\lambda_{max}$, pri katerem je dose"zen maksimum funkcije $L$ (oz. maksimum funkcije $\ln(L)$)
    \begin{align*}
        &\ln L(x_1 \cdots x_n; \lambda) = n \cdot \ln \lambda - \lambda \sum_{i=1}^{n} x_i \\
        &\frac{\partial}{\partial \lambda} (\ln L(x_1 \cdots x_n; \lambda)) = \frac{n}{\lambda} -
            \sum_{i=1}^{n} x_i = 0 \\
        &\quad \implies \lambda_{max} = \frac{n}{\sum_{i=1}^n x_i} = \frac{1}{\overline{x}}
    \end{align*}
    Ker je $\frac{\partial^2}{\partial \lambda^2} \ln L(x_1 \cdots x_n; \lambda) = -\frac{n}{\lambda^2} < 0$,
    je v $\lambda_{max}$ maksimum. \\
    Cenilka za $\lambda$ je $c := \frac{1}{\overline{X}}$ \\
    Isto cenilko dobimo z metodo momentov:
    \begin{align*}
        &z_1 = E(X) = \frac{0}{\infty} x \lambda e^{-\lambda x} dx = \stackrel{\text{D.N.}}{\cdots} \frac{1}{\lambda} &
        &\implies \lambda = \frac{1}{z_1} = \frac{1}{\overline{x}}
    \end{align*}
    cenilka za $\lambda$ je $c := \frac{1}{\overline{X}}$
\end{ex}

\begin{ex}
    $X \sim N(\mu, \sigma)$, $\mu, \sigma$ neznana parametra, ki ju ocenjujemo
    \begin{align*}
        &L(x_1 \cdots x_n; \mu, \sigma) := \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2} (\frac{x_1-\mu}{\sigma})^2} \cdot
            \cdots \cdot \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2} (\frac{x_n-\mu}{\sigma})^2} = \\
        &= \frac{1}{(2\pi)^{\frac{n}{2}}} \cdot \frac{1}{\sigma^n} e^{-\frac{1}{2 \sigma^2}
            (x_1 - \mu)^2 + \cdots + (x_n - \mu)^2} \\
        &\ln L = -\frac{n}{2} \ln 2\pi - n \cdot \ln \sigma - \frac{1}{2 \sigma^2} ((x_1 - \mu)^2 + \cdots +
            (x_n - \mu)^2) \\
        &\frac{\partial}{\partial \mu} \ln L = -\frac{1}{2 \sigma^2} (2(x_1 - \mu)(-1) + \cdots +
            2(x_1 - \mu)(-1)) = \frac{1}{\sigma^2} (x_1 - \mu + \cdots + x_n - \mu) = 0 \\
        &x_1 + \cdots + x_n - n\mu = 0 \implies \mu = \frac{x_1 + \cdots + x_n}{n} = \overline{x} \\
        &\frac{\partial}{\partial \sigma} \ln L = -\frac{n}{\sigma} + \frac{1}{\sigma^3}
            ((x_1 - \mu)^2 + \cdots + (x_n - \mu)^2) = 0 \\
        &\implies \sigma^2 = \frac{1}{n} ((x_1 - \mu)^2 + \cdots + (x_n - \mu)^2) = \\
        &= \frac{1}{n} ((x_1 - \overline{x})^2 + \cdots + (x_n - \overline{x})^2) = s_0^2
    \end{align*}
    Cenilka za $\mu$ je $\overline{X}$, cenilka za $\sigma^2$ je $S_0^2 =
    \frac{1}{n} \sum_{i=1}^{n} (X_i - \overline{X})^2$
\end{ex}

\begin{ex}
    $Bin(1,p) = Ber(p), X: \begin{pmatrix}0 & 1 \\ q & p\end{pmatrix} q = 1-p, p$ neznan parameter
    \begin{align*}
        &P(X=x) = p^x (1-p)^{1-x} x \in \{0, 1\} \\
        &L(x_1 \cdots x_n; p) = p^{x_1} (1-p)^{1-x_1} \cdot \cdots \cdot p^{x_n} (1-p)^{1-x_n} = \\
        &= p^{x_1 + \cdots + x_n} (1-p)^{n - (x_1 + \cdots + x_n)} \\
        &x := x_1 + \cdots + x_n \implies L(x_1 \cdots x_n; p) = p^x (1-p)^{1-x} x \in \{0, 1 \cdots n\} \\
        &\ln L = x \ln p + (n-x) \ln(1-p) \\
        &\frac{\partial}{\partial p} \ln L = \frac{x}{p} - \frac{n-x}{1-p} = 0 \\
        &\implies x(1-p) = (n-x)p \implies x - xp = np - xp \implies p = \frac{x}{n} = \overline{x}
    \end{align*}
    Cenilka za $p$ je $P := \overline{X} = \frac{X_1 + \cdots + X_n}{n}$ \\
    Ker je
    \begin{equation*}
        E(P) = \frac{1}{n} (E(X_1) + \cdots + E(X_n)) = p
    \end{equation*}
    je $P$ nepristranska cenilka \\
    Ker je
    \begin{equation*}
        D(P) = \frac{1}{n^2} (D(X_1 + \cdots + D(X_n))) = \frac{1}{n^2} n D(X_1) =
        \frac{1}{n} D(X_1) \stackrel{n \to \infty}{\to} 0
    \end{equation*}
    po trditvi sledi, da je $\overline{X}$ dosledna cenilka za $P$
\end{ex}

\subsection{Intervalsko ocenjevanje parametrov}

\begin{defn}[Interval zaupanja]
    Naj bo gostota slu"cajne spremenljivke $X$ odvisna od parametra $\xi$. Interval $[A,B]$ (odvisen le od
    $(x_1 \cdots x_n)$ in ne do $\xi$) je interval zaupanja za parameter $\xi$, pri stopnji tveganja
    $\alpha \in (0,1)$, "ce je
    \begin{equation*}
        P(\xi \in [A,B]) = 1-\alpha \text{ oz. } P(\xi \notin [A,B]) = \alpha
    \end{equation*}
\end{defn}

Za $\alpha$ obi"cajno vzamemo vrednost $0.05$ (ali $0.01$) \\
$A$ in $B$ sta vzor"cni statistiki, $1 - \alpha$ je stopnja zaupanja

\begin{ex}
    $X \sim N(\mu, \sigma), \sigma$ poznamo, $\mu$ pa je neznan parameter. \\
    Slu"cajna spremenljivka $Z := \frac{\overline{X} - \mu}{\sigma} \sqrt{n} \sim N(0,1)$ \\
    Pri dani stopnji tveganja $\alpha$ najdemo $z_{\frac{\alpha}{2}} > 0$, da je
    $P(-z_{\frac{\alpha}{2}} < Z < z_{\frac{\alpha}{2}}) = 1-\alpha$ oz.
    $P(|Z| > z_{\frac{\alpha}{2}}) = \alpha$ oz. $P(Z > z_{\frac{\alpha}{2}}) = \frac{\alpha}{2}$ \\
    Pogoj $|Z| < z_{\frac{\alpha}{2}}$ pomeni: $|\overline{X} - \mu| < z_{\frac{\alpha}{2}} \cdot
    \frac{\sigma}{\sqrt{n}}$ \\
    \begin{align*}
        &A := \overline{X} - Z_{\frac{\alpha}{2}} \frac{\sigma}{\sqrt{n}} < \mu <\\
        &< \overline{X} + Z_{\frac{\alpha}{2}} \frac{\sigma}{\sqrt{n}} =: B
    \end{align*}
    $[A,B]$ je interval zaupranja za $\mu$ pri stopnji tveganja $\alpha$ \\
    Konkreten zgled: imejmo vzorec velikosti $n=36$, za katerega izra"cunamo $\overline{x} = 2.6$ in
    $s = \sqrt{\frac{1}{n-1} \sum_{i=1}^{n} (x_i - \overline{x})^2} = 0.3$. Predpostavimo, da imamo
    $X \sim N(\mu, \sigma)$ in predpostavimo, da je $\sigma := s = 0.3$ (kar pogosto naredimo, "ce je
    n razmeroma velik). Vzemimo $\alpha = 0.05$. Iz tabele razberemo $z_{\frac{\alpha}{2}} = 1.96$, torej
    $P(Z > z_{\frac{\alpha}{2}}) = \frac{\alpha}{2}$. Tedaj je vzor"cna vrednost za $A$ enaka
    \begin{equation*}
        \overline{x} - z_{\frac{\alpha}{2}} \frac{\sigma}{\sqrt{n}} = 2.6 - 1.96 \frac{0.3}{\sqrt{36}} = 2.5
    \end{equation*}
    vzor"cna vrednost za $B$ je $\overline{x} - z_{\frac{\alpha}{2}} \frac{\sigma}{\sqrt{n}} = 2.7$ \\
    Interval zaupanja za $\mu$ je $[2.5, 2.7]$, t.j.
    \begin{equation*}
        P(\mu \in [2.5, 2.7]) = 1 - \alpha = 0.95
    \end{equation*}
\end{ex}

\begin{ex}
    $X \sim N(\mu, \sigma)$, $\mu$ in $\sigma$ sta neznana. \\
    I"s"cemo interval zaupanja za $\mu$. \\
    Slu"cajna spremenljivka $T := \frac{\overline{X} - \mu}{\sigma} \sqrt{n} \sim Student(n-1)$ \\
    Pri danem tveganju $\alpha$ izberemo $t_{\frac{\alpha}{2}} > 0$, da je $P(|T| < t_{\frac{\alpha}{2}}) =
    1 - \alpha$ oz. $P(T > t_{\frac{\alpha}{2}}) = \frac{\alpha}{2}$ \\
    Sedaj imamo podobno situacijo kot v primeru 1. \\
    Pogoj $|T| < t_{\frac{\alpha}{2}}$ pomeni
    \begin{equation*}
        A := \overline{X} - t_{\frac{\alpha}{2}} \frac{\sigma}{\sqrt{n}} < \mu <
        \overline{X} + t_{\frac{\alpha}{2}} \frac{\sigma}{\sqrt{n}} =: B
    \end{equation*}
    Konkreten zgled: "zivljenska doba "zarnic v vzorcu je $9.8, 10.2, 10.4, 9.8, 10.0, 10.2, 9.6$ (v dneh),
    $n=7$. Predpostavimo normalni model $N(\mu, \sigma)$ z neznanima parametroma $\mu$ in $\sigma$
    \begin{align*}
        &\overline{x} = \frac{1}{n} \sum_{i=1}^n x_i = 10.0 \\
        &s := \sqrt{\frac{1}{n-1} \sum_{i=1}^n (x_i - \overline{x})^2} = 0.283
    \end{align*}
    Vzemimo $\alpha = 0.05$, iz tabele za $Student(5)$ razberemo $t_{\frac{\alpha}{2}} = 2.45$ \\
    Vzor"cna vrednost za $A$ je $a = \overline{x} - t_{\frac{\alpha}{2}} \cdot \frac{s}{\sqrt{n}} = 9.74$ \\
    Vzor"cna vrednost za $B$ je $b = \overline{x} + t_{\frac{\alpha}{2}} \cdot \frac{s}{\sqrt{n}} = 10.26$ \\
    Interval zaupanja za $\mu$ je $[9.74, 10.26]$, kar zapi"semo kot $\mu = 10.0 \pm 0.26$, Verjetnost, da
    je $\mu \in [9.74, 10.26]$ je 0.95
\end{ex}



% 23. predavanje: 11.4.

\begin{ex}
    Pri normalni porazdelitvi $N(\mu, \sigma)$ ocenjujemo parameter $\sigma$. Vzor"cna statistika

    \begin{equation*}
        \chi^2 := \frac{1}{\sigma^2} \sum_{i=1}^{n} (x_i - \overline{x})^2 =
        \frac{n-1}{\sigma^2} \sum_{i=1}^{n} (x_i - \overline{x})^2S 
    \end{equation*}

    je porazdeljena po $\chi^2(n-1)$ \\
    Izberimo $c_1$ in $c_2$ da je

    \begin{equation*}
        P(\chi^2 < c_1) = \frac{\alpha}{2} = P(\chi^2 > c_2)
    \end{equation*}

    oz.

    \begin{equation*}
        P(c_1 < \chi^2 < c_2) = 1 - \alpha
    \end{equation*}

    Pogoj $c_1 < \chi^2 < c_2$ pomeni

    \begin{align*}
        &c_1 < \frac{n-1}{\sigma^2} s^2 < c_2 \iff \\
        &\iff \frac{1}{c_1} > \frac{\sigma^2}{(n-1)s^2} > \frac{1}{c_2} \iff \\
        &\iff B := \frac{(n-1)s^2}{c_1} > \sigma^2 > \frac{(n-1)s^2}{c_1} =: A 
    \end{align*}

    $[A,B]$ je interval zaupanja za $\sigma^2$ pri stopnji tveganja $\alpha$

    \begin{align*}
        &A = \frac{1}{c_2} \sum_{i=1}^{n} (x_i - \overline{x})^2, \\
        &B = \frac{1}{c_1} \sum_{i=1}^{n} (x_i - \overline{x})^2
    \end{align*}

    \begin{ex}
        "Zarnice iz prej"snjega primera: \\
        $n=7, (n-1)s^2 = \sum_{i=1}^{n} (x_i - \overline{x})^2 = 0.481, \alpha = 0.05$9

        \begin{align*}
            &\chi^2(6): c_1 = 1.24, c_2 = 14.45 \\
            &a = \frac{1}{14.45} 0.481 = 0.033, b = \frac{1}{1.24} 0.481 = 0.388 \\
            &\implies P(0.033 < \sigma^2 < 0.388) = 0.95 \\
            &P(0.182 < \sigma < 0.623) = 0.95
        \end{align*}

        $[0.182, 0.623]$ je interval zaupanja za $\sigma$ pri stopnji tveganja $0.05$
    \end{ex}
\end{ex}

\begin{ex}
    $X: \begin{pmatrix}
        0 & 1 \\
        q & p
    \end{pmatrix}, q = 1-p, p$ neznan parameter, ki ga ocenjujemo \\
    $(x_1 \cdots x_n)$ vzorec. Potem je $S_n = X_1 + \cdots + X_n \sim Bin(n,p)$ in $\overline{X} = \frac{S_n}{n}$
    je nepristranska in dosledna cenilka za $p$. Po CLI (Laplaceovi formuli) je pri velikih n
    $Z := \frac{S_n - np}{\sqrt{npq}} \sim N(0,1)$ oz. $Z = \frac{\overline{X} - p}{\sqrt{pq}} \sqrt{n} \sim N(0,1)$
    oz. $\overline{X} \sim N(p, \sqrt{\frac{pq}{n}})$ \\
    Pri danem $\alpha > 0$ izberimo $z_{\frac{\alpha}{2}} > 0$, da je $P(|Z| < z_{\frac{\alpha}{2}}) = 1 - \alpha$ \\P
    Pogoj $|Z| < z_{\frac{\alpha}{2}}$ pomeni $|S - np| < z_{\frac{\alpha}{2}} \sqrt{npq}$ oz.
    $|\overline{X} - p| < z_{\frac{\alpha}{2}} \cdot \sqrt{\frac{pq}{n}}$ \\
    "Ce na desni strani naredimo aproksimacijo $\overline{X} \approx p$, dobimo pogoj

    \begin{equation*}
        |\overline{X} - p| < z_{\frac{\alpha}{2}} \sqrt{\frac{\overline{X} (1-\overline{X})}{n}}
    \end{equation*}

    od koder dobimo interval zaupanja za $p$:

    \begin{align*}
        &A := \overline{X} - z_{\frac{\alpha}{2}} \sqrt{\frac{\overline{X} (1-\overline{X})}{n}} \\
        &B := \overline{X} + z_{\frac{\alpha}{2}} \sqrt{\frac{\overline{X} (1-\overline{X})}{n}} \\
        &A < p < B
    \end{align*}

    \begin{ex}
        Predsedni"ske volitve v ZDA leta 2000: \\
        Anketa na $2207$ volivcev: $n=2207, \alpha = 0.05 \implies z_{\frac{\alpha}{2}} = 1.96$ \\
        George Bush: $47\%$, Algore: $44\%$, Ralph Nader: $2\%$ \\
        Dolo"cimo intervale zaupanja

        \begin{equation*}
            p_{Bush} = 0.47 \pm 1.96 \sqrt{\frac{0.47 (1-0.47)}{2207}} \doteq 0.47 \pm 0.02
        \end{equation*}

        Interval zaupanja za $p_{Bush}$ je $[0.45, 0.49]$

        \begin{equation*}
            p_{Algore} = 0.44 \pm 1.96 \sqrt{\frac{0.44 \cdot 0.56}{2207}} \doteq 0.44 \pm 0.02
        \end{equation*}

        Interval zaupanja za $p_{Algore}$ je $[0.42, 0.46]$

        \begin{equation*}
            p_{Nader} = 0.02 \pm 1.96 \sqrt{\frac{0.02 \cdot 0.98}{2207}} \doteq 0.02 \pm 0.006
        \end{equation*}

        Odstopanje:

        \begin{align*}
            &z_{\frac{\alpha}{2}} \sqrt{\frac{\overline{x} (1-\overline{x})}{n}} < 
                2 \sqrt{\frac{\frac{1}{4}}{n}} = \frac{1}{\sqrt{n}} \\
            &x(1-x) \leq \frac{1}{4} \iff x-x^2 \leq \frac{1}{4} \iff \\
            &\iff 0 \leq x^2 - x + \frac{1}{4} = (x-\frac{1}{2})^2
        \end{align*}
    \end{ex}
\end{ex}

\subsection{Preizku"sanje statisti"cnih hipotez}

\begin{defn}[Statisti"cna hipoteza]
    Statisti"cna hipoteza je vsaka domneva o porazdelitvi slu"cajne spremenljivke $X$ na populaciji
\end{defn}

\begin{defn}[Enostavnost hipoteze]
    Hipoteza je enostavna, "ce natanko dolo"ca porazdelitev, sicer je sestavljena
\end{defn}

\begin{ex}
    $X \sim N(\mu, \sigma), \sigma$ poznamo, $\mu$ je neznan parameter \\
    $H(\mu = 0)$ je primer enostavne hipoteze. "Ce $\sigma$ ne poznamo, je to sestavljena hipoteza
\end{ex}

Vedno preizku"samo eno ni"celno hipotezo $H_0$ nasproti alternativni hipotezi $H_1$

\begin{ex}
    $X \sim N(\mu, \sigma), \sigma$ poznamo \\
    $H_0(\mu = 0): H_1(\mu \neq 0)$
\end{ex}

Za $H_0$ obi"cajno vzamemo enostavno hipotezo, za katero upamo, da jo bomo zavrnili \\
Hipoteza je lahko pravilna ali nepravilna. Ideal je sprejeti pravilno in zavrniti nepravilno. Odlo"citi se
moramo na osnovi vzorca. "Ce vzor"cni podatki preve"c odstopajo od hipoteze, potem niso konsistentni z njo oz.
so razlike zna"cilne (signifikantne); tedaj hipotezo zavrnemo \\
Vnaprej dolo"cimo stopnjo zna"cilnosti $\alpha \in [0,1]$, to je verjetnost, da zavrnemo pravilo hipotezo.
Obi"cajno je $\alpha = 0.05$ ali $\alpha = 0.01$. Take teste imenujemo testi zna"cilnosti

Primeri testov znacilnosti

\subsubsection{test $Z$}

$X \sim N(\mu, \sigma), \sigma$ znan parameter \\
Ni"celna domneva je $H_=(\mu = \mu_0)$, kjer je $\mu_0$ damo realno "stevilo \\
Pri predpostavki $H_0(\mu = \mu_0)$ je $Z := \frac{\overline{X} - \mu}{\sigma} \sqrt{n}$ porazdeljena
$N(0,1)$, saj je $\overline{X} \sim N(\mu_0, \frac{\sigma}{\sqrt{n}})$ \\
Vzemimo $H_1(\mu \neq \mu_0)$. Tedaj $H_0$ zavrnemo, "ce vzor"cna vrednost za $Z$ le"zi na kriticnem obmocju

\begin{equation*}
    K_{\alpha} = (-\infty, -z_{\frac{\alpha}{2}}] \cup [z_{\frac{\alpha}{2}}, \infty)
\end{equation*}

kjer je $\alpha$ stopnja zna"cilnosti in $P(Z > z_{\frac{\alpha}{2}}) = \frac{\alpha}{2}$



\end{document}