\documentclass[a4paper,12pt]{article}

% General document formatting
%\usepackage[margin=0.7in]{geometry}
\usepackage[parfill]{parskip}
\usepackage{url, hyperref}
\usepackage{color}
\usepackage[usestackEOL]{stackengine}[2013-10-15] % formatting Pascal
\usepackage[dvipsnames]{xcolor}

\usepackage{cancel}
\usepackage[export]{adjustbox}

% Related to math
\usepackage{amsmath,amssymb,amsfonts,amsthm}
\usepackage{mathtools}
\usepackage{youngtab} % \young diagram
\usepackage{tikz}

% encoding and language
\usepackage{lmodern}
\usepackage[slovene]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

% multiline comments
\usepackage{verbatim}

% enumerate with letters
\usepackage{enumitem}

% images
\usepackage{graphicx}
\graphicspath{ {./images/} }

% theorems
\theoremstyle{definition}
\newtheorem{counter}{Counter}[section] % not for use
\newtheorem{defn}[counter]{Definicija}
\newtheorem{lemma}[counter]{Lema}
\newtheorem{conseq}[counter]{Posledica}
\newtheorem{claim}[counter]{Trditev}
\newtheorem{theorem}[counter]{Izrek}
%%
\theoremstyle{remark}
\newtheorem*{ex}{Primer}
\newtheorem*{rem}{Opomba}
\newtheorem{rem*}[counter]{Opomba}
\newtheorem{ex*}[counter]{Primer}
\newtheorem{general}[counter]{Posplo"sitev}

% I like my squares DARK
\renewcommand\qedsymbol{$\blacksquare$}

% common commands redefined convenience purposes
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\ch}{\operatorname{char}}

% \cycle{1, 2, 3}
\ExplSyntaxOn
\NewDocumentCommand{\cycle}{ O{\;} m }{(\alec_cycle:nn { #1 } { #2 })}
\seq_new:N \l_alec_cycle_seq
\cs_new_protected:Npn \alec_cycle:nn #1 #2 {
	\seq_set_split:Nnn \l_alec_cycle_seq { , } { #2 }\seq_use:Nn \l_alec_cycle_seq { #1
}}
\ExplSyntaxOff

% Hack za Pascalov trikotnik
% https://newbedev.com/pascal-s-triangle-style
\def\x{\hspace{3ex}}    %BETWEEN TWO 1-DIGIT NUMBERS
\def\y{\hspace{2.45ex}}  %BETWEEN 1 AND 2 DIGIT NUMBERS
\def\z{\hspace{1.9ex}}    %BETWEEN TWO 2-DIGIT NUMBERS
\stackMath

\begin{document}

\title{Verjetnost in statistika - zapiski s predavanj prof. Drnovška}
\author{
	Toma"z Poljan"sek
}
\date{študijsko leto 2022/23}
\maketitle


\pagenumbering{roman}
\tableofcontents
\newpage
\pagenumbering{arabic}



% 1. predavanje: 7.10.

\section{Verjetnost}

\subsection{Neformalni uvod v verjetnost}

Začetki verjetnosti (kot vede) so v 17. stoletju, motivacija igre na sre"co \\
17. stol: Fermat, Pascal, Bernoulli \\
18. in 19 stol: Laplace, Poisson, "Cebi"sev, Markov \\
20. stol: Kolmogorov (okoli 1930), utemeljitelj sodobnega verjetnostnega ra"cuna \\

\begin{defn}[Dogodek]
    Izvajamo poskus, opazujemo nek pojav, ki se lahko zgodi in ga imenujemo dogodek
\end{defn}

\begin{ex}
    Met po"stene kocke, dogodek je npr. pade "sestica, ali npr. pade sodo "stevilo pik
\end{ex}

\begin{defn}[Frekvenca]
    Poskus ponovimo $n$-krat. Opazujemo dogodek $A$. \\
    Naj bo $K_n(A)$ frekvenca dogodka $A$, t.j. "stevilo tistih ponovitev, pri katerih se je dogodek $A$ zgodil. \\
    Relativna frekvenca je $f_n(A) = \frac{K_n(A)}{n} \in [0,1]$
\end{defn}

Dokazati je mogo"ce, da zaporedje $\{f_n(A)\}$ konvergira, recimo h $p \in [0,1]$. \\
Statisti"cna definicija verjetnosti: $P(A) := p$. \\
Pogosto verjetnost lahko dolo"cimo vnaprej: \\
Klasi"cna definicija verjetnosti: $P(A) = \frac{\text{"stevilo ugodnih izidov za dogodek }A}{\text{"stevilo vseh izidov}}$
pri pogoju, da imajo vsi izidi enake mo"znosti

\begin{ex}
    met kocke: \\
    $P(\text{pade "sestica}) = \frac{1}{6}$ \\
    $P(\text{pade sodo "stevilo pik}) = \frac{3}{6} = \frac{1}{2}$
\end{ex}

\begin{ex}
    Kolik"sna je verjetnost, da pri metu dveh kock zna"sa vsota pik 7? \\
    Mo"zne vsote: $2, 3, 4 \cdots 7 \cdots 12$: 11 mo"znosti \\
    Ali je $P(\text{vsota 7}) = \frac{1}{11}$? Ne, ker te vsote nimajo enakih mo"znosti: \\
    $2=1+1, 3=2+1=1+2$ \\
    Vsi mo"zni izidi so $\{(i,j): i,j \in \{1, 2 \cdots 6\}\} = \{1, 2 \cdots 6\} \times \{1, 2 \cdots 6\}$ \\
    $\begin{matrix}
        (1,1) & (1,2) & \cdots & (1,6) \\
        (2,1) & (2,2) & \cdots & (2,6) \\
         & \vdots & & \\
        (6,1) & (6,2) & \cdots & (6,6) \\
    \end{matrix}$ \\
    $P(\text{vsota 7}) = \frac{6}{36} = \frac{1}{6}$
\end{ex}

"Ce je vseh izidov neskon"cno, si lahko pomagamo z geometrijsko definicijo verjetnosti

\begin{ex}
    Osebi se dogovorita za sestanek med 10. in 11. uro; "cas prihoda je slu"cajen. Vsak "caka najve"c 20 minut, najdlje
    do 11. ure; "ce v tem "casu drugega ni, odide. Kolik"sna je verjetnost sre"canja? \\
    Za"cnimo "cas "steti ob 10. uri. Naj bo $x$ "cas prihoda 1. osebe, $y$ pa "cas prihoda druge osebe \\
    Mo"zni izidi so kvadrat $[0,1]^2 = [0,1] \times [0,1]$ \\
    Ugodni izidi so $|x-y| \leq \frac{1}{3}$
    \begin{enumerate}
        \item $x \geq y: \; x - y \leq \frac{1}{3}$ oz. $x - \frac{1}{3} \leq y$
        \item $x \leq y: \; y - x \leq \frac{1}{3}$ oz. $y \leq x + \frac{1}{3}$
    \end{enumerate}
    $P(\text{sre"canje}) = \frac{\text{plo"s"cina ozna"cenega lika}}{\text{polo"s"cina kvadrata}} =
    \frac{1 - (\frac{2}{3})^2}{1} = \frac{5}{9}$
\end{ex}

\begin{ex}
    Slu"cajno razporedimo $n$ kroglic v $m$ posod, kjer je $m>n$. Kolik"sna je verjetnost, da so vse kroglice v prvih
    $n$ posodah, v vsaki ena? \\
    Obravnavajmo 3 variante:
    \begin{enumerate}
        \item kroglice razlikujemo \\
            Vsi izidi: $m \cdot m \cdots m = m^2$ variacije \\
            Ugodni izidi: $n \cdot (n-1) \cdots 1 = n!$ permutacija \\
            $\implies P(A) = \frac{n!}{m^n}$
        \item kroglic ne razlikujemo \\
            $n$ kroglic, $m-1$ "crtic $\implies (m+n-1)$ mest \\
            Vsi izidi: $\binom{m+n-1}{n}$ kombinacije s ponavljanjem \\
            Ugidni izidi: 1 \\
            $P(A) = \frac{1}{\binom{m+n-1}{n}}$
        \item kroglic ne razlikujemo, v vsaki posodi je kve"cjemu ena \\
            "Stevilo vseh izidov je $\binom{m}{n}$ \\
            Ugoden izid je samo eden \\
            Torej je $P(A) = \frac{1}{\binom{m}{n}}$
    \end{enumerate}
\end{ex}

V kvantni mehaniki so kroglice razli"cni delci, posode so energetska stanja \\
V primeru (a) imamo Maxwell-Boltzmanovi statistiki, velja za molekule plina \\
V primeru (b) imamo Bose-Einsteinovo statistiko, velja za bozone (npr. fotoni) \\
V primeru (c) imamo Fermi-Diracovo statistiko, velja za fermione (npr elektroni);
zanje velja Diracovo izklju"citveno na"celo: v vsakem stanju je najve"c en delec

\subsection{Aksiomati"cna definicija verjetnosti}

Kolmogorov (okoli 1930)

\begin{defn}[Dogodek]
    Imamo prostor vseh dogodkov $\Omega$ (mo"zna oznaka je $G$). Dogodki so nekatere (ne nujno vse)
    podmno"zice $A \subseteq \Omega$
\end{defn}

\begin{ex}
    Met kocke: $\Omega = \{1,2,3,4,5,6\}$, dogodki so vse podmno"zice,  $\{6\} \cdots$ dogodek, da pade "sestica,
    $\{2,4,6\} \cdots$ dogodek, da pade sodo "stevilo pik
\end{ex}

Ra"cunanje z dogodki

\begin{enumerate}
    \item Vsota dogodkov oz. unija dogodkov: $A + B$ oz. $A \cup B$: dogodek, da se zgodi vsaj eden od $A$ in $B$
    \item Produkt dogodkov oz. presek dogodkov: $A \cdot B$ oz. $A \cap B$: dogodek, da se zgodita oda dogodka $A$ in $B$
    \item Nasprotni dogodek oz. komplement dogodka: $\overline{A} = A^C$
\end{enumerate}

Pravila za ra"cunanje z dogodki:

\begin{itemize}
    \item idempotentnost: $A \cup A = A = A \cap A$
    \item komutativnost: $A \cap B = B \cap A$ in $A \cup B = B \cup A$



% 2. predavanje: 14.10.

    \item asociativnost: $(A \cup B) \cup C = A \cup (B \cup C), (A \cap B) \cap C = A \cap (B \cap C)$
    \item distributivnost: $(A \cap B) \cup C = (A \cup C) \cap (B \cup C), (A \cup B) \cap C = (A \cap C) \cup (B \cap C)$
        oz. $(A \cdot B) + C = (A + C) \cdot (B + C), (A + B) \cdot C = (A \cdot C) + (B \cdot C)$
    \item deMorganova zakona: $(A \cup B)^C = A^C \cap B^C, (A \cap B)^C = A^C \cup B^C$ \\
        "se ve"c: $(\cup_{i \in I} A_i)^C = \cap_{i \in I} A_i^C, (\cap_{i \in I} A_i)^C = \cup_{i \in I} A_i^C$
\end{itemize}

\begin{defn}[$\sigma$-algebra]
    Neprazna dru"zina podmno"zic dogodkov $F$ v $\Omega$ je $\sigma$-algebra, "ce velja:

    \begin{enumerate}
        \item $A \in F \implies A^C \in F$ (zaprtost za komplement)
        \item $A_1, A_2 \cdots \in F \implies \cup_{i=1}^{\infty} A_i \in F$ (zaprtost za "stevne unije)
    \end{enumerate}

    "Ce v 2) zahtevamo manj: \\
    $A, B \in F \implies A \cup B \in F$ ("sibkej"si pogoj) pravimo, da je $F$ algebra
\end{defn}

V algebri imamo zaprtost za kon"cne unije, t.j. $A_1 \cdots A_n \in F \implies \cup_{i=1}^n A_j \in F$ (zaradi
indukcije). Ker je $\cap_i A_i^C = (\cup_i A_i)^C$ (deMorgan), je algebra zaprta za kon"cne preseke, $\sigma$-algebra
pa za "stevne preseke. \\
Ker je $A$ \textbackslash $B = A \cap B^C$, je algebra zaprta za razlike dogodkov. \\
Vsaka algebra vsebuje $\{\emptyset, \Omega\}$: ker je neprazna, obstaja dogodek $A \in F$, potem je $A^C \in F$ in
zato je 

\begin{equation*}
    \Omega = A \cup A^C \in F, \emptyset = A \cap A^C \in F
\end{equation*}

Najmanj"sa ($\sigma$-)algebra je $F = \{\emptyset, \Omega\}$, najve"cja ($\sigma$-)algebra je poten"cna mno"zica
$P(\Omega)$

\begin{ex}
    Izberimo $\emptyset \neq A \subsetneq \Omega$. Najmanj"sa $\sigma$-algebra, ki vsebuje $\{1\}, \{2\}, \{3\} \cdots$
    je $P(\N)$, saj je $A = \cup_{k \in A} \{k\}$ za $\forall k \subseteq \N$ (kon"cna sli "stevna unija) \\
    Najmanj"sa algebra $F$, ki vsebuje $\{1\}, \{2\}, \{3\} \cdots$ je enaka algebri

    \begin{equation*}
        g = \{A \subseteq \N: A \text{ je kon"cna ali } A^C \text{ je kon"cna}\}
    \end{equation*}

    Dokazujemo $g$ je algebra:
    
    \begin{enumerate}
        \item zaprtost za komplemente: $A \in g \implies$
        \begin{enumerate}
            \item bodisi $A$ je kon"cna mno"zica $\implies A = (A^C)^C \implies A^C \in g$ (zaprtost za komplement)
            \item bodisi je $A^C$ kon"cna mno"zica $\implies A^C \in g$
        \end{enumerate}
        \item $A,B \in g \stackrel{?}{\implies} A \cup B \in g$
        \begin{enumerate}
            \item $A \cup B$ je kon"cna $\implies (A \cup B) \in g$ (vse kon"cne mno"zice)
            \item $A \cup B$ ni kon"cna $\implies$ vsaj ena izmed $A$ in $B$ ni kon"cna, recimo $A$ je neskon"cna. \\
                Toda $A \in G \implies A^C$ je kon"cna mno"zica. \\
                Ker je $(A \cup B)^C \subseteq A^C$ in $A^C$ je kon"cna, je $(A \cup B)^c$ tudi kon"cna mno"zica,
                torej $A \cup B \in g$ \\
                $A \in g$:
                \begin{itemize}
                    \item $A$ je kon"cna: $A = \cup_{k \in A} \{k\} \in F$ (kon"cna unija)
                    \item $A^C$ je kon"cna: $A^C = (\cup_{k \in A^C})^C \in F$ (kon"cna unija)
                \end{itemize}
                $\implies g \in F$
        \end{enumerate}
    \end{enumerate}

    Ker je $F$ najmanj"sa algebra, ki vsebuje $\{1\}, \{2\} \cdots$, je tukaj ena"caj, torej $g = F$ \\
    Ker npr. mno"zica sodih "stevil ni v $g \in F$, je $g \notin P(\N)$, torej $g$ ni $\sigma$-algebra
\end{ex}

\begin{defn}[Nezdru"zljivost dogodkov]
    Dogodka $A$ in $B$ sta nezdru"zljiva ali disjunktna, "ce je $A \cap B = \emptyset$
\end{defn}

\begin{defn}[Popoln sistem dogodkov]
    Zaporedje $\{A_i\}_i$ (kon"cno ali "stevno mnogo) je popoln sistem dogodkov, "ce $\Omega = \cup_i A_i$ in
    $A_i \cap A_j = \emptyset \forall i \neq j$
\end{defn}

\begin{defn}[Verjetnost]
    Naj bo $F \sigma$-algebra na $\Omega$. Verjetnost na $(\Omega, F)$ je preslikava $P: F \to \R$ z lastnostmi:

    \begin{enumerate}
        \item $P(A) \geq 0 \forall A \in F$
        \item $P(\Omega) = 1$ \\
        \item Za poljubne paroma nezdru"zljive dogodke $A_1, A_2 \cdots$ velja
            \begin{equation*}
                P(\cup_{i=1}^{\infty} A_i) = \sum_{i=1}^{\infty} P(A_i)
            \end{equation*}
            "stevna aditivnost (verjetnostne preslikave)
    \end{enumerate}
\end{defn}

Lastnosti preslikave $P$:

\begin{enumerate}
    \item $P(\emptyset) = 0$
        \begin{proof}
            v 3) vstavimo $A_1 = A_2 = \cdots = \emptyset$:
            \begin{align*}
                &P(\emptyset) = P(\emptyset) + P(\emptyset) + \cdots + P(\emptyset) = k \cdot P(\emptyset) \implies \\
                &\implies (k-1) P(\emptyset) = 0 \implies P(\emptyset) = 0
            \end{align*}
        \end{proof}
    \item $P$ je kon"cno aditivna, t.j. za poljubne paroma nezdru"zljive dogodke $A_1 \cdots A_n$ velja
        \begin{equation*}
            P(\cup_{i=1}^n A_i) = \sum_{i=1}^{n} P(A_i)
        \end{equation*}
        \begin{proof}
            v 3) vzamemo $A_{n+1} = A_{n+2} = \cdots = \emptyset$:
            \begin{equation*}
                P(\cup_{i=1}^n A_i) = \sum_{i=1}^{\infty} P(A_i) = \sum_{i=1}^{n} P(A_i)
                (\text{zaradi} P(\emptyset) = 0)
            \end{equation*}
        \end{proof}
    \item $P$ je monotona, t.j. iz $A \subseteq B$ ($A,B \in F$) sledi $P(A) \subseteq P(B)$, "se ve"c:
        $A \subseteq B \implies P(B \text{\textbackslash} A) = P(B) - P(A)$
        \begin{proof}
            Ker je $B = A \cup (B \text{\textbackslash} A)$ in $A \cap (B \text{\textbackslash} A) = \emptyset$,
            je po b) $P(B) = P(A) + P(B \text{\textbackslash} A)$
        \end{proof}
    \item $P(A^C) = 1 - P(A)$ za $A \in F$
        \begin{proof}
            \begin{align*}
                &B = \Omega \implies P(A^C) = P(\Omega \text{\textbackslash} A) = \\
                &\stackrel{c)}{=} P(\Omega) - P(A) \stackrel{2)}{=} 1 - P(A)
            \end{align*}
        \end{proof}
    \item $P$ je zvezna, t.j.
        \begin{enumerate}
            \item iz $A_1 \subseteq A_2 \subseteq \cdots A_i \in F$ sledi
                $P(\cup_{i=1}^{\infty} A_i) = \lim_{i \to \infty} P(A_i)$
            \item iz $B_1 \supseteq B_2 \supseteq \cdots B_i \in F$ sledi
                $P(\cap_{i=1}^{\infty} B_i) = \lim_{i \to \infty} P(B_i)$
        \end{enumerate}
        Definiramo
        \begin{align*}
            &C_1 = A_1 \\
            &C_i = A_i - A_{i-1} \text{ za } i \geq 2
        \end{align*}
        Potem $C_i \cap C_j = \emptyset$ za $i \neq j$, $A_n = C_1 \cup \cdots \cup C_n$ in
        $\cup_{i=1}^{\infty} A_i = \cup_{i=1}^{\infty} C_i$ \\
        Torej imamo
        \begin{align*}
            &P(\cup_{i=1}^{\infty} A_i) = P(\cup_{i=1}^{\infty} C_i) = \\
            &\stackrel{3)}{=} \sum_{i=1}^{\infty} P(C_i) = \lim_{n \to \infty} \sum_{i=1}^{n} P(C_i) = \\
            &\stackrel{b)}{=} \lim_{n \to \infty} P(A_i) 
        \end{align*}
        \begin{proof}
            Dokazujemo ii):
            iz $B_1 \supseteq B_2 \supseteq \cdots$ sledi $B_1^C \subseteq B_2^C \subseteq \cdots$ in zato po (i)
            \begin{equation*}
                P(\cup_{i=1}^{\infty} B_i^C) = \lim_{i \to \infty} P(B_i^C) \stackrel{a)}{=} 1 - \lim_{i \to \infty} P(B_i)
            \end{equation*}
            Toda
            \begin{equation*}
                P(\cup_{i=1}^{\infty} B_i^C) = P((\cup_{i=1}^{\infty} B_i)^C) \stackrel{d)}{=} 1 - P(\cap_{i=1}^{\infty} B_i)
            \end{equation*}
            Od tod sledi "zelena enakost
            \begin{equation*}
                P(\cap_{i=1}^{\infty} B_i^C) = \lim_{i \to \infty} P(B_i)
            \end{equation*}
        \end{proof}
\end{enumerate}

$(\Omega, F, P)$ verjetnostni prostor

\begin{ex}
    (kon"cni ali "stevni verjetnostni prostor) \\
    $\Omega = \{w_1, w_2 \cdots\}$ kon"cna ali "stevna mno"zica, paroma razli"cni, \\
    $F = P(\Omega), A = \cup_{i \in A} \{w_i\}$ kon"cna ali "stevna unija \\
    $\{w_1\}, \{w_2\} \cdots$ so popoln sistem dogodkov \\
    "ce je $p_i := P(\{w_i\})$, je $P(A) = \sum_{i: w_i \in A} p_i$ in $\sum_i p_i = 1 = P(\Omega)$ \\
    Poseben primer: $\Omega$ ima $n$ elementov in $p_i = \frac{1}{n}, P(A) \frac{\text{mo"c}(A)}{n}$ \\
    To je kla"si"cna definicija verjetnosti
\end{ex}



% 3. predavanje: 28.10.

$(\Omega, \Phi, P)$

\begin{ex}
    (Ne"stevni neskon"cni verjetnostni prostor) \\
    $\Omega = [0,1] \times [0,1]$ \\
    $\Phi :=$ najmanj"sa $\sigma$-algebra, ki vsebuje vse odprte pravokotnike $(a,b) \times (c,d),
    \; a,b,c,d \in (0,1)$ \\
    npr. elipse: $\frac{1}{n}$ radij, za $\forall n$ vzamemo kvadrate v elipsi, izberemo unijo \\
    = najmanj"sa $\sigma$-algebra, ki vsebuje vse zaprte pravokotnike $[a,b] \times [c,d], \; a,b,c,d \in [0,1]$ -
    Borelova $\sigma$-algebra \\
    Izka"ze se, da $\Phi \neq P(\Omega)$
\end{ex}

Verjetnost $P$ definiramo na pravokotnikih s $P((a,b) \times (c,d)) = (b-a) (d-c)$ \\
Ni lahko videti, da je to mo"zno raz"siriti do "stevno aditivne preslikave na $P(\Omega)$ \\
Verjetnostna preslikava $P$ (na $\Phi$) se imenuje Lebesgueova mera \\
To je geometrijska definicija verjetnosti:

\begin{equation*}
    \square = \cap_{n=1}^{\infty} (a-\frac{1}{n}, b+\frac{1}{n}) \times (c-\frac{1}{n}, d+\frac{1}{n})
\end{equation*}

\subsection{Pogojna verjetnost}

\begin{defn}[Pogojna verjetnost]
    Fiksirajmo dogodek $B$ s $P(B) > 0$. Pogojna verjetnost dogodka $A$ pri pogoju $B$ je

    \begin{equation*}
        P(A \mid B) = \frac{P(A \cap B)}{P(B)}
    \end{equation*}
\end{defn}

\begin{ex}
    V posodi sta 2 beli in ena "crna kroglica. Slu"cajno izberemo eno kroglico, jo vrnemo v posodo in potem ponovno
    izberemo kroglico. Kolik"sna je verjetnost, da smo v drugo izbrali belo kroglico, "ce smo v prvo izbrali belo
    kroglico?
    $\Omega = \begin{matrix}
        B_1B_1 & B_1B_2 & B_1\text{"C} \\
        B_2B_1 & B_2B_2 & B_2\text{"C} \\
        \text{"C}B_1 & \text{"C}B_2 & \text{"C"C} \\
    \end{matrix}$ \\
    \begin{align*}
        &P(\text{prvi"c bela}) = \frac{6}{9} = \frac{2}{3} \\
        &P(\text{drugi"c bela} \mid \text{prvi"c bela}) = \frac{P(\text{prvi"c in drugi"c bela})}{P(\text{prvi"c bela})}
            = \frac{\frac{4}{9}}{\frac{2}{3}} = \frac{2}{3}
    \end{align*}
\end{ex}

Iz definicije sledi $P(A \cap B) = P(B) \cdot P(A \mid B)$ \\
Za poljubne dogodke $A, B, C$ velja

\begin{align*}
    &P(A \cap (B \cap C)) = P(B \cap C) \cdot P(A \mid B \cap C) = \\
    &= P(C) \cdot P(B \mid C) \cdot P(A \mid B \cap C)
\end{align*}

oz. ``lep"se''

\begin{equation*}
    P(A \cap B \cap C) = P(A) \cdot P(B \mid A) \cdot P(C \mid A \cap B)
\end{equation*}

To posplo"simo na $n$ dogodkov $A_1, A_2 \cdots A_n$:

\begin{align*}
    &P(A_1 \cap \cdots \cap A_n) = P(A_1) \cdot P(A_2 \mid A_1) \cdots P(A_n \mid A_1 \cap \cdots \cap A_{n-1}) = \\
    &= P(A_1) \cdot \prod_{i=2}^{n} P(A_i \mid \cap_{j=1}^{i-1} A_j)
\end{align*}

Desna stran:

\begin{equation*}
    P(A_1) \cdot \frac{P(A_1 \cap A_2)}{P(A_1)} \cdot \frac{P(A_1 \cap A_2 \cap A_3)}{P(A_1 \cap A_2)} \cdots
    \frac{P(A_1 \cap \cdots \cap A_n)}{P(A_1 \cap \cdots \cap A_{n-1})}
\end{equation*}

Imejmo poskus v dveh korakih (fazah). V 1. koraku se zgodi natanko en dogodek iz popolnega sistema dogodkov
$H_1, H_2 \cdots$ (kon"cno/"stevno mnogo). V drugem koraku nas zanima dogodek $A$. Izrazimo $P(A)$ z verjetnostmi
$P(H_1), P(H_2 \cdots)$ in $P(A \mid H_1), P(A \mid H_2) \cdots$. \\
Ker je $A = A \cap \Omega = A \cap (\cup_i H_i) = \cup_i (A \cap H_i)$ in ker so $\{A \cap H_i\}_i$ paroma
nezrdu"zljivi dogodki (zaradi $H_i$), je

\begin{equation*}
    P(A) = \sum_i P(A \cap H_i) = \sum_i P(H_i) \cdot P(A \mid H_i)
\end{equation*}

To je formula o popolni verjetnosti

\begin{ex}
    Na sre"colovu je $n$ sre"ck, od tega je $m$ dobitnih ($m < n$). Ali imamo pred za"cetkom sre"colova ve"cje
    mo"znosti za dobitek, "ce izbiramo prvi ali drugi? \\
    $H_1$: prvi dobi, $H_2$: prvi ne dobi, $A$: drugi zadane \\
    \begin{align*}
        &P(\text{prvi dobi}) = \frac{m}{n} \\
        &P(\text{drugi dobi}) = P(\text{prvi dobi}) \cdot P(\text{drugi dobi} \mid \text{prvi dobi}) + \\
        &+ P(\text{prvi ne dobi}) \cdot P(\text{drugi dobi} \mid \text{prvi ne dobi}) = \\
        &= \frac{m}{n} \cdot \frac{m-1}{n-1} + \frac{n-m}{n} \cdot \frac{m}{n-1} = \cdots = \frac{m}{n} 
    \end{align*}
\end{ex}

Pri dvofaznem poskusu nas zanima 

\begin{equation*}
    P(H_k \mid A) = \frac{P(H_k \cap A)}{P(A)} =
    \frac{P(H_k) \cdot P(A \mid H_k)}{\sum_i P(H_i) \cdot P(A \mid H_i)}
\end{equation*}

- Bayesova formula

\begin{ex}
    Test s poligrafom (= detektor la"zi) \\
    Resnicoljub opravi test s poligrafom z verjetnostjo $0.95$. Z enako verjetnostjo poligraf prepozna la"znivca.
    Izmed $1000$ oseb, med katerimi je natanko en la"znivec, slu"cajno izberemo eno osebo, katero poligraf
    proglasi za la"znivca. Kolik"sna je pogojna verjetnost, da je ta oseba res la"znivec? \\
    Naj bo $L$ dogodek, da je oseba la"znivec. \\
    Naj bo $L_p$ dogodek, da poligraf za osebo pravi, da je la"znivec. Potem je

    \begin{align*}
        &P(L_p \mid L) = 0.95 \text{ in } P(L_P^C \mid L^C) = 0.95 \text{ oz.} \\
        &P(L_P \mid L^C) = 0.05 \\
        &P(L) = 0.001
    \end{align*}

    I"s"cemo verjetnost $P(L \mid L_p)$ \\
    $H_1 = L, H_2 = L^C, A = L_p$ \\

    \begin{align*}
        &P(L \mid L_p) = \frac{P(L) \cdot P(L_p \mid L)}{P(L) \cdot P(L_p \mid L) + P(L^C) \cdot P(L_p^C \mid L^C)} = \\
        &= \frac{0.001 \cdot 0.95}{0.001 \cdot 0.95 + 0.999 \cdot 0.05} = \frac{95}{5050} \doteq 0.02 = \frac{1}{50}
    \end{align*}
\end{ex}

Matemati"cno ekvivalenten problem je presejalni test, npr. program DORA. (Pogojna) verjetnost, da je oseba bolna, "ce
je test pozitiven, je majhna. \\
Dogodka $A$ in $B$ sta neodvisna, "ce je $P(A \cap B) = P(A) \cdot P(B)$ \\
"Ce je $P(B) > 0$, potem lahko ta pogoj zapi"semo kot $P(A) = \frac{P(A \cap B)}{P(B)} = P(A \mid B)$



% 4. predavanje: 28.10.

\begin{defn}[Neodvisnost]
    $A$ in $B$ sta neodvisna, "ce $P(A \cap B) = P(A) \cdot P(B)$ \\
    Dogodki $\{A_i\}_i$ so neodvisni, "ce za poljuben kon"cen nabor razli"cnih dogodkov $A_{i_1}, A_{i_2} \cdots
    A_{i_n}$ velja

    \begin{equation*}
        P(A_{i_i} \cap \cdots \cap A_{i_n}) = P(A_{i_1}) \cdot \cdots \cdot P(A_{i_n})
    \end{equation*}

    "Ce zahtevamo le za $n=2$, t.j. $P(A_i \cap A_j) = P(A_i) \cdot P(A_j), i \neq j$, tedaj so dogodki paroma
    neodvisni
\end{defn}

O"citno iz neodvisnosti sledi paroma neodvisnost. Obratno ne velja

\begin{ex}
    \begin{align*}
        &\Omega = \{1, 2, 3, 4\}, P(\{k\}) = \frac{1}{4} \text{ za } k = 1,2,3,4 \text{ npr. met tetraedra} \\
        &A = \{1, 2\}, B = \{1, 3\}, C = \{1, 4\} \\
        &P(A) = P(B) = P(C) = \frac{2}{4} = \frac{1}{2} \\
        &A \cap B = B \cap C = A \cap C = \{1\} \\
        &\implies P(A \cap B) = P(B \cap C) = P(A \cap C) = \frac{1}{4} \\
        &\implies A, B, C \text{ so paroma neodvisni} \\
        &A \cap B \cap C = \{1\} \\
        &P(A \cap B \cap C) = \frac{1}{4} \neq \frac{1}{8} = P(A) \cdot P(B) \cdot P(C) \\
        &\implies \text{ niso neodvisni}
    \end{align*}
\end{ex}

\begin{claim}
    Naj bosta $A$ in $B$ neodvisna dogodka. Potem sta neodvisna tudi $A$ in $B^C$. Prav tako tudo $A^C$ in $B$
    ter $A^C$ in $B^C$ (komplementiranje ohranja neodvisnost)
\end{claim}

\begin{proof}
    Ker je $A \cap B^C = A \text{\textbackslash} (A \cap B)$ je

    \begin{align*}
        &P(A \cap B^C) = P(A \text{\textbackslash} (A \cap B)) = P(A) - P(A \cap B) = \\
        &\stackrel{A, B \text{ neodvisna}}{=} P(A) - P(A) \cdot P(B) = P(A) (1-P(B)) = P(A) \cdot P(B^C)
    \end{align*}

    podobno za ostale kombinacije
\end{proof}

\subsection{Zaporedja neodvisnih ponovitev poskusa}

\begin{defn}
    Imejmo zaporedje $n$ neodvisnih ponovitev poskusa, dolo"cenega v verjetnostnem prostoru $(\Omega, \Phi, P)$,
    v katerem je mo"zen $A$ s $P(A) = p \in (0,1)$. Potem je $q := P(A^C) = 1 - p$
\end{defn}

Z $A_n(k)$ ozna"cimo dogodek, da se v $k$ ponovitvah poskusa $A$ zgodi natanko $n$-krat, $k = 0, 1 \cdots n$ \\
Poka"zimo, da je njegova verjetnost $P_n(k) := P(A_n(k)) = \binom{n}{k} p^k q^{n-k}$ - Bernoullijeva formula \\
$A_n(k)$ je disjunktna unija $\binom{n}{k}$ dogodkov, da se $A$ zgodi na predpisanih $k$ mestih, $A^C$ pa na
preostalih $(n-k)$ mestih. Verjetnost le teh je produkt $p$-jev in $q$-jev: $p^k q^{n-k}$. Od tod sledi
Bernoullijeva formula

\begin{ex}
    Kaljivost semen je $95\%$. Kolik"sna je verjetnost, da izmed $1000$ semen vzkali to"cno $950$ semen? \\
    $A$ = seme ne vzkali \\
    $P(A) = p = 0.05, q = 0.95$ \\
    $P_{1000}(50) = \binom{1000}{50} 0.05^{50} \cdot 0.95^{950} \doteq 0.05779$
\end{ex}

Brez ra"cunala je to te"zko izra"cunati tudi "ce uporabimo Stirlingovo formulo na $n!$:

\begin{equation*}
    n! \sim \sqrt{2\pi n} (\frac{n}{e})^n
\end{equation*}

Tukaj $\sim$ pomeni: $a_n \sim b_n$ "ce $\lim_{n \to \infty} \frac{a_n}{b_n} = 1$ \\
Torej je $\lim_{n \to \infty} \frac{\sqrt{2\pi n}}{n!} (\frac{n}{e})^n = 1$

\subsubsection{Aproksimacijski formuli za $P_n(k)$}

\subsubsection{Poissonova formula}

"Ce je $n$ velik in $k$ majhen, je $P_n(k) \approx \frac{\lambda^k}{k!} e^{-\lambda}$, kjer je $\lambda = np$

\begin{proof}
    \begin{align*}
        &P_n(k) \stackrel{\text{def}}{=} \binom{n}{k} p^k q^{n-k} =
            \frac{n(n-1) \cdots (n-k+1)}{k!} (\frac{\lambda}{n})^k (1 - \frac{\lambda}{n})^{n-k} = \\
        &= \frac{\lambda}{k!} \frac{n}{n} \frac{n-1}{n} \cdots \frac{n-k+1}{n}
            (1 - \frac{\lambda}{n})^n (1 - \frac{\lambda}{n})^{-k} \approx \\
        &\frac{n-i}{n} \to 1, (1 - \frac{\lambda}{n})^n \to e^{-\lambda}, (1 - \frac{\lambda}{n})^{-k} \to 1 \\
        &\approx \frac{\lambda^k}{k!} e^{-\lambda}
    \end{align*}
\end{proof}

\begin{ex}
    Kaljivost semen

    \begin{align*}
        &P_{1000}(50) \doteq \frac{50^{50}}{50!} e^{-50} = \frac{1}{50!} (\frac{50}{e})^{50} = \\
        &\stackrel{\text{Stirling}}{=} \frac{1}{\sqrt{2\pi50}} = \frac{1}{10\sqrt{pi}} \doteq 0.05642
    \end{align*}
\end{ex}

\subsubsection{Laplaceova lokalna formula}

"Ce je $n$ velik, potem je $P_n(k) \approx \frac{1}{\sqrt{2\pi npq}} \cdot e^{-\frac{(k-np)^2}{2npq}}$ \\
Kasneje (2. semester) bomo dokazali splo"snej"si izrek (centralni limitni izrek) \\
Nari"simo zaporedje $\{P_n(k)\}_{k=0}^n$, $n$ fiksen

\begin{align*}
    &P_n(0) = q^n \\
    &P_n(1) = n p q^{n-1} \\
    &P_n(2) = \frac{n(n-1)}{2} p^2 q^{n-2}
\end{align*}

Pomaknjena in raztegnjena funkcija $f(x) = \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}}$ \\

\begin{align*}
    &P_n(k) \leq P_n(k+1)\text{?} \\
    &\frac{n!}{k!(n-k)!} p^k q^{n-k} \leq \frac{n!}{(k+1)!(n-k-1)!} p^{k+1} q^{n-k-1} \\
    &\frac{q}{n-k} \leq \frac{p}{k+1} \iff kq + q \leq np - kp \iff \\
    &\iff k(p+q) + q \leq np \iff k + q \leq np
\end{align*}

Neenakost se obrne pri $k \approx np$

\begin{ex}
    Kaljivost semen

    \begin{align*}
        &p = 0.05, q = 0.95, k = 50 \implies np = 50 \\
        &P_{1000}(50) \approx \frac{1}{\sqrt{2 \pi \cdot 50 \cdot 0.95}} = \frac{1}{\sqrt{95\pi}}
            \doteq 0.05788
    \end{align*}
\end{ex}

\subsubsection{Laplaceova integralska formula}

Zanima nas dogodek $B_n(k_1, k_2)$, da se n $n$ ponovitvah poskusa dogodek $A$ zgodi vsaj $k_1$-krat in manj kot
$k_2$-krat, $0 \leq k_1 < k_2 \leq n+1$ \\
Ker je 

\begin{equation*}
    B_n(k_1, k_2) = A_n(k_1) \cup A_n(k_1+1) \cup \cdots \cup A_n(k_2-1)
\end{equation*}

(disjunktna unija), je

\begin{equation*}
    P_n(k_1, k_2) := P(B_n(k_1, k_2)) = \sum_{k=k_1}^{k_2-1} |A_n(k)| = \sum_{k=k_1}^{k_2-1} P_n(k)
\end{equation*}

Po Laplaceovi lokalni formuli je

\begin{align*}
    &P_n(k_1, k_2) \approx \frac{1}{\sqrt{2\pi npq}} \sum_{k=k_1}^{k_2-1} e^{-\frac{(k-nq)^2}{2npq}} = \\
    &\doteq \frac{1}{\sqrt{2\pi}} \sum_{k=k_1}^{k_2-1} e^{-\frac{1}{2} x_k^2} \Delta x_k
\end{align*}

kjer je

\begin{align*}
    &x_k := \frac{k-np}{\sqrt{npq}} \\
    &\implies \Delta x_k := x_{k-1} - x_k = \frac{k+1-np}{\sqrt{npq}} - \frac{k-np}{\sqrt{npq}} =
        \frac{1}{\sqrt{npq}}
\end{align*}

To je integralaska (Riemannova) vsota za funkcijo $f(x) = \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}}$ \\
$P_n(k_1, k_2) \approx \sum_{k=k_1}^{k_2-1} f(x_k) \Delta x_k$ na intervalu $a = \frac{k_1-np}{\sqrt{npq}},
b = \frac{k_2-np}{\sqrt{npq}}$ \\
Za velik $n$ torej velja:

\begin{equation*}
    P_n(k_1, k_2) \approx \int_a^b f(x) dx = \int_{\frac{k_1-np}{\sqrt{npq}}}^{\frac{k_2-np}{\sqrt{npq}}}
    e^{-\frac{x^2}{2}} dx
\end{equation*}

- Laplaceova integralska formula \\
$\Phi(x) = \frac{1}{\sqrt{2\pi}} \int_0^x e^{-\frac{t^2}{2}} dt$ - verjetnostni integral



% 5. predavanje; 4.11.

Vpeljimo verjetnostni integral

\begin{equation*}
    \Phi(x) = \frac{1}{\sqrt{2\pi}} \int_0^x e^{-\frac{t^2}{2}} dt
\end{equation*}

$\Phi$ je liha funkcija, zvezno odvedljiva in strogo nara"s"cajo"ca \\
$\Phi(0) = 0$ in $\Phi(x) = f(x) = \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}}$ \\
Poka"zimo, da je $\lim_{x \to \infty} \Phi(x) = \frac{1}{2}$. S pomo"cjo $\Gamma$ funkcije imamo

\begin{align*}
    &\lim_{x \to \infty} \Phi(x) = \frac{1}{\sqrt{2\pi}} \int_{0}^{\infty} e^{-\frac{x^2}{2}} dt = \\
    &x = \frac{t^2}{2}, dx = t dt, dt = \frac{dx}{t} = \frac{dx}{\sqrt{2x}} \\
    &= \frac{1}{\sqrt{2\pi}} \int_{0}^{\infty} e^{-x} \frac{dx}{\sqrt{2x}} = \\
    &= \frac{1}{2\sqrt{\pi}} \int_{0}^{\infty} x^{-\frac{1}{2}} e^{-x} dx = \\
    &\stackrel{\Gamma(\frac{1}{2}) = \sqrt{pi}}{=} \frac{1}{2}
\end{align*}

Laplaceova formula se glasi:

\begin{equation*}
    P_n(k_1, k_2) = \Phi(\frac{k_2 - np}{\sqrt{npq}}) - \Phi(\frac{k_1 - np}{\sqrt{npq}})
\end{equation*}

\begin{ex}
    Kaljivost semen \\
    Kolik"sna je verjetnost, da vzkali ve"c kot $950$ semen v zavoj"cku s $1000$ semeni \\
    $A$: seme ne vzkali, $p = P(A) = 0.05, q = 0.95, n = 1000 \implies np = 1000$

    \begin{align*}
        &P_{1000}(0,50) = \Phi(\frac{50-50}{\sqrt{50 \cdot 0.95}}) - \Phi(\frac{0-50}{\sqrt{50 \cdot 0.95}}) \doteq \\
        &\doteq \Phi(7.36) \approx 0.500
    \end{align*}

    - verjetnost, da ne vzkali manj kot $50$ semen
\end{ex}

\subsection{slu"cajne spremenljivke}

Danemu poskusu priredimo dolo"ceno "stevilsko koli"cino, katere verjetnost je odvisna od slu"cajna

\begin{ex} \text{} \\
    \begin{enumerate}
        \item Met kocke, "stevilo pik
        \item Streljanje v tar"co, razdalja zadetka od sredi"s"ca tar"ce
    \end{enumerate}
\end{ex}

\begin{defn}[Slu"cajna spremenljivka]
    Realna slu"cajna spremenljivka na verjetnostnem prostoru $(\Omega, \Phi, P)$ je funkcije $X: \Omega \to \R$ z
    lastnostjo, da je za $\forall x \in \R$ mno"zica $\{\omega \in \Omega: X(\omega) \leq x\}$ v $\Phi$, se pravi dogodek
\end{defn}

Oznaka: $\{\omega \in \Omega: X(\omega) \leq x\} \equiv X^{-1}((-\infty, x]) \equiv (X \leq x)$ (ali $\{X \leq x\}$)

\begin{defn}[Porazdelitvena funkcija]
    Porazdelitvena funkcija $F_X: \R \to \R$ je funkcija, definirana s predpisom $F_X(x) = P(X \leq x) \equiv P((X \leq x))$
\end{defn}

Dogovor: $P((X \leq x)) \leftrightarrow P(X \leq x)$ \\
Lastnosti porazdelitvene funkcije $F_X \equiv F$:

\begin{enumerate}
    \item $0 \leq F(X) \leq 1$ za $\forall x \in \R$ (verjetnost)
    \item $F$ je nara"s"cajo"ca funkcija, t.j. iz $x_1 < x_2$ sledi $F(x_1) \leq F(x_2)$
        \begin{proof}
            sledi iz $(X \leq x_1) \subseteq (X \leq x_2) \quad / P()$
        \end{proof}
    \item $\lim_{x \to \infty} F(x) = 1, \lim_{x \to -\infty} F(x) = 0$
        \begin{proof}
            limita $\lim_{x \to \infty} F(x)$ obstaja, ker je $F$ nara"s"cajo"ca in navzgor omejena z 1. \\
            Vzemimo strogo nara"s"cajo"ce zaporedje $\{x_n\} \subseteq \R$, ki je neomejeno. Potem je
            \begin{align*}
                &\lim_{x \to \infty} F(x) = \lim_{n \to \infty} F(x_n) = \lim_{n \to \infty} P(X \leq x_n) = \\
                &\cup_{n = 1}^{\infty} (X \leq x_n) = \Omega: \\
                &\qquad (\subseteq): \text{logi"cno} \\
                &\qquad (\supseteq): \omega \in \Omega \implies \exists n \in \N: X(\omega) = x_n \\
                &\qquad \quad \implies \omega \in (X \leq x_n) \\
                &\stackrel{P \text{ je zvezna}}{=} P(\cup_{n=1}^{\infty} (X \leq x_n)) = P(\Omega) = 1
            \end{align*}

            Drugo poka"zemo podobno (namesto $\cup$ je $\cap$)
        \end{proof}
    \item $F$ je zvezna z desne, t.j. $F(X+) = F(X) \forall x \in \R$
        \begin{proof}
            obstoj limite ni problemati"cen: $F(x+) = \lim_{x \to 0} F(x+h) = \lim_{n \to \infty} F(x_n)$, kjer
            je $\{x_n\}_n \subseteq \R$ strogo nara"s"cajo"ce zaporedje z limito v $x$ \\
            $\{(X \leq x_n)\}_{n \in \N}$ je padajo"ce zaporedje s presekom

            \begin{align*}
                &\{(X \leq x_n)\} = \cap_{n=1}^{\infty} \{\omega \in \Omega: X(\omega) \leq x_n\} = \\
                &= \{\omega \in \Omega: X(\omega) \leq x\} = (X \leq x): \\
                &\qquad (\supseteq): \text{ o"citno} \\
                &\qquad (\subseteq): \omega \in \Omega \implies \text{za vsak n izpolnjeno} \implies    
                    \lim \text{ obstaja}
            \end{align*}

            \begin{align*}
                &F(x+) = \lim_{n \to \infty} F(x_n) = \lim_{n \to \infty} P(X \leq x_n) = \\
                &= P(\cap_{n=1}^{\infty} (X \leq x_n)) = P(X \leq x) = F(x)
            \end{align*}
        \end{proof}
    \item $F(X-) = P(X < x) \neq F(x)$ v splo"snem
        \begin{align*}
            &P(x_1 < X \leq x_2) = P((X \leq x_2) \text{\textbackslash} (X \leq x_1)) = \\
            &= P(X \leq x_2) - P(X \leq x_1) = F(x_2) - F(x_1) \\
            &P(x_1 < X < x_2) = P(X < x_2) - P(X \leq x_1) = F(x_2-) - F(x_1) \\
            &P(x_1 \leq X \leq x_2) = F(x_2) - F(x_1-) \\
            &P(x_1 \leq X < x_2) = F(x_2-) - F(x_1-)
        \end{align*}
\end{enumerate}

\begin{rem}
    V nekaterih u"cbenikih je porazdelitvena funkcija definirana z $F(x) = P(X < x)$ - zvezna z leve
\end{rem}

Najpomembnej"sa razreda slu"cajnih spremenljivk sta

\subsubsection{Diskretna slu"cajna spremenljivka}

\begin{defn}[Diskretna slu"cajna spremenljivka]
    Slu"cajna spremenljivka $X: \Omega \to \R$ je diskretno porazdeljena, "ce je njena zaloga vrednosti kon"cna ali
    "stevna mno"zica. Naj bo $\{x_1, x_2 \cdots\}$ zaloga vrednosti slu"cajne spremenljivke $X$.
\end{defn}

Vpeljimo verjetnostno funkcijo $p_n := P(X = x_n) \; n = 1, 2 \cdots$. Potem je

\begin{equation*}
    \sum_n p_n = P(\cup_n (X = x_n)) = P(\Omega) = 1
\end{equation*}

in

\begin{align*}
    &F_X(x) = P(X \leq x) = P(\cup_{n: x_n \leq x} (X = x_n)) = \\
    &\text{paroma nezdru"zljivi dogodki} \\
    &= \sum_{n: x_n \leq x} P(X = x_n) = \sum_{n: x_n \leq x} p_n
\end{align*}

npr. naj bodo $x_1 < x_2 < x_3$ v zalogi vrednosti slu"cajne spremenljivke $X$ \\
$F$ je odsekoma konstantna

\begin{equation*}
    X: \begin{pmatrix}
        x_1 & x_2 & \cdots \\
        p_1 & p_2 & \cdots
    \end{pmatrix}
\end{equation*}

Pomembnej"se diskretne porazdelitve:

\subsubsection{Enakomerna diskretna porazdelitev} na $n$ to"ckah

\begin{equation*}
    X: \begin{pmatrix}
        x_1 & x_2 & \cdots & x_n \\
        \frac{1}{n} & \frac{1}{n} & \cdots & \frac{1}{n}
    \end{pmatrix}
\end{equation*}

\begin{ex}
    Met kocke, $X: \begin{pmatrix}
        1 & 2 & 3 & 4 & 5 & 6 \\
        \frac{1}{6} & \frac{1}{6} & \frac{1}{6} & \frac{1}{6} & \frac{1}{6} & \frac{1}{6}
    \end{pmatrix}$
\end{ex}

\subsubsection{Binomska porazdelitev}

$Bin(n,p), n \in \N, p \in (0,1), n-$krat ponovimo poskus, gledamo dogodek $A$ z verjetnostjo $P(A) = p$, $X$ je
frekvenca dogodka $A$ v $n$ ponovitvah

\begin{align*}
    &X: \begin{pmatrix}
            0 & 1 & \cdots & n \\
            p_0 & p_1 \cdots p_n
        \end{pmatrix} \\
    &p_k = \binom{n}{k} p^k q^{n-k}
\end{align*}

\begin{ex}
    $n$-krat vr"zemo kocko. $X$ je frekvenca "sestice. $X \sim Bin(n, \frac{1}{6})$
\end{ex}



% 6. predavanje: 11.11.


\subsubsection{Poissonova porazdelitev}

$Poi(\lambda), \lambda > 0$

\begin{align*}
    &p_k = P(X = k) = \frac{\lambda^k}{k!} e^{-\lambda} \; k = 0, 1, 2 \cdots \\
    &\sum_{k=0}^{\infty} p_k = (\sum_{k=0}^{\infty} \frac{\lambda^k}{k!}) e^{-\lambda} =
        e^{\lambda} e^{-\lambda} = 1
\end{align*}

$\implies$ to je res porazdelitev ($p_i \geq 0, \sum p_i = 1$)

\begin{ex}
    "Stevilo klicev v telefonskem omre"zju v "casovni enoti \\
    Binomska, velik $n$, majhen $p \implies$ Poissonova \\
    Lahko modeliramo z binomsko porazdelitvijo $Bin(n,p)$, kjer je $n$ "stevilo naro"cnikov in $p$ verjetnost, da se
    posameznik odlo"ci za klic v "casovni enoti. Ker je $n$ velik in $p$ majhen, je to pribli"zno $Poi(\lambda)$, kjer
    je $\lambda = np$ (v praksi ni za vse ista) 
\end{ex}

\begin{ex}
    "Stevilo napa"cnih "crk v knjigi \\
    (Veliko "crk v knigi, melo verjetno, da se zmotimo.) \\
    Lahko modeliramo z $Bin(n,p)$, kjer je $n$ "stevilo vseh "crk v knjigi, $p$ je verjetnost, da si izberemo
    napa"cno "crko \\
    Ker je $n$ velik, $p$ pa majhen, lahko to aproksimiramo s $Poi(\lambda)$, kjer je $\lambda = np$ \\
    Raje vzamemo $Poi(\lambda)$ kot $Bin(n,p)$, ker je preprostej"sa
\end{ex}

\subsubsection{Geometrijska porazdelitev}

$Geo(p), \; p \in (0,1)$ \\
Ponavljamo poskus, v katerem opazujemo dogodek $A$ s $P(A) = p, q = 1-p$. $(X=k)$ je dogodek, da se $A$ zgodi prvi"c
v k-ti ponovitvi

\begin{equation*}
    p_k = P(A = k) = p \cdot q^{k-1} \; k = 1, 2 \cdots
\end{equation*}

\begin{equation*}
    \sum_{k=1}^{\infty} p_k = p \cdot \sum_{k=1}^{\infty} q^{k-1} = p \sum_{k=0}^{\infty} q^k =
    p \frac{1}{1-q} = \frac{p}{p} = 1
\end{equation*}

\begin{ex}
    Me"cemo kocko, $X$ je "stevilo metov, da pade "sestica prvi"c. Potem je $X \sim Geo(\frac{1}{6})$
\end{ex}

\subsubsection{Pascalova ali negativna binomska porazdelitev}

$Pas(m,p), \; m \in \N, p \in (0,1)$ \\
Ponavljamo poskus, v katerem nas zanima dogodek $A$ s $P(A) = p$. $(X=k)$ je dogodek, da se $A$ zgodi m-ti"c v
k-ti ponovitvi poskusa. Torej $Pas(1,p) = Geo(p)$

\begin{equation*}
    p_k = P(X = k) = \binom{k-1}{m-1} p^m q^{k-m} \; k = m, m+1 \cdots
\end{equation*}

($A$ se zgodi ($m-1$)-krat, $\overline{A}$ pa ($k-m$)-krat) \\
DN: Enakost $\sum_{k=m}^{\infty} p_k = 1$ analiti"cno preverimo z ($m-1$)-kratnim odvajanjem geometrijske vrste

\begin{equation*}
    \sum_{k=0}^{\infty} q^{k-1} = \frac{1}{1-q}
\end{equation*}

oz. z direktno uporabo binomske vrste:

\begin{equation*}
    (1-q)^{-m} = \sum_{j=0}^{\infty} \binom{-m}{j} q^j
\end{equation*}

\begin{ex}
    Me"cemo kocko, $X$ je "stevilo potrebnih metov, da pade "sestica $m$-krat. Potem je $X \sim Pas(m, \frac{1}{6})$
\end{ex}

\subsubsection{Hipergeometrijska porazdelitev}

$Hip(n; M, N), \; 0 < M < N, n,M,N \in \N, n \leq \min\{M, N-M\}$ \\
V posodi je $N$ kroglic, od tega $M$ belih, ostale "crne. Slu"cajno izberemo $n$ kroglic (brez vra"canja). $X$ je
"stevilo belih kroglic med izbranimi kroglicami. Torej $(X=k)$ je dogodek, da je med izbranimi $n$ kroglicami $k$
belih

\begin{align*}
    &p_k = P(X = k) = \frac{\binom{m}{k} \binom{N-m}{n-k}}{\binom{N}{n}} \; k = 0, 1 \cdots n \\
    &\binom{m}{k} \cdots k \text{ belih} \\
    &\binom{N-m}{n-k} \cdots \text{ ostale "crne} \\
    &\binom{n}{N} \cdots \text{ izberemo } n \text{ izmed } N
\end{align*}

Ker je $\{(X = k)\}^n$ popoln sistem dogodkov, je jasno, da je $\sum_{k=0}^{n} p_k = 1$ \\
Torej velja binomska identiteta

\begin{equation*}
    \sum_{k=0}^{n} \binom{m}{k} \binom{N-m}{n-k} = \binom{N}{n}
\end{equation*}

- verjetnostni dokaz

\begin{ex}
    V ribniku je $N$ rib, od tega $M$ krapov. Ulovimo $n$ rib. Naj bo $X$ "stevilo ulovljenih krapov. Potem je
    $X \sim Hip(n; M, N)$
\end{ex}

"Ce je $n << \min\{M, N-M\}$, potem je $Hip(n; M, n) \approx Bin(n, \frac{M}{N})$:

\begin{align*}
    &p_k = \frac{\frac{M(M-1) \cdots (M_k+1)}{k!} \frac{(N-m)(N-m+1) \cdots (N-m-n+k+1)}{(n-k)!}}
        {\frac{N(N-1) \cdots (N-n+1)}{n!}} \approx \\
    &\stackrel{\substack{k \leq m \\ n \leq N}}{\approx} \frac{\frac{M^k}{k!} \frac{(N-m)^{n-k}}{(n-k)!}}
        {\frac{N^n}{n!}} = \binom{n}{k} (\frac{M}{N})^k (\frac{N-M}{N})^{n-k} = \binom{n}{k} p^k q^{n-k}
\end{align*}

Intuicija: vzemanje kroglic, $n << \min\{M, N-M\}$ \\
"Ce je $n << \min\{M, N-M\}$, ne naredimo velike napake, "ce kroglice vra"camo. Tedaj je "stevilo belih
izvle"cenih kroglic binomsko porazdeljeno: $X \sim Bin(n, \frac{M}{N})$

\subsubsection{Zvezno porazdeljene slu"cajne spremenljivke}

\begin{defn}[Zvezna porazdelitev]
    Slu"cajna spremenljivka $X$ je zvezno porazdeljena (zvezna), "ce obstaja nenegativa integrabilna funkcije
    $p_X$, imenovana gostota porazdelitve, da je

    \begin{equation*}
        F_X(x) = \int_{-\infty}^{\infty} p_X(t) dt \text{ za } \forall x \in \R
    \end{equation*}
\end{defn}

Analogija z diskretnimi porazdelitvami: $F_X(x) = \sum_{n: X_n \leq x} p_k, X: \begin{pmatrix}
    x_1 & \cdots \\
    p_1 & \cdots
\end{pmatrix}$ \\
Tedaj je $F_X$ zvezna funkcija. V to"ckah, kjer je $p_X$ zvezna, je $F_X$ zvezno odvedljiva in velja
$F_X^{'}(x) = p_X(x)$ \\
Ker je $\lim_{x \to \infty} F_X(x) = 1$, je $\int_{-\infty}^{\infty} p_X(t) dt = 1$ \\
Za $x_1 < x_2$ velja

\begin{equation*}
    P(x_1 < X < x_2) = F_X(x_2-) - F_X(x_1+) = \int_{-\infty}^{x_2} p_X(t) dx - \int_{-\infty}^{x_1} p_X(t) dt =
    \int_{x_1}^{x_2} p_X(t) dt
\end{equation*}

Pomembnej"se zvezne porazdelitve:

\subsubsection{Enakomerna zvezna porazdelitev na $[a,b]$}

\begin{align*}
    &p_X(x) = \begin{cases}
            \frac{1}{b-a} \text{ "ce } a < x < b \\
            0 \text{ sicer}
        \end{cases} \\
    &F_X(x) = \begin{cases}
        0 \text{ "ce } x \leq a \\
        \frac{x-a}{b-a} \text{ "ce } a < x < b \\
        1 \text{ "ce } x \geq b
    \end{cases}
\end{align*}

\begin{ex}
    Slu"cajno izberemo $X$ na $[0,1]$
\end{ex}

\subsubsection{Normalna ali Gaussova porazdelitev}

$N(\mu, \sigma), \; \mu \in \R, \sigma > 0$

\begin{equation*}
    p_X(x) = \frac{1}{\sigma \sqrt{2\pi}} e^{-\frac{1}{2} (\frac{x-\mu}{\sigma})^2}
\end{equation*}

$N(0,1): p_X(x) = \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}}$ - standardizirana normalna porazdelitev



% 7. predavanje: 18.11.

$\sigma$ velik: \\
$\sigma$ majhen: \\
Porazdelitvena funkcija:

\begin{align*}
    &F(X) = \frac{1}{\sigma \sqrt{2\pi}} \int_{-\infty}^{x} e^{-\frac{1}{2} (\frac{t-\mu}{\sigma})^2} dt = \\
    &u = \frac{t-\mu}{\sigma}, du = \frac{dt}{\sigma} \\
    &= \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\frac{x-\mu}{\sigma}} e^{-\frac{1}{2}u^2} du = \\
    &= \frac{1}{\sqrt{2\pi}} (\int_{-\infty}^{0} \cdots + \int_{0}^{\frac{x-\mu}{\sigma}} \cdots) = \\
    &= \frac{1}{2} + \Phi(\frac{x-\mu}{\sigma})
\end{align*}

Laplaceova integralaska formula pravi, da je $Bin(n,p) \approx N(np, \sqrt{npq})$ za velik $n:$

\begin{equation*}
    P_n(k) = \frac{1}{\sqrt{2\pi npq}} - \frac{1}{2} (\frac{k - np}{\sqrt{npq}})^2
\end{equation*}

\begin{ex}
    Sistoli"cni krvni tlak \\
    %slika%
    verjetnost, da ima slu"cajno oseba krvni tlak med $120$ in $130$ mmHg
\end{ex}

\subsubsection{Eksponentna porazdelitev}

\begin{align*}
    &Exp(\lambda), \lambda > 0 \\
    &p(x) = \begin{cases}
            \lambda e^{-\lambda x} \lambda \geq 0 \\
            0 \quad \text{sicer}
        \end{cases}
\end{align*}

\begin{equation*}
    F(x) = \begin{cases}
        1 - e^{-\lambda x} \text{ "ce } x \geq 0 \\
        0 \quad \text{ "ce } x \leq 0
    \end{cases}
\end{equation*}

\begin{ex}
    Radioaktivni razpad \\
    $F(x)$ je verjetnost, da se radioaktivni razpad zgodi pred trenutkom $x \in \R^{+}$
\end{ex}

\subsubsection{Porazdelitev gama}

\begin{align*}
    &\Gamma(b,c), \; b, c > 0 \\
    &p(x) = \begin{cases}
            \frac{c^b}{\Gamma(b)} x^{b-1} e^{-cx} x > 0 \\
            0 \quad \text{sicer}
        \end{cases}
\end{align*}

O"citno je $Exp(\lambda) = \Gamma(1, \lambda)$

\begin{equation*}
    \Gamma(y) = \int_{0}^{\infty} x^{y-1} e^{-x} dx
\end{equation*}

\begin{align*}
    &\int_{-\infty}^{\infty} p(x) dx = \frac{c^b}{\Gamma(b)} \int_{0}^{\infty} x^{b-1} e^{-cx} dx = \\
    &t = cx, dt = c dx \\
    &= \frac{c^b}{\Gamma(b)} \int_{0}^{\infty} (cx)^{b-1} e^{-cx} c dx = \\
    &= \frac{1}{\Gamma(b)} \cdot \Gamma(b) = 1
\end{align*}

- je porazdelitev

\subsubsection{Porazdelitev $\chi^2(n)$}

(hi-kvadrat), $n \in \N$, $n$ je "stevilo prostorskih stopenj

\begin{align*}
    &\chi^2(n) = \Gamma(\frac{n}{2}, \frac{1}{2}) \\
    &p(x) = \begin{cases}
        \frac{1}{2^{\frac{1}{2}} \Gamma(\frac{1}{2})} x^{\frac{n}{2} - 1} e^{-\frac{x}{2}} x > 0 \\
        0 \qquad \text{sicer}
    \end{cases}
\end{align*}

\subsubsection{Cauchyjeva porazdelitev}

\begin{equation*}
    p(x) = \frac{1}{\pi (1+x^2)} \; x \in \R
\end{equation*}

\begin{align*}
    &F(x) = \frac{1}{\pi} \int_{-\infty}^{\infty} \frac{dt}{1+t^2} = \frac{1}{\pi} \arctan t \vert_{-\infty}^x = \\
    &= \frac{1}{\pi} \arctan x - \frac{1}{\pi} \cdot \frac{\pi}{2} = \frac{1}{\pi} \arctan x + \frac{1}{2}
\end{align*}

\begin{ex}
    Slu"cajna spremenljivka, ki ni niti zvezno niti disktretno porazdeljena \\
    Vr"zemo kovanec, "ce pade grbc, postavimo $X=1$, "ce pade cifra, pa naj bo $X$ slu"cajno izbrano stevilo na
    $[0,2]$ \\
    Izra"cunamo porazdelitveno funkcijo:

    \begin{align*}
        &F(x) = P(X \leq x) = \stackrel{x \in [0,2]}{=} P(\text{grb}) \cdot P(X \leq x \mid \text{grb}) +
            P(\text{cifra}) \cdot P(X \leq x \mid \text{cifra})
    \end{align*}

    "Ce je $0 \leq x \leq 1$, potem je

    \begin{equation*}
        F(x) = \frac{1}{2} \cdot 0 + \frac{1}{2} \cdot \frac{x}{2} = \frac{x}{4}
    \end{equation*}

    "Ce je $1 \leq x \leq 2$, potem je

    \begin{equation*}
        F(x) = \frac{1}{2} \cdot 1 + \frac{1}{2} \cdot \frac{x}{2} = \frac{1}{2} + \frac{x}{4}
    \end{equation*}

    \begin{align*}
        F(x) = \begin{cases}
            0 \text{ "ce } x \leq 0 \\
            \frac{x}{4} \text{ "ce } 0 \leq x < 1 \\
            \frac{1}{2} + \frac{x}{4} \text{ "ce } 1 \leq x \leq 2 \\
            1 \text{ "ce } x \geq 2
        \end{cases}
    \end{align*}

    Ker $F$ ni zvezna funkcija, $X$ ni zvezno porazdeljena \\
    Ker $F$ ni odsekoma konstantna, $X$ ni diskretno porazdeljena
\end{ex}

\subsection{Slu"cajni vektorji}

\begin{defn}[Slu"cajni vektor]
    Naj bo $(\Omega, \Phi, P)$ verjetnostni prostor. Slu"cajni vektor je n-terica slu"cajnih spremenljivk
    $x = (x_1 \cdots x_n): \Omega \to \R^n$ z lastnostjo, da je mno"zica
    
    \begin{equation*}
        (X_1 \leq x_1 \cdots X_n \leq x_n) := \{\omega \in \Omega: X_1(\omega) \leq x_1 \cdots X_n(\omega) \leq x_n\}
    \end{equation*}

    dogodek za vse n-terice $x = (x_1 \cdots x_n)$, se pravi v $\Phi$ za $\forall x = (x_1 \cdots x_n) \in \R^n$
\end{defn}

\begin{defn}[Porazdelitvena funkcija]
    Porazdelitvena funkcija slu"cajnega vektorja $X = (X_1 \cdots X_n)$ je funkcija, definirana z

    \begin{equation*}
        F_X(x) = F_{(X_1 \cdots X_n)}(x_1 \cdots x_n) := P(X_1 \leq x_1 \cdots X_n \leq x_n)
    \end{equation*}

    Torej $F_X: \R^n \to \R$
\end{defn}

$F_X$ ima podobne lastnosti kot v primeru $n=1$ \\
O"citno je $0 \leq F_X(x) \leq 1$ za $\forall x \in \R^n$, glede na vsako spremenljivko je $F_X$ nara"s"cajo"ca
in z desne zvezna, velja "se:

\begin{equation*}
    \lim_{\substack{x_1 \to \infty \\ \vdots \\ x_n \to \infty}} F_{(X_1 \cdots X_n)}(x_1 \cdots x_n) = 1
\end{equation*}

\begin{defn}[Robna porazdelitev]
    "Ce po"sljemo v $\infty$ samo nekatere spremenljivke, dobimo porazdelitveno funkcijo slu"cajnega podvektorja, npr.

    \begin{equation*}
        \lim_{\substack{x_2 \to \infty \\ \vdots \\ x_n \to \infty}} F_{(X_1 \cdots X_n)}(x_1 \cdots x_n) = F_{X_1}(x_1)
    \end{equation*}

    ali pa

    \begin{equation*}
        \lim_{x_n \to \infty} F_{(X_1 \cdots X_n)}(x_1 \cdots x_n) =
            F_{X_1 \cdots X_{n-1}}(x_1 \cdots x_{n-1})
    \end{equation*}

    Takim porazdelitvam re"cemo robne (marginalne) porazdelitve
\end{defn}

Oglejmo si dvorazse"zni primer ($n=2$):

\begin{equation*}
    (X,Y): \Omega \to \R^2
\end{equation*}

za $\forall (x,y) \in \R^2$ je 

\begin{equation*}
    (X \leq x, Y \leq y) := \{\omega \in \Omega: X(\omega) \leq x, Y(\omega) \leq y\}
\end{equation*}

dogodek \\
Porazdelitvena funkcija $F_{(X,Y)}: \R^2 \to \R$ je definirana z

\begin{align*}
    &F_{(X,Y)}(x,y) := P(X \leq x, Y \leq y) \\
    &\lim_{x \to \infty} F_{(X,Y)}(x,y) = P(Y \leq y) = F_Y(y) \\
    &\lim_{y \to \infty} F_{(X,Y)}(x,y) = P(X \leq x) = F_X(x) \\
\end{align*}

Izrazimo $P(a < X \leq b, c < Y \leq d)$ s porazdelitveno fukncijo $F(X,Y) = F$. To bo posplo"sitev formule

\begin{equation*}
    P(a < X \leq b) = F_X(b) - F_X(a)
\end{equation*}

ki smo jo imeli v primeru $n=1$



% 8. predavanje: 25.11.

\begin{align*}
    &(X,Y): \Omega \to \R^2 \text{ slu"cajni vektor} \\
    &F_{(X,Y)}(x,y) = P(X \leq x, Y \leq y) = P((x,y) \in (-\infty, x] \times (-\infty, y])
\end{align*}

Izrazimo z $F_{(X,Y)} = F$ verjetnost $P(a < X < b, c < Y < d)$. To bo posplo"sitev formule
$P(a < X < b) = F_X(b) - F_X(a)$ \\
Najprej vzemimo posebni primer:

\begin{align*}
    &P(a < X \leq b, Y \leq d) = P((X \leq b, Y \leq d) \text{\textbackslash} (X \leq a, Y \leq d)) = \\
    &=P (X \leq b, Y \leq d) - P(X \leq a, Y \leq d) = F(b,d) - F(a,b)
\end{align*}

V splo"snem primeru pa imamo

\begin{align*}
    &P(a < X \leq b, c < Y \leq d) = P((a < X \leq b, Y \leq d) \text{\textbackslash}
        (a < X \leq b, Y \leq c)) = \\
    &= P(a < X \leq b, Y \leq d) - P(a < X \leq b, Y \leq c) = \\
    &\stackrel{\text{fiks. y}}{=} (F(b,d) - F(a,d)) - (F(b,c) - F(a,c))
\end{align*}

Torej je

\begin{equation*}
    P(a < X \leq b, c < Y \leq d) = F(b,d) - F(a,d) - F(b,c) + F(a,c)
\end{equation*}

Najpomembnej"sa razreda ve"crazse"znih porazdelitev sta

\subsubsection{Diskretne porazdelitve}

\begin{defn}
    Slu"cajni vektor $X = (X_1 \cdots X_n): \Omega \to \R^n$ je diskretno porazdeljen, "ce je njegova zaloga vrednosti
    kon"cna/"stevna mno"zica to"ck v $\R^n$. Omejimo se na $n=2: \Omega \to \R^2$. \\
    Naj bo $\{x_1, x_2 \cdots\}$ zaloga vrednosti slu"cajne spremenljivke $X$ in $\{y_1, y_2 \cdots\}$ zaloga
    vrednosti slu"cajne spremenljivke $Y$. Potem je zaloga vrednosti vektorja $(X,Y)$ vsebovana v
    $\{(x_i, y_i): i = 1,2 \cdots j = 1,2 \cdots \}$. \\
    Definiramo verjetnostno funkcijo $p_{ij} := P(X = x_i, Y = y_j) i = 1,2 \cdots j = 1,2 \cdots$ \\
    Ker je $\{(X = x_i, Y = y_j)\}_{ij}$ popoln sistem dogodkov, je $\sum_i \sum_j p_{ij} = 1$
\end{defn}

%tabelca
\begin{align*}
    &X: \begin{pmatrix}
            x_1 & x_2 & \cdots \\
            p_1 & p_2 & \cdots
        \end{pmatrix} \\
    &p_i = P(X = x_i) = P(\cup_j (X = x_i, Y = y_j)) = \sum_j P(X = x_i, Y = y_j) = \sum_j p_{ij} \; i = 1,2 \cdots \\
    &\text{"ce je } Y: \begin{pmatrix}
            y_1 & y_2 & \cdots \\
            q_1 & q_2 & \cdots
        \end{pmatrix} \text{, je } \\
    &q_j = P(Y = y_i) = P(\cup_i (X = x_i, Y = y_j)) = \sum_i P(X = x_i, Y = y_j) = \sum_i p_{ij} \; j = 1,2 \cdots
\end{align*}

\begin{ex}
    Met dveh kock: $X$ "stevilo pik na 1. kocki, $Y$ na 2.
\end{ex}

\subsubsection{Zvezne porazdelitve}

\begin{defn}
    Slu"cajni vektor $X = (X_1 \cdots X_n)$ je zvezno porazdeljen, "ce obstaja integrabilna funkcija $p_X: \R^n \to \R$,
    imenovana gostota porazdelitve, da je

    \begin{align*}
        &F_X(x) = F_{(X_1 \cdots X_n)}(x_1 \cdots x_n) = \int_{-\infty}^{x_1} dt_1 \int_{-\infty}^{x_2} dt_2 \cdots
            \int_{-\infty}^{x_n} p_X(t_1 \cdots t_n) dt_n \text{ za } \forall x = (x_1 \cdots x_n) \in \R^n
    \end{align*}
\end{defn}

Ker je $\lim_{\substack{x_1 \to \infty \\ \vdots \\ x_n \to \infty}} F_X(x_1 \cdots x_n) = 1$, je

\begin{equation*}
    \int \cdots_{\R^n} \int p_X(t_1 \cdots t_n) dt_1 \cdots dt_n = 1
\end{equation*}

Za vsako Borelovo mno"zico $A \subseteq \R^n$ (najmanj"sa $\sigma$-algebra z vsemi odprtimi pravokotniki) je

\begin{equation*}
    P(X \in A) \equiv P((x_1 \cdots x_n) \in A) = \int \cdots_{A} \int p_X(t_1 \cdots t_n) dt_1 \cdots dt_n
\end{equation*}

Omejimo se na $n=2: F_{(X,Y)}(x,y) = \int_{-\infty}^{x} du \int_{-\infty}^{y} p_{(X,Y)}(u,v) dv$ \\
Robni porazdelitvi sta:

\begin{align*}
    &F_X(x) = \lim_{y \to \infty} F_{(X,Y)}(x,y) = \text{ (brez utemeljevanja)} \\
    &= \int_{-\infty}^{x} du \int_{-\infty}^{\infty} p_{(X,Y)}(u,v) dv
\end{align*}

ki ima gostoto

\begin{align*}
    p_X(x) = \int_{-\infty}^{\infty} p_{(X,Y)}(x,y) dy
\end{align*}

in

\begin{align*}
    &F_Y(y) = \lim_{x \to \infty} F_{(X,Y)}(x,y) = \\
    &= \int_{-\infty}^{y} dv \int_{-\infty}^{\infty} p_{(X,Y)}(u,v) du
\end{align*}

ki ima gostoto

\begin{align*}
    p_Y(y) = \int_{-\infty}^{\infty} p_{(X,Y)}(x,y) dx
\end{align*}

(ekvivalentno vsoti v diskretnem primeru). \\
Najpomembnej"sa dvorazse"zna zvezna porazdelitev je normalna:

\begin{align*}
    &N(\mu_x, \mu_y, \sigma_x, \sigma_y, \rho), \mu_x, \mu_y \in \R, \sigma_x, \sigma_y > 0, \rho \in (-1,1) \\
    &p(x,y) = \frac{1}{2\pi \sigma_x \sigma_y \sqrt{1-\rho^2}} e^{-\frac{1}{2 (1-\rho^2)}
        ((\frac{x-\mu_x}{\sigma_x})^2 - 2 \rho \frac{x-\mu_x}{\sigma_x} \frac{y-\mu_y}{\sigma_y} +
        (\frac{y-\mu_y}{\sigma_y})^2)} \\
    &(\mu_x, \mu_y) \text{ premik, } (\sigma_x, \sigma_y) \text{ razteg} \\
    &N(0,0,1,1,\rho): p(x,y) = \frac{1}{2\pi \sqrt{1-\rho^2}} e^{-\frac{1}{2 (1-\rho^2)} (x^2 - 2 \rho x y + y^2)}
\end{align*}

Nivojnice, izohipse se: $x^2 - 2 \rho x y + y^2 = c$

\begin{itemize}
    \item $\rho = 0$: kro"znica
    \item $\rho \in (-1,1)$: elipsa
\end{itemize}

Robni porazdelitvi sta

\begin{equation*}
    p_X(x) = \int_{-\infty}^{\infty} p(x,y) dx = \cdots = \frac{1}{\sigma_x \sqrt{2\pi}}
    e^{-\frac{1}{2} (\frac{x-\mu_x}{\sigma_x})^2}
\end{equation*}

torej $X \sim N(\mu_x, \sigma_x)$. Podobno $Y \sim N(\mu_y, \sigma_y)$

\begin{ex}
    Krvni tlak, $X$ je sistoli"cni, $Y$ je diastoli"cni krvni tlak \\
    $\mu_x = 120, \mu_y = 75, \rho \doteq 0.7$
\end{ex}

Dvorazse"zna normalna porazdelitev je posebni primer ve"crazse"zne normalne porazdelitve $N(\mu, A)$, kjer je
$\mu = (\mu_1 \cdots \mu_n)^T$ in $A$ pozitivno definitna matrika. \\
Gostota v to"cki $x = (x_1 \cdots x_n)^T$ je

\begin{align*}
    &p(X) = \sqrt{\frac{det A}{(2\pi)^n}} e^{-\frac{1}{2} (x-\mu)^T A (x-\mu)} \\
    &(x-\mu)^T A (x-\mu) = \langle A(x-\mu), x-\mu \rangle
\end{align*}

Za dokaz enakosti

\begin{equation*}
    \int \cdots_{\R^n} \int p(x) dx_1 \cdots dx_n = 1
\end{equation*}

izra"cunajmo integral

\begin{equation*}
    \int \cdots_{\R^n} \int e^{-\frac{1}{2} (x-\mu)^T A (x-\mu)} dx_1 \cdots dx_n =
    \sqrt{\frac{(2\pi)^n}{det A}}
\end{equation*}



% 9. predavanje: 2.12.

$N(\mu, A), \mu \in \R^n, A \in \R^{m \times n}$ pozitivna definitna matrika, t.j. sebi adjungirana matrika,
za katero velja

\begin{equation*}
    x^T A x = \langle Ax, x \rangle > 0 \; \forall x \in \R^n \text{\textbackslash} \{0^n\}
\end{equation*}

V to"cki $x = (x_1 \cdots x_n)^T$ je

\begin{equation*}
    p(x) = \sqrt{\frac{detA}{(2\pi)^n}} \cdot e^{-\frac{1}{2} (x-\mu)^T A (x-\mu)}
\end{equation*}

Izra"cunajmo integral

\begin{align*}
    &\int \underbrace{\cdots}_{\R^n} \int e^{-\frac{1}{2} (x-\mu)^T A (x-\mu)} dx = \\
    &y = x - \mu \implies dy = dx \\
    &= \int \underbrace{\cdots}_{\R^n} \int e^{-\frac{1}{2} y^T A y} dy
\end{align*}

Ker je $A$ pozitivna definitna matrika, obstaja ortogonalna matrika $U$ in diagonalna matrika
$D = diag(\lambda_1 \cdots \lambda_n)$, da je $A = U^T D U$

\begin{align*}
    &= \int \underbrace{\cdots}_{\R^n} \int e^{-\frac{1}{2} y^T U^T D U y} dy = \\
    &z = U y, y = U^T z, dy = |det U^T| dz = dz \\
    &= \int \underbrace{\cdots}_{\R^n} \int e^{-\frac{1}{2} z^T D z} dz = \\
    &= \int \underbrace{\cdots}_{\R^n} \int e^{-\frac{1}{2} (\lambda_1 z_1^2 + \cdots + \lambda_n z_n^2)}
        dz_1 \cdots dz_n = \\
    &= \int_{\R} e^{-\frac{1}{2} \lambda_1 z_1^2} dz_1 \cdots \int_{\R} e^{-\frac{1}{2} \lambda_1 z_n^2} dz_n = \\
\end{align*}

Ker je $\int_{\R} e^{-\frac{1}{2} \lambda z^2} dz = \sqrt{\frac{2\pi}{\lambda}}$ - $z \in \R$ - s pomo"cjo
$\Gamma$ funkcije, Bronsterin, sledi iz

\begin{equation*}
    \frac{1}{\sqrt{2\pi}\sigma} = \int_{\R} e^{-\frac{1}{2} (\frac{x}{\sigma})^2} dx = 1
\end{equation*}

Gostota za $N(0, \sigma), \lambda := \frac{1}{\sigma^2}, \sigma = \frac{1}{\sqrt{\lambda}}$

\begin{align*}
    &= \sqrt{\frac{2\pi}{\lambda_1}} \cdot \cdots \cdot \sqrt{\frac{2\pi}{\lambda_1}} =
        \sqrt{\frac{(2\pi)^n}{det A}}
\end{align*}

Torej je $\int \underbrace{\cdots}_{\R^n} \int p(x) dx = 1$ \\
Dvoraz"se"zni primer je posebni primer

\begin{align*}
    &A = \frac{1}{1-\rho^2} \begin{bmatrix}
            \frac{1}{\sigma_x^2} & -\frac{\rho}{\sigma_x \sigma_y} \\
            -\frac{\rho}{\sigma_x \sigma_y} & \frac{q}{\sigma_y^2} \\
        \end{bmatrix}, \mu = \begin{bmatrix}
            \mu_x \\
            \mu_y
        \end{bmatrix} \\
    &det A = \frac{1}{1-\rho^2} (\frac{1}{\sigma_x^2 \sigma_y^2} - \frac{\rho^2}{\sigma_x^2 \sigma_y^2})
        \stackrel{\text{?}}{=} \frac{1}{\sigma_x^2 \sigma_y^2}
\end{align*}

$K = A^{-1} = \begin{bmatrix}
    \sigma_x^2 & \rho \sigma_x \sigma_y \\
    -\rho \sigma_x \sigma_y & \sigma_y^2
\end{bmatrix}$ kovarian"cna matrika (slu"cajnemu vektorju $X,Y$)

\subsection{Neovdisnost slu"cajnih spremenljivk}

\begin{defn}[Neodvisnost]
    Slu"cjane spremenljivke $x_1, x_2 \cdots x_n$ v slu"cjanem vektorju $x = (x_1 \cdots x_n)$ so neodvisne,
    "ce je

    \begin{equation*}
        F_X(x_1 \cdots x_n) = F_{X_1}(x_1) \cdots F_{X_n}(x_n) \text{ za } \forall x \in \R^n
    \end{equation*}
    
    oziroma

    \begin{equation*}
        P(X_1 \leq x_1, X_2 \leq x_2 \cdots X_n \leq x_n) = P(X_1 \leq x_1) \cdots P(X_n \leq x_n)
    \end{equation*}

    oziroma dogodki $(X_1 \leq x_1) \cdots (X_n \leq x_n)$ so neodvisni
\end{defn}

Oglejmo si dvorazse"zni diskretni primer

\begin{claim}
    Naj bo $(X,Y)$ diskretno porazdeljen vektor:

    \begin{equation*}
        p_{ij} = P(X = x_i, Y = y_j), p_i = P(X = x_i), q_j = P(Y = y_j)
    \end{equation*}

    Potem sta $X$ in $Y$ neodvisni $\iff p_{ij} = p_i \cdot q_j \; \forall i,j$
\end{claim}

\begin{proof}
    $F \equiv F_{(X,Y)}$ porazdelitvena funkcija vektorja $(x,y)$ \\
    $(\Rightarrow)$

    \begin{align*}
        &p_{ij} \stackrel{\text{def}}{=} P(X = x_i, Y = y_j) =
            \lim_{h \to 0} P(x_i - h < X \leq x_i, y_j - h < Y \leq y_j) = \\
        &= \lim_{h \to 0} (F_X(x_i) F_Y(y_j) - F_X(x_i - h) F_Y(y_j) -
            F_X(x_i) F_Y(y_j - h) - F_X(x_i - h) F_Y(y_j - h)) = \\
        &\stackrel{\text{neodv.}}{=} \lim_{h \to 0} (F_X(x_i) - F_X(x_i - h)) (F_Y(y_j) - F_Y(y_j - h)) = \\
        &= \lim_{h \to 0} P(x_i - h < X \leq x_i) \cdot P(y_j - h < Y \leq y_j) = \\
        &= \lim_{h \to 0} P(x_i - h < X \leq x_i) \cdot \lim_{h \to 0} P(y_j - h < Y \leq y_j) = \\
        &= P(X = x_i) \cdot P(Y = y_j) = p_i \cdot q_j
    \end{align*}

    $(\Leftarrow)$

    \begin{align*}
        &F_{(X,Y)}(x,y) = P(X \leq x, Y \leq y) = P(\cup_{i: x_i \leq x} \cup_{j: y_j \leq y} (X = x_i, Y = y_j)) = \\
        &\stackrel{\text{disjunktni}}{=} \sum_{i: x_i \leq x} \sum_{j: y_j \leq y} P(X = x_i, Y = y_j) = \\
        &\stackrel{\text{predpostavka}}{=} \sum_{i: x_i \leq x} \sum_{j: y_j \leq y} p_i q_j = \\
        &= (\sum_{i: x_i \leq x} p_i) (\sum_{j: y_j \leq y} q_j) = \\
        &= P(X \leq x_i) \cdot P(Y \leq y_j) = F_X(x) \cdot F_Y(y)
    \end{align*}
\end{proof}

Torej sta $X$ in $Y$ neodvisni slu"cajni spremenljivki
%tabelca

\begin{claim}
    Naj bo $(X,Y)$ zvezno porazdeljen slu"cajni vektore z gostoto $p(x,y)$. Potem sta $X$ in $Y$ neodvisni
    slu"cajni spremenljivki $\iff$ $p_{(X,Y)}(x,y) = p_X(x) \cdot p_Y(y)$ za (skoraj) vse $x,y \in \R$
\end{claim}

\begin{proof}
    (ideja): $X$ in $Y$ sta neodvisni, "ce $F_{(X,Y)}(x,y) = F_X(x) \cdot F_Y(y) \forall x, y \in \R$. "Ce parcialno
    odvajamo po $x$ in po $y$, dobimo $p_{(X,Y)}(x,y) = p_X(x) \cdot p_Y(y)$. Obratno dobimo z integriranjem po
    $x$ in po $y$
\end{proof}

\begin{ex}
    $(X,Y) \sim N(\mu_x, \mu_y, \sigma_x, \sigma_y, \rho)$. Tedaj je

    \begin{align*}
        &X \sim N(\mu_x, \sigma), Y \sim N(\mu_y, \sigma_y) \\
        &X \text{ in } Y \text{ sta neodvisni } \iff \rho = 0 \\
        &p_{(X,Y)}(x,y) = \frac{1}{2\pi \sigma_x \sigma_y} e^{-\frac{1}{2} ((\frac{x-\mu_x}{\sigma_x})^2 +
            (\frac{y-\mu_y}{\sigma_y})^2)} = \\
        &= \frac{1}{\sqrt{2\pi} \sigma_x} e^{-\frac{1}{2} (\frac{x-\mu_x}{\sigma_x})^2} +
            \frac{1}{\sqrt{2\pi} \sigma_y} e^{-\frac{1}{2} (\frac{y-\mu_y}{\sigma_y})^2} = p_X(x) \cdot p_Y(y)
    \end{align*}

    \begin{align*}
        &N(0,0,1,1,\rho): x^2 - 2\rho x y + y^2 = c \text{ - ovojnica} \\
        &\quad \rho = 0: x^2 + y^2 = c \text{ - kro"znica}
    \end{align*}
\end{ex}

\begin{claim}
    Naj bo $(X,Y)$ zvezno porazdeljen slu"cajni vektor. Potem sta $X$ in $Y$ neodvisni $\iff p_{(X,Y)}(x,y) =
    f(x) \cdot g(y)$ za neki integrabilni funkciji $f$ in $g$
\end{claim}

\begin{proof}
    ($\Rightarrow$) jasno na zadnji trditvi \\
    ($\Leftarrow$)

    \begin{align*}
        &p_X(x) = \int_{-\infty}^{\infty} p_{(X,Y)}(x,y) dy \stackrel{\text{predpostavka}}{=}
            f(x) \int_{-\infty}^{\infty} g(y) dy \text{ in } \\
        &p_Y(y) = \int_{-\infty}^{\infty} p_{(X,Y)}(x,y) dx \stackrel{\text{predpostavka}}{=}
            g(y) \int_{-\infty}^{\infty} f(x) dx \\
    \end{align*}

    Ker je $\iint\limits_{\R^2} p_{(X,Y)}(x,y) dx dy = 1$, je

    \begin{equation*}
        \int_{-\infty}^{\infty} f(x) dx \cdot \int_{-\infty}^{\infty} g(y) dy = 1 \text{ predpostavka}
    \end{equation*}

    Zato je $p_X(x) \cdot p_Y(y) = f(x) \cdot g(y) = p_{(X,Y)}(x,y)$, kar pomeni neodvisnost po prej"snji trditvi
\end{proof}

\begin{theorem}
    Slu"cajni spremenljivki $X$ in $Y$ sta neodvisni $\iff$ za vsaki Borelovi mno"zici $A, B \subseteq \R$ velja

    \begin{equation*}
        P(X \in A, Y \in B) = P(X \in A) \cdot P(Y \in B)
    \end{equation*}

    t.j. dogodka $(X \in A)$ in $(Y \in B)$ sta neodvisna \\
    (Borelova $\sigma$-algebra: najmanj"sa $\sigma$-algebra z odprtimi mno"zicami)
\end{theorem}

\begin{proof}
    ($\Leftarrow$)

    \begin{align*}
        &A = (-\infty, x], B = (-\infty, y] \\
        &P(X \leq x, Y \leq y) = P(X \in (-\infty, x], Y \in (-\infty, y]) = \\
        &= P(x \in (-\infty, x]) \cdot P(Y \in (-\infty, y]) = P(X \leq x) P(Y \leq y) \\
        &\implies F_{(X,Y)}(x,y) = F_X(x) \cdot F_Y(y)
    \end{align*}

    ($\Rightarrow$) izpustimo
\end{proof}

\subsection{Funkcije slu"cajnih spremenljivk in slu"cajnih vektorjev}

Naj bo $X: \Omega \to \R$ slu"cajna spremenljivka in $f: \R \to \R$ zvezna. Potem je $Y := f \circ X: \omega \to \R$
tudi slu"cajna spremenljivka.



% 10. predavanje: 9.12.

$f \circ X = f(X)$ \\
saj je mno"zica

\begin{align*}
    &(Y \leq y) \equiv \{\omega \in \Omega: f(X(\omega)) \leq y\} = \\
    &= \{\omega \in \Omega: f(X(\omega)) \in (-\infty, y]\} = \\
    &= \{\omega \in \Omega: X(\omega) \in f^{-1}((-\infty, y])\} = \\
    &= \{X \in f^{-1}((-\infty, y])\} 
\end{align*}

dogodek, ker je $f^{-1}((-\infty, y])$ zaprta mno"zica, torej Borelova. \\
$y$ je funkcija slu"cajne spremenljivke X. \\
Naj bo $f$ strogo nara"s"cajo"ca funkcija z zalogo vrednosti $(a,b)$, kjer je $-\infty \leq a < b \leq \infty$ \\
Vzemimo $y \in (a,b)$. Potem je

\begin{align*}
    &F_Y(y) \stackrel{\text{def}}{=} P(Y \leq y) = P(f \circ X \leq y) = \\
    &\text{f nara"s"cajo"ca} \to \text{obrnljiva} \\
    &= P(X \leq f^{-1}(y)) = F_X(f^{-1}(y))
\end{align*}

kjer je $f^{-1}: (a,b) \to \R$ inverzna funkcija k funkciji $f$ \\
"ce je $y \geq b$ je $F_Y(y) = 1$ \\
"ce je $y \leq a$ je $F_Y(y) = 0$ \\
"Ce je "se $f$ zvezno odvedljiva in $X$ zvezno porazdeljena slu"cajna spremenljivka, potem je $y$ tudi zvezno
porazdeljena z gostoto $\Phi$

\begin{equation*}
    \Phi_Y(y) = F_Y^{'}(y) = F_X^{'}(f^{-1}(y)) \cdot (f^{-1}(y))^{'}
\end{equation*}

za $y \in (a,b)$, "ce je $y \notin (a,b)$, je $p_Y(y) = 0$ \\
Podobno ravnamo v primeru, ko je f strogo padajo"ca ($(a,b)$ zaloga vrednosti)

\begin{align*}
    &F_Y(y) = P(Y \leq y) = P(f \circ X \leq y) = P(X \geq f^{-1}(y)) = \\
    &= 1 - P(X \leq f^{-1}(y)) = 1 - F_X(f^{-1}(y)-)
\end{align*}

\begin{ex}
    $X \sim N(0,1)$, $f(x) = kx + n, k \neq 0, n \in \R$ \\
    Vzemimo, da je $k > 0$. Definiramo $Y = f(X)$. Tedaj je

    \begin{equation*}
        p_Y(y) = p_X(\frac{y - n}{k}) \cdot \frac{1}{k}
    \end{equation*}

    po formuli (prej).
    \begin{align*}
        &y = kx + n \implies x = \frac{y-n}{k} = f^{-1}(y)
    \end{align*}

    To je enako

    \begin{align*}
        p_Y(y) = \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2} (\frac{y-n}{k})^2} \frac{1}{k}
    \end{align*}

    torej je $Y \sim N(n, k)$ \\
    "Ce je $k < 0$, potem je $p_Y(y) = p_X(\frac{y-n}{k}) \cdot \frac{1}{-k}$, torej za poljuben
    $k \in \R \textbackslash \{0\}$ je $Y \sim N(n, |k|)$
\end{ex}

\begin{ex}
    $X \sim N(0,1), f(x) = x^2$. Tedaj ima $Y = f(X) = X^2$ porazdelitveno funkcijo
    
    \begin{equation*}
        F_Y(y) = P(Y \leq y) = P(X^2 \leq y) = 0
    \end{equation*}

    za $y \leq 0$ in

    \begin{align*}
        &F_Y(y) = P(|X| \leq \sqrt{y}) = P(-\sqrt{y} \leq X \leq \sqrt{y}) = F_X(\sqrt{y}) - F_X(-\sqrt{y}) = \\
        &= \frac{1}{\sqrt{2\pi} \int_{-\sqrt{y}}^{\sqrt{y}}} e^{-\frac{x^2}{2}} dx
            \stackrel{e^{-\frac{x^2}{2}}\text{ soda}}{under}\text{=} \frac{2}{\sqrt{2\pi}}
            \int_{0}^{\sqrt{y}} e^{-\frac{x^2}{2}} dx
    \end{align*}

    za $y \geq 0$ \\
    Gostota za $Y$ pa je
    
    \begin{align*}
        &p_Y(y) = F_Y^{'}(y) = \frac{2}{\sqrt{2\pi}} \cdot e^{-\frac{1}{2}(\sqrt{y})^2} \cdot \frac{1}{2\sqrt{y}} = \\
        &= \frac{1}{\sqrt{2\pi}} y^{-\frac{1}{2}} e^{-\frac{y}{2}}
    \end{align*}

    kar je $\chi^2(1)$, saj je

    \begin{equation*}
        \chi^2(n): p_X(x) = \frac{1}{2^{\frac{n}{2}} \gamma(\frac{n}{2})} x^{\frac{n}{2} - 1} e^{-\frac{x}{2}}
    \end{equation*}

    za $x > 0$, sicer $p_X(x) = 0$
\end{ex}

\begin{claim}
    "Ce sta $X$ in $Y$ neodvisni slu"cajni spremenljivki, $f$ in $g: \R \to \R$ zvezni funkciji, potem sta tudi
    $U = f(X)$ in $V = g(Y)$ neodvisni slu"cajni spremenljivki
\end{claim}

\begin{proof}
    \begin{align*}
        &F_{(U,V)}(u,v) = P(f(x) \leq u, g(y) \leq v) = P(X \in f^{-1}((-\infty, u]), Y \in g^{-1}((-\infty, v])) =\\
        &f^{-1}((-\infty, u]) \text{ in } g^{-1}((-\infty, v]) \text{ zaprti } \implies \text{ Borelovi} \\
        &\stackrel{\text{Borelov izrek}}{=} P(X \in f^{-1}((-\infty, u])) \cdot P(Y \in g^{-1}((-\infty, v])) = \\
        &= P(f(X) \leq u) \cdot P(g(Y) \leq v) = F_U(u) \cdot F_V(v) \; \forall u, v \in \R
    \end{align*}
\end{proof}

\begin{theorem}
    Naj bo $X = (X_1 \cdots X_n): \Omega \to \R^n$ slu"cajni vektor in $f = (f_1 \cdots f_m): \R^n \to \R^m$ zvezna
    preslikava. Potem je $Y = f \circ X \equiv f(X): \Omega \to \R^m$ tudi slu"cajni vektor (brez dokaza).
\end{theorem}

$Y$ je funkcija slu"cajnega vektorja $X$ in ima $m$ komponent $Y = (Y_1 \cdots Y_m)$. \\
Porazdelitvena funkcija za $Y_j, (j = 1, 2 \cdots m)$ je

\begin{equation*}
    F_{Y_j}(y) = P(f_i(x_1 \cdots x_n) \leq y) = P((x_1 \cdots x_n) \in f_j^{-1}((-\infty, y])) \text{ mno"zica v} \R^n
\end{equation*}

"Ce je $X = (X_1 \cdots X_n)$ zvezno porazdeljena, je torej

\begin{equation*}
    F_{Y_j}(y) = \int \underbrace{\cdots}_{f^{-1}((-\infty, y])} \int p_X(x_1 \cdots x_n) dx_1 \cdots dx_n
\end{equation*}

\begin{ex}
    $n = 2, m = 1$, $(x,y): \Omega \to \R^2$ zvezno porazdeljen

    \begin{align*}
        &F_Z(z) = P(Z \leq z) = P(f(x,y) \leq z) = P((X,Y) \in f^{-1}((-\infty, z])) = \\
        &= \iint\limits_{x + y \leq z} p_{(X,Y)}(x,y) dx dy = \int_{-\infty}^{\infty} dx
            \int_{-\infty}^{z-x} p_{(X,Y)}(x,y) dy
    \end{align*}

    od tod sledi, da je gostota slu"cajne spremenljivke $Z$

    \begin{equation*}
        p_Z(z) = F_Z^{'}(z) = \int_{-\infty}^{\infty} p_{(X,Y)}(x,z-x) dx
    \end{equation*}

    "Ce sta "se $X$ in $Y$ neodvisni slu"cajni spremenljivki, potem je

    \begin{equation*}
        p_Z(z) = \int_{-\infty}^{\infty} p_X(x) \cdot p_Y(z-x) dx \text{ - konvolucija funkcij } p_X \text{ in } p_Y
    \end{equation*}
\end{ex}

Vzemimo posebni primer $X \sim \chi^2(m)$, $Y \sim \chi^2(n)$, torej

\begin{equation*}
    p_X(x) = \frac{1}{2^{\frac{m}{2}} \Gamma(\frac{m}{2})} x^{\frac{m}{2} - 1} e^{-\frac{x}{2}} \text{ za } x > 0
    \text{ in 0 sicer}
\end{equation*}

za $p_Y(y)$ podobno. \\
Po zadnji formuli je $p_Z(z) = \int_{-\infty}^{\infty} p_X(x) \cdot p_Y(z-x) dx = 0$ za $z \leq 0$, \\
sicer je za $z > 0$

\begin{align*}
    &p_Z(z) = \frac{1}{2^{\frac{m}{2}} \Gamma(\frac{m}{2}) 2^{\frac{m}{2}} \Gamma(\frac{n}{2})} e^{-\frac{z}{2}}
        \int_{0}^{z} x^{\frac{m}{2} - 1} (z-x)^{\frac{m}{2} - 1 + \frac{n}{2} - 1 + 1} e^{-\frac{x}{2}}
        e^{-\frac{z-x}{2}} dx = \\
    &= \frac{1}{2^{\frac{m+n}{2}} \Gamma(\frac{m}{2}) \Gamma(\frac{n}{2})} e^{-\frac{z}{2}}
        \int_0^z x^{\frac{m}{2} - 1} (z-x)^{\frac{n}{2} - 1} dx =
\end{align*}

\begin{align*}
    &B(p,q) = \int_0^1 t^{p-1} (1-t)^{q-1} dt \\
    &x = tz \; dx = z dt
\end{align*}

\begin{align*}
    &= \frac{1}{2^{\frac{m+n}{2}} \Gamma(\frac{m}{2}) \Gamma(\frac{n}{2})} e^{-\frac{z}{2}}
        z^{\frac{m}{2} - 1 + \frac{n}{2} - 1 + 1} \int_0^1 t^{\frac{m}{2} - 1} (1-t)^{\frac{n}{2} - 1} dt =
\end{align*}

\begin{align*}
    &B(p,q) = \frac{\Gamma(p) \Gamma(q)}{\Gamma(p+q)} \\
    &\to B(\frac{m}{2}, \frac{n}{2}) = \frac{\Gamma(\frac{m}{2}) \Gamma(\frac{n}{2})}{\Gamma(\frac{m+n}{2})}
\end{align*}

\begin{align*}
    &= \frac{1}{2^{\frac{m+n}{2}} \Gamma(\frac{m+n}{2})} e^{-\frac{z}{2}} z^{\frac{m+n}{2} - 1}
\end{align*}

Torej $X + Y \sim \chi^n(m+n)$ \\
Dokazali smo

\begin{claim}
    Naj bosta neodvisni slu"cajni spremenljivki $X \sim \chi^2(m), Y \sim \chi^2(n)$z. Potem je
    $X + Y \sim \chi^2(m+n)$
\end{claim}



% 11. predavanje: 16.12.

\begin{conseq}
    "Ce so $X_1, X_2 \cdots X_n$ neodvisne slu"cajne spremenljivke, porazdeljene $N(0,1)$, potem je
    $Y := X_1^2 + \cdots + X_n^2$ porazdeljena po $\chi^2(n)$
\end{conseq}

\begin{proof}
    Vemo, da je $X_i^2 \sim \chi^2(1)$ in da so $X_1^2 \cdots X_n^2$ neodvisne spremenljivke. Potem je po trditvi
    + indukciji $Y \sim \chi^2(1 + \cdots + 1) = \chi^2(n)$
\end{proof}

Oglejmo si transformacijo $f: \R^2 \to \R, (x,y) \to (u,v)$, ki preslika zvezno porazdeljen slu"cajni vektor $(x,y)$ v
zvezno porazdeljen slu"cajni vektor $(u,v)$, torej $U = u(x,y), V = v(x,y)$ \\
Ozna"cimo "se $A_{u,v} = (-\infty,u] \times (-\infty,v]$ \\
Potem je

\begin{equation*}
    F_{(U,V)}(u,v) = \underset{A_{u,v}}{\iint} p_{(U,V)}(s,t) ds dt
\end{equation*}\label{eqn:Auv}

Pot drugi strani pa je

\begin{equation*}
    F_{(U,V)}(u,v) = P((U,V) \in A_{u,v}) = P((X,Y) \in f^{-1}(A_{u,v})) = \underset{f^{-1}(A_{u,v})}{\iint}
    p_{(X,Y)}(x,y) dx dy
\end{equation*}

Privzemimo "se, da je f zvezno odvedljiva bijekcija. Potem je $f: \R^2 \to \R^2, (u,v) \to (x,y)$ tudi zvezno
odvedljiva. Z zamenjavo spremenljivk $x = X(u,v), y = Y(u,v)$ v zadnjem intergalu dobimo

\begin{equation*}
    F_{(U,V)}(u,v) = \underset{A_{u,v}}{\iint} p_{(X,Y)}(x(s,t),y(s,t)) \cdot |J(s,t)| dx ds
\end{equation*}

kjer je

\begin{equation*}
    J(u,v) = \begin{bmatrix}
        \frac{\partial x}{\partial u} & \frac{\partial x}{\partial v} \\
        \frac{\partial y}{\partial u} & \frac{\partial y}{\partial v}
    \end{bmatrix}(u,v)
\end{equation*}

Jacobijeva determinanta.

Zaradi \ref{eqn:Auv} imamo torej $p_{(U,V)}(u,v) = p_{(X,Y)}(x(u,v), y(u,v)) |J(u,v)|$

Oglejmo si poseben primer

\begin{ex}
    $U = X, V = v(x,y)$ oz $X = U, Y = y(u,v)$ \\
    Tedaj je $p_{(U,V)}(u,v) = p_{(X,Y)}(u, y(u,v)) |\frac{\partial y}{\partial v}(u,v)|$ \\
    Gostota spremenljivke $V$ je $\int_{-\infty}^{\infty}  p_{(X,Y)}(u, y(u,v)) |\frac{\partial y}{\partial v}(u,v)| dv =
    p_V(v)$ \\
    Pi"simo $Z = V$, torej je $Y = y(x,z)$, saj je $U=X$ \\
    Potem prepi"semo $p_Z(z) = \int_{-\infty}^{\infty}  p_{(X,Y)}(u, y(x,z)) |\frac{\partial y}{\partial z}(x,z)| dx$
\end{ex}

\begin{ex} \text{} \\
    \begin{enumerate}
        \item $Z = X + Y \implies Y = Z - X$, torej je $y(x,z) = z - x, \frac{\partial y}{\partial z}(x,z) = 1$
            \begin{equation*}
                p_{X+Y}(z) = \int_{-\infty}^{\infty} p_{(X,Y)}(x,z-x) \cdot 1 dx
            \end{equation*}
        \item $Z = X \cdot Y \implies Y = \frac{Z}{X}$, torej je $y(x,z) = \frac{z}{x},
            \frac{\partial y}{\partial z}(x,z) = \frac{1}{x}$
            \begin{equation*}
                p_{X \cdot Y}(z) = \int_{-\infty}^{\infty} p_{(X,Y)}(x,\frac{z}{x}) \frac{1}{|x|} dx
            \end{equation*}
            "Ce sta "se $X$ in $Y$ neodvisni slu"cajni spremenljivki, potem je
            \begin{equation*}
                p_{X \cdot Y}(z) = \int_{-\infty}^{\infty} p_X(x) \cdot p_Y(\frac{z}{x}) \cdot \frac{1}{|x|} dx
            \end{equation*}
    \end{enumerate}
\end{ex}

\subsection{Matemati"cno upanje oz. pri"cakovana vrednost}

V primeru $X : \begin{pmatrix}x_1 & \cdots & x_n \\ p_1 & \cdots & p_n
\end{pmatrix}$ je matemati"cno upanje oz. pricakovana vrednost vsota $E(X) := \sum_{k=1}^{n} x_k \cdot p_k$ \\

V posebnem primeru $p_1 = \cdots = p_n = \frac{1}{n}$ je $E(X) = \frac{1}{n} \sum_{k=1}^{n} x_k =
\frac{x_1 + \cdots + x_n}{n}$ - povpre"cje "stevil $x_1 \cdots x_n$ \\

expected value, expectation, mean value \\

Naj bo $X$ diskretno porazdeljena slu"cajna spremenljivka z neskon"cno zalogo vrednosti:

\begin{equation*}
    X: \begin{pmatrix}
        x_1 & x_2 & x_3 & \cdots \\
        p_1 & p_2 & p_3 & \cdots \\
    \end{pmatrix}
\end{equation*}

$X$ ima matemati"cno upanje oz. pri"cakovano vrednost, "ce je $\sum_{k=1}^{\infty} |x_k| p_k < \infty$ \\
Tedaj je matemati"cno upanje definirano kot $E(X) = \sum_{k=1}^{\infty} x_k \cdot p_k$ \\
Naj bo sedaj $X$ zvezno porazdeljena slu"cajna spremenljivka z gostoto $p_X$. Potem ima $X$ matemati"cno
upanje, "ce je $\int_{-\infty}^{\infty} |x| \cdot p_X(x) dx < \infty$. Tedaje je matemati"cno upanje
slu"cajne spremenljivke $X$ enako $E(X) = \int_{-\infty}^{\infty} x \cdot p_X(x) dx$

\begin{ex} \text{} \\
    \begin{enumerate}
        \item $X \sim Ber(p)$ oz. $X: \begin{pmatrix}0 & 1 \\ q & p
            \end{pmatrix} q = 1-p, E(X) = 0 \cdot q + 1 \cdot p = p$
        \item $X \sim Poi(\lambda)$, torej $p_k = P(X=k) = \frac{\lambda^k}{k!} e^{-\lambda} k = 0, 1 \cdots$
            \begin{equation*}
                E(X) = \sum_{k=0}^{\infty} k \cdot p_k = \sum_{k=0}^{\infty} k \cdot \frac{\lambda^k}{k!} e^{-\lambda} =
                e^{-\lambda} \cdot \lambda \sum_{k=1}^{\infty} \frac{\lambda^{k-1}}{(k-1)!} = \lambda
            \end{equation*}
        \item Enakomerna porazdelitev na $[a,b]$
            \begin{align*}
                &p(X) = \begin{cases}
                    \frac{1}{b-a} \; \text{ "ce } a \leq x \leq b \\
                    0 \quad \text{sicer}
                \end{cases} \\
                &E(X) = \int_{a}^{b} x \cdot \frac{1}{b-a} dx = \frac{1}{b-a} \cdot \frac{x^2}{2} \vert_{a}^b =
                    \frac{b^2 - a^2}{2(b-a)} = \frac{b+a}{2}
            \end{align*}
        \item $X \sim N(\mu, \sigma)$ %skica
            \begin{align*}
                &E(X) = \frac{1}{\sigma \sqrt{2\pi}} \cdot \frac{-\infty}{\infty} x \cdot
                e^{-\frac{1}{2} (\frac{x-\mu}{\sigma})^2} dx =
            \end{align*}
            \begin{equation*}
                U = \frac{x - \mu}{\sigma} \implies du = \frac{dx}{\sigma}
            \end{equation*}
            \begin{align}
                &= \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} (\sigma u + \mu) e^{-\frac{1}{2} u^2} du =
                &= \frac{1}{\sqrt{2\pi}} \sigma \int_{-\infty}^{\infty} u \cdot e^{-\frac{1}{2} u^2} du +
                    \mu \cdot \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} u \cdot e^{-\frac{1}{2} u^2} du =
                &= \mu
            \end{align}
            Ker je v predzadnjem koraku 1. funkcija (v integralu) liha, 2. pa je gostota porazdelitve $N(0,1)$
        \item Cauchyjeva porazdelitev $p(x) = \frac{1}{\pi (1+x^2)}$ \\ % skica
            Nima matemati"cnega upanja, saj je $\int_{-\infty}^{\infty} |x| \cdot \frac{1}{\pi (1+x^2)} dx =
            \frac{2}{\pi} \int_{0}^{\infty} \frac{x}{1+x^2} dx = \frac{1}{\pi} ln(1+x^2) \vert_{0}^{\infty} = \infty$
        \item $1 - \frac{1}{2} + \frac{1}{3} - \cdots$ je pogojno konvergentna vrsta, t.j. konvergira, a ne absolutno
            \begin{align*}
                &X: \begin{pmatrix}
                    x_1 & x_2 & \cdots \\
                    p_1 & p_2 & \cdots \\
                \end{pmatrix}, \sum_{k=1}^{\infty} x_k \cdot p_k = \sum_{k=1}^{\infty} \frac{(-1)^{k-1}}{k} \\
                &x_k \cdot p_k = \frac{(-1)^{k-1}}{k} \\
                &\sum_{k=1}^{\infty} p_k = 1
                &p_k := \frac{1}{2^k} \text{ npr. ker je vsota 1} \\
                &x_k := \frac{(-1)^{k-1}}{k} = 2^k
            \end{align*}
            Ta porazdelitev nima matemati"cnega upanja, ker vrsta ne konvergira absolutno
    \end{enumerate}
\end{ex}

\begin{claim}
    Naj bo $f: \R \to \R$ zvezna funkcija
    \begin{enumerate}[label=(\alph*)]
        \item "Ce je $X: \begin{pmatrix} x_1 & x_2 & \cdots \\ p_1 & p_2 & \cdots \end{pmatrix}$ \\
            potem je $E(f \circ X) \equiv E(f(X)) = \sum_{k=1}^{\infty} f(x_k) \cdot p_k$
            ("ce le to matema"ticno upanje obstaja)
        \item "Ce je $X$ zvezno porazdeljena z gostoto $p_X$, potem je $E(f \circ X) =
            \int_{-\infty}^{\infty} f(x) \cdot p_X(x) dx$
    \end{enumerate}
\end{claim}

\begin{proof}
    (samo (a)): \\
    \begin{equation*}
        f \circ X: \begin{pmatrix}
            f(x_1) & f(x_2) & \cdots \\
            p_1 & p_2 & \cdots \\
        \end{pmatrix}
    \end{equation*}
    npr "ce $f(x_1) = f(x_3) \implies \begin{pmatrix}
        f(x_1) & f(x_2) & \cdots \\
        p_1 + p_3 & p_2 & \cdots \\
    \end{pmatrix}$ \\
    ($E(f \circ X) = \int_{-\infty}^{\infty} x \cdot p_{f(x)}(x) dx = \cdots =
    \int_{-\infty}^{\infty} f(x) \cdot p_X(x) dx$ - substitucija $y=f(x)$ v integralu)
\end{proof}



% 12. predavanje: 23.12.

\begin{conseq}
    Slu"cajna spremenljivka $X$ ima matemati"cno upanje $\iff$ $X$ ima matemati"cno upanje. Tedaj velja
    $|E(X)| = E(|X|)$
\end{conseq}

\begin{proof}
    (samo diskreten primer): \\
    \begin{equation*}
        E(|X|) \stackrel{\text{trd.} f(x)=|x|}{=} \sum_i |x_i| \cdot p_i \geq |\sum_i x_i \cdot p_i| = |E(X)|
    \end{equation*}
\end{proof}

\begin{conseq}
    Za $\forall a \in \R$ in vsako slu"cjano spremenljivko $X$ z matemati"cnim upanjem velja $E(a \cdot X) = a \cdot E(X)$
    (homogenost)
\end{conseq}

\begin{proof}
    $f(x) = a \cdot x$, trditev (od prej)
\end{proof}

Podobno kot zadnjo trditev se doka"ze

\begin{claim}
    Naj bo $f: \R^2 \to \R$ zvezna funkcija in $(X,Y)$ slu"cajni vektor
    \begin{enumerate}[label=(\alph*)]
        \item Naj bo $(X,Y)$ diskretno porazdeljen $p_{ij} := P(X=x_i, Y=y_j)$. Potem je $E(f(X,Y)) = \sum_i \sum_i
            f(x_i,y_j) \cdot p_{ij}$ ("ce le vrsta (oz. kon"cna vsota) absolutno konvergira)
        \item Naj bo $(X,Y)$ zvezno porazdeljen z gostoto $p(X,Y)$. Potem je $E(f(X,Y)) = \int_{-\infty}^{\infty} dx
            \int_{-\infty}^{\infty} f(x,y) p_{(X,Y)}(x,y) dy$ ("ce le integral absolutno konvergira)
    \end{enumerate}
\end{claim}

\begin{conseq}
    "Ce slu"cajni spremenljivki $X$ in $Y$ imata matamati"cno upanje, potem ga ima tudi $X+Y$ in velja $E(X+Y) = E(X) + E(Y)$
    (aditivnost)
\end{conseq}

\begin{proof}
    (samo zvezen primer): \\
    \begin{align*}
        &E(X,Y) \stackrel{f(x,y)=x+y}{=} \int_{-\infty}^{\infty} dx \int_{-\infty}^{\infty} (x+y) p_{(X,Y)}(x,y) dy =\\
        &= \int_{-\infty}^{\infty} x dx \int_{-\infty}^{\infty}p_{(X,Y)}(x,y) dy +
        \int_{-\infty}^{\infty} y dy \int_{-\infty}^{\infty}p_{(X,Y)}(x,y) dx =\\
        &= \int_{-\infty}^{\infty} x p_X(x) dx + \int_{-\infty}^{\infty} y p_{Y}(y) dy = E(X) + E(Y)
    \end{align*}
\end{proof}

\begin{conseq}
    Za slu"cajne spremenljivke $X_1 \cdots X_n$, ki imajo matemati"cno upanje, velja $E(a_1 X_1 + \cdots + a_n X_n) =
    a_1 E(X_1) + \cdots + a_n E(X_n)$ z $\forall a_1 \cdots a_n \in \R$
\end{conseq}

\begin{equation*}
    E(X+Y) = \int_{-\infty}^{\infty} x \cdot p_{X+Y}(x) dx \stackrel{\text{?}}{=} E(X) + E(Y) \text{ ni o"citno iz tega}
\end{equation*}

\begin{ex}
    \begin{enumerate}
        \item "Ce ima $X$ matemati"cno upanje, potem $E(X-E(X)) = E(X) - E(E(X)) = E(X) - E(X) = 0$
        \item $X_k \sim Ber(p)$, t.j. $X_k \sim \begin{pmatrix}0 & 1 \\ q & p\end{pmatrix}, q = 1 - p$
            \begin{equation*}
                X = X_1 + \cdots + X_n \implies E(X) = E(X_1) + \cdots + E(X_n) = n \cdot p
            \end{equation*}
    \end{enumerate}
\end{ex}

Posebej to (v 2. zgledu) velja v primeru, ko so $\{X_k\}_{i=1}^n$ neodvisne. To velja tudi za Bernoullijevo zaporedje
ponovitev poskusa: opazujemo dogodek A s $P(A) = p$. $X$ je frekvenca dogodka A v n ponovitvah poskusa. Potem je
$X \sim Bin(n,p)$ in $X = X_1 + \cdots + X_n$, kjer je $(X_k=1)$ dogodek, da se A zgodi v k-ti ponovitvi poskusa,
sicer je $(X_k=0)$. Po zgornjem je $E(X) = n \cdot p$. Do tega lahko pridemo tudi direktno:
\begin{align*}
    &E(X) = \sum_{k=0}^{n} k \cdot p_k = \sum_{k=0}^{n} k \cdot \binom{n}{k} p^k q^{n-k} = \\
    &= \sum_{k=1}^{n} k \cdot \frac{n}{k} \binom{n-1}{k-1} p^k q^{n-k} =
        np \sum_{k=1}^{n} \binom{n-1}{k-1} p^{k-1} q^{n-k} \stackrel{j=k-1}{=} \\
    &= np (\sum_{j=0}^{n-1} \binom{n-1}{j} p^j q^{n-1-j}) = np (p+q)^{n-1} = np
\end{align*}

\begin{claim}[Cauchy-Schwartzova neenakost]
    "Ce obstajata $E(X^2)$ in $E(Y^2)$, potem obstaja tudi $E(X,Y)$ in velja $E(|X \cdot Y|) \leq \sqrt{E(X^2) \cdot E(Y^2)}$.
    Ena"caj velja samo v primeru $|Y| = \sqrt{\frac{E(Y^2)}{E(X^2)}}|X|$ z verjetnostjo 1
\end{claim}

\begin{proof}
    Ker za nenegativa realna "stevila velja neenakost
    \begin{equation*}
        u \cdot v \leq \frac{1}{2}(u^2 + v^2) \; \iff \; (u-v)^2 \geq 0
    \end{equation*}
    za nenegativni slu"cajni spremenljivki $U$ in $V$ velja neenakost
    \begin{equation*}
        U \cdot V \leq \frac{1}{2}(U^2 + V^2)
    \end{equation*}
    Enakost velja samo v to"ckah $\omega \in \Omega$, za katere je $U(\omega) = V(\omega)$ \\
    "Ce vstavimo $U = a \cdot |X|$ in $V = \frac{1}{a}|Y|$ za $a > 0$, dobimo
    $|X \cdot Y| \leq \frac{1}{2} (a^2 Y^2 + \frac{1}{a^2}Y^2)$ in zato je
    \begin{equation}
        E(|X \cdot Y|) \leq \frac{1}{2} (a^2 E(X^2) + \frac{1}{a^2} E(Y^2)) \text{ za } \forall a > 0
    \end{equation}
    "Ce vstavimo $a^2 = \sqrt{\frac{E(Y^2)}{E(X^2)}}$ na desni strani dobimo
    \begin{equation*}
        \frac{1}{2} (\sqrt{E(Y^2) + E(X^2)} + \sqrt{E(X^2 + E(Y^2))}) = \sqrt{E(X^2) + E(Y^2)}
    \end{equation*}
    Torej je
    \begin{equation*}
        E(|X \cdot Y|) \leq \sqrt{E(X^2) \cdot E(Y^2)}
    \end{equation*}
    Enakost v neenakosti velja $\iff a |X| = \frac{1}{a} |Y|$, torej $|Y| = a^2 |X| = \frac{E(Y^2)}{E(X^2)} |X|$
    z verjetnostjo 1
\end{proof}

\begin{conseq}
    "Ce obstaja $E(X^2)$, potem obstaja $E(X)$ in velja $(E(X))^2 \leq E(X^2)$
\end{conseq}

\begin{proof}
    $Y=1$, t.j. $Y: \begin{pmatrix}1 \\ 1\end{pmatrix} \implies$
    \begin{align*}
        &E(|X \cdot 1|) \leq \sqrt{E(X^2) \cdot 1}  /^2
        &(E(|X|))^2 \leq E(X^2)
    \end{align*}
\end{proof}

\begin{claim}
    Naj bosta $X$ in $Y$ neodvisni slu"cajni spremenljivki, ki imata matemati"cni upanji. Potem ima matemati"cno
    upanje tudi $X \cdot Y$ in velja $E(X \cdot Y) = E(X) \cdot E(Y)$
\end{claim}

\begin{proof}
    (samo zvezem primer): \\
    \begin{align*}
        &E(X \cdot Y) \stackrel{\text{trd}}{=} \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} x \cdot y
            \cdot p_{(X,Y)}(x,y) dx dx \stackrel{\text{neodvisnost}}{=} &
        &=\underset{\R}{\iint} x \cdot y \cdot p_X(x) \cdot p_Y(y) dx dy =
            \int_{-\infty}^{\infty} x p_X(x) dx \cdot \int_{-\infty}^{\infty} x p_Y(y) dy = E(X) \cdot E(Y)
    \end{align*}
\end{proof}

\begin{defn}[Nekoreliranost]
    Slu"cajni spremenljivki $X$ in $Y$ sta nekorelirani, "ce velja $E(X \cdot Y) = E(X) \cdot E(Y)$, sicer sta
    korelirani.
\end{defn}

Po trditvi iz neodvisnosti sledi nekoreliranost. Obratno pa ne velja:

\begin{ex}
    \begin{align*}
        &U = \begin{pmatrix}0 & \frac{\pi}{2} & \pi \\ \frac{1}{3} & \frac{1}{3} & \frac{1}{3}\end{pmatrix} \\
        &X = cos(U): \begin{pmatrix}1 & 0 & -1 \\ \frac{1}{3} & \frac{1}{3} & \frac{1}{3}\end{pmatrix} \\
        &Y = sin(U): \begin{pmatrix}0 & 1 & 0 \\ \frac{1}{3} & \frac{1}{3} & \frac{1}{3}\end{pmatrix} =
            \begin{pmatrix}0 & 1 \\ \frac{2}{3} & \frac{1}{3}\end{pmatrix} \\
        &E(X) = 0, E(Y) = \frac{1}{3} \\
        &X \cdot Y = sin(U) \cdot cos(U) = 0 \implies E(X \cdot Y) = 0 \implies \text{ X in Y sta nekorelirani
            slu"cajni spremenljivki}
    \end{align*}
    \begin{center}
        \begin{tabular}{ c | c c | c}
            X \textbackslash Y & 0 & 1 & $\Sigma$ \\
            \hline
            -1 & $\frac{1}{3}$ & 0 & $\frac{1}{3}$ \\
            0 & 0 & $\frac{1}{3}$ & $\frac{1}{3}$ \\
            -1 & $\frac{1}{3}$ & 0 & $\frac{1}{3}$ \\
            \hline
            $\sum$ & $\frac{2}{3}$ & $\frac{1}{3}$ & 1
        \end{tabular}
        $\implies$ nista neodvisni, npr $\frac{1}{3} = P(X=1,Y=0) \neq P(X=1) \cdot P(Y=0) = \frac{1}{3} \cdot \frac{2}{3}$
    \end{center}
\end{ex}

\begin{claim}
    $X: \begin{pmatrix}x_1 & x_2 \\ p_1 & p_2\end{pmatrix}$,
    $Y: \begin{pmatrix}y_1 & y_2 \\ q_1 & q_2\end{pmatrix}$ \\
    Potem sta $X$ in $Y$ neodvisni $\iff$ nekorelirani \\
    $\iff E(X \cdot Y) = E(X) \cdot E(Y)$   % D.N?
\end{claim}

\subsection{Disperzija, kovarianco in korelacijski koeficient}

\begin{defn}[Disperzija]
    Naj obstaja $E(X^2)$. Disperzija oz. varianca slu"cajne spremenljivke $X$ je $D(X) \equiv var(X) := E((X-E(X))^2)$
\end{defn}

Disperzija meri razpr"senost slu"cajne spremenljivke $X$ okoli $E(X)$ \\
Ker je $E((X-E(X))^2) = E(X^2 - 2E(X)X + (E(X))^2) = E(X^2) - 2E(X)E(X) + (E(X))^2 = E(X^2) - (E(X))^2$, je
$D(X) = E(X^2) - (E(X))^2$



% 13. predavanje: 6.1.

Lastnosti disperzije:

\begin{itemize}
    \item $D(X) \geq 0$ in $D(X) = 0 \iff P(X=E(X)) = 1$, t.j. X je izrojena slu"cajna spremenljivka
    \item $D(a \cdot X) = a^2 D(X) \; a \in \R$
    \item $\forall a \in \R$ velja: $E((X-a)^2) \geq D(X)$. Enakost velja le v primeru $a = E(X)$
        \begin{proof}
            \begin{align*}
                &E((X-a)^2) = E(X^2 - 2aX + a^2) = E(X^2) - 2E(x)|a| + a^2 =
                &= (a-E(X))^2 + E(X^2) - (E(X))^2 = D(X) + (a-E(X))^2 \geq D(X)
            \end{align*}
            Enakost velja samo za $a=E(X)$
        \end{proof}
\end{itemize}

\begin{defn}[Standardna deviacija]
    Standardna deviacija ali standardni odklon slu"cajne spremenljivke $X$ je $\sigma(X) := \sqrt{D(X)}$
\end{defn}

Zanjo velja $\sigma(aX) = |a| \cdot \sigma(X)$ za $\forall a \in \R$ \\
Primeri nekaterih $E(X)$ in $D(X)$

\begin{enumerate}
    \item enakomerna diskretna porazdelitev: $\begin{pmatrix}x_1 & \cdots & x_n \\ \frac{1}{n} & \cdots & \frac{1}{n}\end{pmatrix}$
        \begin{align*}
            E(X) = \frac{x_1 + \cdots + x_n}{n}, D(X) = E(X^2) - (E(X))^2 = \frac{x_1^2 + \cdots + x_n^2}{2} -
            (\frac{x_1 + \cdots + x_n}{2})^2
        \end{align*}
    \item Binomska porazdelitev $Bin(n,p), n \in \N, p \in (0,1), q = 1-p$
        \begin{align*}
            E(X) = n \cdot p, D(X) = npq, \sigma(X) = \sqrt{npq}
        \end{align*}
    \item Poissonova porazdelitev $Poi(\lambda), \lambda > 0$
        \begin{align*}
            E(X) = \lambda, D(X) = \lambda
        \end{align*}
    \item Geometrijska porazdelitev $geo(p), p \in (0,1), q = 1-p$
        \begin{align*}
            E(X) = \frac{1}{p}, D(X) = \frac{q}{p^2}
        \end{align*}
    \item Pascalova porazdelitev $Pas(m,p), m \in \N, p \in (0,1)$
        \begin{align*}
            E(X) = \frac{m}{p}, D(X) = \frac{mq}{p^2}
        \end{align*}
    \item Enakomerna zvezna porazdelitev $Ed$ na $[a,b]$    % ed oznaka?
        \begin{align*}
            E(X) = \frac{a+b}{2}, D(X) = \frac{(b-a)^2}{12}
        \end{align*}
    \item Normalna porazdelitev $N(\mu, \sigma)$
        \begin{align*}
            E(X) = \mu, D(X) = \sigma^2, \sigma(X) = \sigma
        \end{align*}
    \item Porazdelitev gama $\gamma(b,c)$
        \begin{align*}
            E(X) = \frac{b}{c}, D(X) = \frac{b}{c^2}
        \end{align*}
    \item Porazdelitev $\chi^2(n) = \gamma(\frac{n}{2}, \frac{1}{2})$
        \begin{align*}
            E(X) = n, D(X) = 2n
        \end{align*}
    \item Eksponentna porazdelitev $Exp(\lambda), \lambda > 0$ $= \gamma(1,\lambda)$
        \begin{align*}
            E(X) = \frac{1}{\lambda}, D(X) = \frac{1}{\lambda^2}, \sigma(X) = \frac{1}{\lambda}
        \end{align*}
\end{enumerate}

Preverimo, da je $D(X) = \sigma^2$ za $X \sim N(\mu, \sigma)$

\begin{align*}
    &D(X) = E((X-E(X))^2) = \frac{1}{\sigma\sqrt{2\pi}} \int_{-\infty}^{\infty} (x-\mu)^2 \cdot
        e^{-\frac{1}{2} (\frac{x-\mu}{\sigma})^2} dx
\end{align*}
\begin{equation*}
    t = \frac{x-\mu}{\sigma} \implies x - \mu = \sigma t, dx = \sigma dt
\end{equation*}
\begin{align*}
    &= \frac{\sigma^2}{\sqrt{2\pi}} \int_{-\infty}^{\infty} t^2 e^{-\frac{1}{2}t^2} =
\end{align*}
\begin{align*}
    &u = t, dv = t \cdot e^{-\frac{1}{2}t^2} \\
    &du = dt, v = -e^{-\frac{1}{2}t^2}
\end{align*}   
\begin{align*}
    &\frac{\sigma^2}{\sqrt{2\pi}} (-t e^{-\frac{1}{2}t^2} \vert_{-\infty}^{\infty}) +
        \int_{-\infty}^{\infty} e^{-\frac{1}{2}t^2} dt=
    &= \frac{\sigma^2}{\sqrt{2\pi}} (0 + \sqrt{2\pi}) = \sigma^2
\end{align*}

\begin{defn}[Kovarianca]
    Kovarianca slu"cajnih spremnljivk $K(X,Y) \equiv Cov(X,Y) := E((X-E(X))(Y-E(Y)))$
\end{defn}

% ??
Ker je
\begin{equation*}
    E((X-E(X))(Y-E(Y))) = E(XY - E(Y)X - E(X)Y + E(X)E(Y)) = E(XY) - E(X)E(X) - E(X)E(Y)+ E(X)E(Y)
\end{equation*}
je $cov(X,Y) = E(XY) - E(X)E(Y)$ \\

Lastnosti:

\begin{enumerate}
    \item $K(X,X) = D(X)$
    \item $K(X,Y) = 0 \iff$ $X$ in $Y$ sta neodvisni
    \item $K$ je simetri"cna in bilinearna funkcija:
        \begin{itemize}
            \item $K(X,Y) = K(Y,X)$
            \item $K(aX+bY,Z) = aK(X,Z) + bK(Y,Z) \forall a,b \in \R$
        \end{itemize}
    \item "Ce obstajata $D(X)$ in $D(Y)$, potem obstaja tudi $K(X,Y)$. Tedaj velja $|K(X,Y)| \leq
        \sqrt{D(X) \cdot D(Y)} = \sigma(X) \cdot \sigma(Y)$ \\
        To sledi iz Cauchy-Schwartzove neenakosti ($|E(U \cdot V)| \leq \sqrt{E(U^2) \cdot E(V^2)}$) za
        slu"cajni spremenljivki $X-E(X)$ in $Y-E(Y)$. Ena"caj v neenakosti velja $\iff$ $Y - E(Y) \pm
        \frac{\sigma(Y)}{\sigma(X)} (X - E(X))$ z verjetnostjo 1
    \item "Ce X in Y imata disperziji, potem jo ima tudi $X+Y$ in valja $D(X+Y) = D(X) + D(Y) + 2K(X,Y)$ \\
        "ce sta $X$ in $Y$ nekorelirani (posebej neodvisni), potem je $D(X+Y) = D(X) + D(Y)$
        \begin{proof}
            Sledi iz enakosti
            \begin{align*}
                &(X+Y-E(X+Y))^2 ? ((X-E(X))+(Y-E(Y)))^2 = (X-E(X))^2 + (Y-E(Y))^2 + 2(X-E(X))(Y-E(Y)) \quad E() \\
                &D(X+Y) = E((X-E(X))^2) + E((Y-E(Y))^2) + E(2(X-E(X))(Y-E(Y))) = D(X) + D(Y) + 2K(X,Y)
            \end{align*}
        \end{proof}
    \item Posplo"sitev: $D(X_1 + \cdots + X_n) = D(X_1) + \cdots + D(X_n) + 2 \sum_{i<j} K(X_i,X_j)$ \\
        "Ce so $X_1 \cdots X_n$ paroma nekorelirani (posebej neodvisni), potem je $D(X_1 + \cdots + X_n) =
        D(X_1) + \cdots + D(X_n)$
\end{enumerate}

\begin{ex}
    $Bin(n,p)$ je vsota $X = X_1 + \cdots + X_n$, kjer je $X_i \sim Ber(p)$, t.j. $X_i \sim
    \begin{pmatrix}0 & 1 \\ q & p\end{pmatrix}$, ki so neodvisne \\
    Zato je $D(X) = D(X_1 + \cdots + X_n) = n \cdot D(X_1) = n \cdot p \cdot q$, saj je
    $D(X_n) = E(X_n^2) - (E(X_n))^2 = p - p^2 = pq$
\end{ex}

\begin{defn}[Standardizacija slu"cajne spremenljivke]
    Standardizacija sku"cajne spremenljivke $X$ je slu"cajna spremenljivka $X_s = \frac{X-E(X)}{\sigma(X)}$ 
\end{defn}

Zanjo velja:
\begin{itemize}
    \item $E(X_s) = 0$
    \item $D(X_s) = \frac{1}{\sigma(x)^2} \cdot D(X-E(X)) = \frac{1}{\sigma(X)^2} D(X) = 1$
\end{itemize}

\begin{ex}
    \begin{equation*}
        X \sim N(\mu, \sigma) \implies X_s = \frac{X-E(X)}{\sigma(X)} = \frac{X-\mu}{\sigma} \sim N(0,1)
    \end{equation*}
\end{ex}

\begin{defn}[Korelacijski koeficient]
    Korelacijski koeficient slu"cajnih spremenljivk $X$ in $Y$ je
    \begin{equation*}
        r(X,Y) = \frac{K(X,Y)}{\sigma(X) \sigma(Y)} = \frac{E((X-E(X))(Y-E(Y)))}{\sigma(X)\sigma(Y)} = E(X_s \cdot Y_s)
    \end{equation*}
\end{defn}

Lastnosti:

\begin{enumerate}
    \item $r(X,Y) = 0 \iff X$ in $Y$ sta nekorelirani
    \item $r(X,Y) \in [-1,1]$, kar sledi iz lastnosti (4) za kovarianco
    \item \begin{itemize}
        \item $r(X,Y) = 1 \iff Y = \frac{\sigma(Y)}{\sigma(X)} (X-E(X)) + E(Y)$ z verjetnostjo 1
        \item $r(X,Y) = -1 \iff Y = -\frac{\sigma(Y)}{\sigma(X)} (X-E(X)) + E(Y)$ z verjetnostjo 1
    \end{itemize}
        Tedaj imamo linearno zvezo med $X$ in $Y$
\end{enumerate}

\begin{ex}
    \begin{equation*}
        (X,Y) \sim N(\mu_x, \mu_y, \sigma_x, \sigma_y, \rho) \; \mu_x, \mu_y \in \R, \sigma_x, \sigma_y \in [0,\infty], \rho \in [-1,1]
    \end{equation*}
    Trdimo, da je $r(X,Y) = \rho$
    \begin{align*}
        &(X_s,Y_s) \sim N(0,0,1,1,\rho) \\
        &r(X,Y) = E(X_s \cdot Y_s) = \frac{1}{2\pi \sqrt{1-p^2}} \underset{\R}{\iint} x y 
            e^{-\frac{1}{2(1-\rho^2)} (x^2 - 2\rho xy ' y^2)} dx dy \\
    \end{align*}
    \begin{equation*}
        x^2 - 2 \rho xy + y^2 = (x-\rho y)^2 + (1-\rho^2)y^2
    \end{equation*}
    \begin{align*}
        %
        %
        %
% 14. predavanje: 13.1.
        %
        &= \int_{-\infty}^{\infty} y e^{-\frac{1}{2}y^2} dy = \frac{1}{\sqrt{2\pi (1-\rho^2)}}
        \int_{-\infty}^{\infty} x e^{-\frac{1}{2(1-\rho^2)} (x - \rho y)^2} dx = \\
        &=E(N(\rho y, \sqrt{1-\rho^2})), \text{ ker je } p(x) = \frac{1}{\sigma \sqrt{2\pi}}
        e^{-\frac{1}{2} (\frac{x - \mu}{\sigma})^2} = \\
        &= \rho \frac{1}{\sqrt(2\pi)} \int_{-\infty}^{\infty} y^2 e^{-\frac{1}{2} y^2} dy = \\
        &= (\frac{1}{\sqrt(2\pi)} \int_{-\infty}^{\infty} y^2 e^{-\frac{1}{2} y^2} = D(N(0,1)) = 1) \implies = \rho
    \end{align*}
\end{ex}
Torej sta X in Y nekorelirani $\stackrel{\text{v splo"snem}}{\iff} \rho = 0 \stackrel{\text{ta primer}}{\iff}
X, Y$ neodvisni \\
Kak"sna je gostota, "ce je $\rho$ blizu 1?
$\rho \uparrow 1:$ %skica
$\rho \downarrow -1:$ \\ %skica
gostota je \"skoraj skoncentrirana\" na neki premici, torej med X in Y obstaja skoraj linearna zveza

\subsection{Pogojna porazdelitev in pogojno matemati"cno upanje}

Izberimo si dogodek B s $P(B) > 0$

\begin{defn}
    Pogojna porazdelitvena funkcija slu"cajne spremnljivke X glede na B je $F_X(X \mid B) := P(X \leq x \mid B) =
    \frac{P(X \leq x \land B)}{P(B)}$ \\
\end{defn}

Ima enake lastnosti kot porazdelitvena funkcija

\begin{enumerate}[label=\Alph*]
    \item Diskreten primer \\
        Naj bo (X,Y) diskretno porazdeljen slu"cajni vektor z verjetnostno funkcijo $p_{ij} = P(X=x_i, Y=y_i) i,j = 1, 2 \cdots$ \\
        Za pogoj B vzemimo $B = (Y=y_j)$ pri nekem j, torej $q_j = P(Y=Y_j)$ \\
        Potem je pogojna porazdelitvena funkcija slu"cajne spremenljivke X glede $F_X(X \mid Y=y) :=
        \frac{P(X \leq x \mid Y=y_j)}{P(Y=y_j)} = \frac{1}{q_{j}} \sum_{j: x_j \leq x} p_{ij}$ \\
        "Ce vpeljemo pogojno verjetnostno funkcije $P_{i \mid j} = P(X=x_i \mid Y=y_j) = \frac{p_{ij}}{q_j}$,
        $F_X(X \mid Y=y_j) = \sum_{i: x_i \leq X} p_{i \mid j}$ \\
        Pogojno matemati"cno upanje slu"cajne spremenljivke X glede na $Y=y_j$ je matemati"cno upanje te porazdelitve:
        \[E(X \mid Y=y_j) := \sum_{i} x_i \cdot p_{i \mid j} = \frac{1}{q_j} \sum_{i} x_j \cdot p_{ij} \]    % e(x_i)?
        Regresijska funkcija $\ell (y_j) = \sum (X \mid Y=y_j)$, ki je definirana na zalogi vrednoti slu"cajne
        spremenljivke Y \\
        Definirajmo novo slu"cajno spremenljivko $E(X \mid Y) = \ell (y)$, ki ji re"cemo pogojno matemati"cno upanje
        slu"cajne spremenljivke X glede slu"cajne spremenljivke Y \\
        Ta ima shemo $E(X \mid Y) = \begin{pmatrix} \ell (y_1) & \ell (y_2) & \cdots \\ q_1 & q_2 & \cdots
        \end{pmatrix} = \begin{pmatrix} E(X \mid Y=y_1) & \cdots \\ q_1 & \cdots
        \end{pmatrix}$ \\
        Zanjo velja
        \[E(X \mid Y) = \sum_j \ell (y_j) \cdot q_j ? \sum_j \sum_i x_i \cdot p_{ij} = \sim_i x_i (\sum_j p_{ij}) =
        \sum_i x_i \cdot p_i = E(X)\]
        kjer je $p_i = P(X=x_i)$ \\
        Kaj dobimo, "ce sta X in Y neodvisni slu"cajni spremenljivki? \\
        Tedaj je $p_{i \mid j} = \frac{p_{ij}}{q_j} = \frac{p_i \cdot q_j}{q_j} = p_i$ in
        $\ell (y_j) = E(E(X \mid Y=y_j)) = \sum_i x_i \cdot p_{i \mid j} = \sum_i x_i \cdot p_i = E(X)$, torej je
        regresijska funkcija kar konstanta $E(X)$ oz. je $E(X \mid Y)$ izrojena slu"cajna spremenljivka z vrednostjo $E(X)$

        \begin{ex}
            Koko"s znese N jajc, kjer je $N \sim Poi(\lambda)$ z $\lambda > 0$. Iz vsakega jajca se z verjetnostjo
            $p \in (0,1)$ izvali pi"s"canec, neodvisno od drugih jajc. Naj bo K "stevilo pi"s"cancev Dolocino $E(K \mid N),
            E(K) in E(N \mid K)$ \\
            \[P(N=n) = \frac{\lambda^n}{n!} e^{-\lambda} \; n = 0, 1, 2 \cdots \]
            \[P(K=k \mid N=n) = \binom{n}{k} p^k q^{n-k} \; k = 0, 1 \cdots n \]
            \[\ell(n) = E(K \mid N=n) = E(Bin(n,p)) = n \cdot p\]
            torej je $E(K \mid N) = \ell(n) = p \cdot N$
            \[E(K \mid N) = \begin{pmatrix}p \cdot 0 & p \cdot 1 & p \cdot 2 & \cdots \\ P(N=0) & P(N=1) & P(N=2) & \cdots
            \end{pmatrix} \]
            \[E(K) = E(E(K \mid N)) = E(p \cdot N) = p \cdot E(N) = p \cdot \lambda \]
            \[P(K=k) = \sum_{n=k}^{\infty} P(K=k \mid N=n) \cdot P(N \leq n) = \sum_{n=k}^{\infty} \frac{n!}{k!(n-k)!}
            p^k q^{n-k} \cdot \frac{\lambda^n}{n!} e^{-\lambda} = \]
            \[= \frac{1}{k!} e^{-\lambda} p^k \lambda^k
            \sum_{n=k}^{\infty} \frac{(qk)^{n-k}}{(n-k)!} = \frac{(p\lambda)^k}{k!} e^{-\lambda} e^{q\lambda} =
            \frac{(p\lambda)^k}{k!} e^{-p\lambda} \; k = 0, 1 \cdots n\]
            Torej je $K \sim Poi(p \cdot \lambda)$ \\
            \[P(N=n \mid K=k) = \frac{P(N=n, K=k)}{P(K=k)} = \frac{P(K=k \mid N=n) \cdot P(N=n)}{P(K=k)} =\]
            \[= \frac{n! p^k q^{n-k}}{k! (n-k)!} \cdot \frac{\lambda^n e^{-\lambda}}{n!} \cdot
            \frac{p k! e^{p\lambda}}{(p \lambda)^k} = \frac{(q\lambda)^{n-k}}{(n-k)!} \cdot e^{-q\lambda} n = k, k+1 \cdots\]
            To je za k premaknjena Poissonova porazdelitev: $k + Poi(q \lambda)$ \\
            Potem je $\psi(k) = E(N \mid K=k) = E(k + Poi(q k)) = k + q \cdot \lambda$ in zato je $E(N \mid K) = \psi(k) =
            k \cdot q + \lambda$ \\
            Preizkus: $E(E(N \mid K)) = E(k + q \cdot \lambda) = p \lambda + q \lambda = \lambda = E(N)$ (ok) \\
            Regresijsko premico je vpeljal Golten (1822-1911)
        \end{ex}
    \item Zvezni primer \\
        Naj bo $(X,Y)$ zvezno porazdeljen slu"cajni vektor z gostoto $p_{(X,Y)}(x,y)$. Vzemimo $B = (y < Y \leq y+k)$ za
        nek $y \in \R, k > 0$. \\
        Potem je $F_X(X \mid y < Y \leq y+k) = P(x \leq x \mid y < Y \leq y+k) =
        \frac{P(X \leq x, y < Y \leq y+k)}{P(y < Y \leq y+k)} = \frac{F_{(X,Y)}(x,y+k) - F_{(X,Y)}(x,y)}{F_Y(y+k) - F_Y(y)}$ \\
        Pogojna porazdelitvena funkcija slu"cajne spremenljivke X glede na dogodek $(Y=y)$ je limita, "ce obstaja:
        \[F_X(x \mid Y=y) = \lim_{h \downarrow 0} F_X(x \mid y < Y \leq y+h) = \lim_{h \downarrow 0}
        \frac{F_{(X,Y)}(x,y+h) - F_{(X,Y)}(x,y)}{F_Y(y+h) - F_Y(y)}\]



% 15. predavanje: 14.2.

        Denimo sedaj, da sta $p_{X,Y}$ in $p_Y$ zvezni funkciji. Tedaj je $F_X(X \mid Y=y) = 
        \frac{\frac{\partial}{\partial y} F_{(X,Y)}(x,y)}{F_Y^{'}(y)} = \frac{1}{p_Y(y)}
        \int_{-\infty}^x p_{(X,Y)}(x,v) dv$ \\
        "Ce vpeljemo pogojno pogojno gostoto $p_X(x \mid Y=y) := \frac{p_{(X,Y)}(x,y)}{p_Y(y)}$, je torej
        \[F_{(X,Y)}(x \mid Y=y) = \int_{-\infty}^x p_X(u \mid y) du \]
        Pogojno matemati"cno upanje slu"cajne spremenljivke X glede na dogodek $(Y=y)$ je 
        \[E(X \mid Y=y) :=
        \int_{-\infty}^{\infty} x \cdot p_X(x|y) dx = \frac{1}{p_Y(y)} \cdot \int_{-\infty}^{\infty}
        x p_{(X,Y)}(x,y) dx\]
        Vpeljimo regresijsko funkcijo $l(y) := E(X \mid Y=y)$, definirano na zalogi vrednosti slu"cajne spremenljivke Y.
        Tako dobimo novo slu"cajno spremenljivko $E(X \mid Y) := l(y)$: pogojno matemati"cno upanje slu"cajne spremenljivke
        X glede na slucajno spremenljivko Y. \\
        Kot v diskretnem primeru se poka"ze enakost $E(E(X \mid Y)) = E(X)$

        \begin{ex}
            $(X,Y) \sim N(\mu_x,\mu_y,\sigma_x,\sigma_y,\rho)$ \\
            Robna gostota za Y je $N(\mu_y,\sigma_y)$ \\
            Zato je pogojna gostota
            \[p_X(x \mid y) = \frac{p_{(X,Y)}(x,y)}{p_y(x)} = \stackrel{\text{D.N.}}{\cdots} = \frac{1}{\sigma_x \sqrt(2\pi) (1-\rho^2)}
            exp(-\frac{1}{2 (1-\rho)^2} (\frac{x-\mu_x}{\sigma_x} - \rho \frac{y-\mu_y}{\sigma_y})^2)\]
            torej je $N(\mu_x + \rho \frac{\sigma_x}{\sigma_y}(y-\mu_y), \sigma_x \sqrt{1 - \rho^2})$ \\
            Eksponent: $\frac{1}{2 (1 -\rho^2)} \sigma_x^2 (x - (\mu_x + \rho \frac{\sigma_x}{\sigma_y} (y-\mu_y)))^2$ \\
            $\implies l(y) = E(X \mid Y=y) = \mu_x + \rho \frac{\sigma_x}{\sigma_y} (y - \mu_y)$ - 1. parameter \\
            $= \alpha + \beta y: \beta = \rho \frac{\sigma_x}{\sigma_y}, \alpha = \mu_x - \frac{\sigma_x}{\sigma_y} \cdot \mu_y$ \\
            Torej je $E(x \mid y) = \alpha + \beta y$
        \end{ex}

        \begin{ex}
            Meritev onesna"zenosti zraka \\
            Slu"cajna spremenljivka X meri koncentracijo ogljikovih delcev (v $\mu g / m^3$), Y pa koncentracijo ozona (v $\mu l/l = ppm$) \\
            Podatki ka"zejo, da ima (X,Y) pribli"zno dvorazse"zno normalno porazdelitev, $\mu_x = 10.7, \sigma_x^2 = 29, \mu_y = 0.1,
            \sigma_y^2 = 0.02, \rho = 0.72$ \\
            Koncentracija ozona je "skodljiva zdravju, "ce je $\geq 0.3$ \\
            Denimo, da naprava za merjenje ozona odpove, koncentracija "skodljivih delcev je $X = 200$
            \begin{enumerate}[label=\alph*]
                \item kolik"sna je pri"cakovana koncentracija ozona?
                \item kolik"sna je verjetnost, da je stopnja ozona zdravju skodljiva
            \end{enumerate}
            \begin{enumerate}[label=\alph*]
                \item \[E(Y \mid X=x) = \mu_y + \rho \frac{\sigma_y}{\sigma_x} (x - \mu_x) =
                    0.1 + 0.72 \sqrt{\frac{0.02}{29} (20 - 10.7)} \dot{=} 0.28 \]
                    % skica
                \item Pogojna porazdelitev $Y \mid X=x$ je $N(\mu_y + \rho \frac{\sigma_y}{\sigma_x} (x - \mu_x), \sigma_x \sqrt{1 - \rho^2}) =
                    N(0.28, 0.1)$ \\
                    \[P(Y>0.3 \mid X=20) = 1 - P(Y \leq 0.3 \mid X=20) = 1 - F_{N(0,1)} (\frac{0.3 - 0.28}{0.1}) \dot{=} 0.42 \]
            \end{enumerate}
        \end{ex}
\end{enumerate}

\subsection{Vi"sji momenti in vrstilne karakteristike}

\begin{defn}[Momenti]
    Naj bo $k \in \N$ in $a \in \R$. Moment reda k glede na to"cko a je $m_k(a) := E((X-a)^k)$ ("ce obstaja)
\end{defn}

Za a obicajno vzamemo
\begin{enumerate}
    \item $a=0$: $z_k := m_k(0) = E(X^k)$ za"cetni moment reda k
    \item $a=E(X)$: $m_k := m_k(E(X))$ cenralni moment reda k
\end{enumerate}

Ocitno je $z_1 = E(X), m_2 = D(X)$

\begin{claim}
    "Ce $\exists m_n(a)$, potem obstajaj tudi moment $m_k(a)$ za vse $k < n$
\end{claim}

\begin{proof}
    (V zveznem primeru): \\
    \[E((X-a)^k) = \int_{-\infty}^{\infty} (x-a)^k p_X(x) dx = \int{a-1}^{a+1} (X-a)^k p_X(x) dx +
    \int_{(-\infty,a-1) \cup (a+1,\infty)} (x-a)^k p_X(x) dx \leq \]
    \[\leq \int_{-\infty}^{\infty} p_X(x) dx + \int_{(-\infty,a-1) \cup (a+1,\infty)} (x-a)^k p_X(x) dx \leq\]
    \[\leq 1 + E((X-a)^k) < \infty\]
\end{proof}

\begin{claim}
    "Ce obstaja zacetni moment $z_n$, potem obstaja $m_n(a)$ glede na poljubno to"cko $a \in \R$
\end{claim}

\begin{proof}
    \[E((X-a)^n) \leq E((|X| + |a|)^n) = \sum_{k=0}^n \binom{n}{k} E(a)^{n-k} \cdot E(|X|^k) < \infty\]
\end{proof}

Centralne momente lahko izrazimo z za"cetnimi:
\[m_n(a) = E((X-a)^n) = \sum_{k=0}^n \binom{n}{k} (-a)^{n-k} E(X^k)\]
\[a = E(X) \; \implies \; m_k = \sum_{k=0}^n \binom{n}{k} (-1)^{n-k} z_1^{n-k} z_k \]

Asimetrija slu"cajne spremenljivke X je $A(X) := E(X_s^3) = E((\frac{X-E(X)}{\sigma_x})^3) =
\frac{m_3}{m_{2}^{\frac{3}{2}}}$ $m_2 = \sigma^2 = D(X)$ \\
$A(N(\mu,\sigma)) = 0, $ ker $A(X) = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} x^3 e^{-\frac{1}{2}x^2} dx$ \\
Splo"s"cenost (kurtozis) $K(X) := E(X_s^4) = \frac{m_4}{m_2^2}$ \\
$K(N(\mu,\sigma)) = 3$ \\
Ce momenti ne obstajajo (npr. "ze $E(X)$ ne), potem si lahko pomagamo z vrstilnimi karakteristikami

\begin{defn}[Mediana]
    Mediana slu"cajne spremenljivke X je vsaka vrednost $x \in \R$, za katero velja $P(X \leq x) \leq \frac{1}{2}$
    in $P(Y \geq x) \geq \frac{1}{2} (1-P(X < x) = 1 - F(x-))$
\end{defn}

"Ce je F porazdelitvena funkcija za X, je to ekvivalentno s pogojem $F(x-) \leq \frac{1}{2} \leq F(x)$ \\
"Ce je X zvezno porazdeljena slu"cajna spremenljivka, dobimo $F(X) = \frac{1}{2}$ oz.
$\int_{-\infty}^{\infty} p(t) dx = \frac{1}{2}$     % skica grafa



% 16. predavanje: 21.2.

Te vrednosti (lahko jih je ve"c) ozna"cimo z $X_{\frac{1}{2}}$

\begin{ex} \text{} \\
    \begin{itemize}
        \item
            $X \sim \begin{pmatrix}0 & 1 \\ \frac{1}{5} & \frac{4}{5} \end{pmatrix}$ \\
            % skica
            $x_{\frac{1}{2}} = 1, E(X) = \frac{4}{5}$
        \item $X: \begin{pmatrix}-1 & 0 & 1 \\ \frac{1}{4} & \frac{1}{4} & \frac{2}{4} \end{pmatrix}$ \\
            % skica
            Mediane so $[0,1]$
        \item % skica
        \item $X \sim N(0,1)$ \\
            % skica
            $x_{\frac{1}{2}} = \mu = E(X)$
    \end{itemize}
\end{ex}

\begin{defn}[Kvantil]
    Kvantil reda p $(p \in (0,1))$ je vsaka vrednost $x_p$, za katero velja $P(X \leq x_p) \geq p$ in $P(X \geq x_p) \geq 1-p$ \\
    Ekvivalentno je $F(x_p-) \leq p \leq F(x_p)$
\end{defn}

"Ce je X zvezno porazdeljena, je pogoj $F(x_p) = p$ t.j. $\int_{-\infty}^{\infty} p(t) dt = p$

\begin{itemize}
    \item Kvartili: $X_{\frac{1}{4}}, X_{\frac{2}{4}}, X_{\frac{3}{4}}$
    \item Percentili: $X_{\frac{1}{100}}, X_{\frac{2}{100}}, \cdots X_{\frac{99}{100}}$
\end{itemize}

\begin{ex}
    Telesna vi"sina odraslih mo"skih
    % skica
\end{ex}

\begin{defn}[(Semiinter)kvartilni razmik]
    $s := \frac{1}{2} (x_{\frac{3}{4}} - x_{\frac{1}{4}})$
\end{defn}

je nadomestek (analog) za standardno deviacijo

\begin{ex} \text{} \\
    \begin{itemize}
        \item $X \sim N(0,1)$ \\
            $X_{\frac{1}{2}} = 0$ \\
            $\int_{-\infty}^{\frac{1}{4}} p(t) dt = \frac{1}{4} \xRightarrow{\text{tabelca}} x_{\frac{1}{4}} \doteq -0.67$ \\
            $\xRightarrow{\text{simetrija}} x_{\frac{3}{4}} \doteq 0.67 \implies s = 0.67, \sigma(x) = 1$ \\
        \item $X$ naj ima Cauchyjevo porazdelitev \\
            $p(x) = \frac{1}{\pi(1 + x^2)}$ \\
            $x_{\frac{1}{2}} = 0$ \\
            % skica
            Momenti ne obstajajo \\
            \[\int_{-\infty}^{x_{\frac{1}{4}}} \frac{1}{\pi} \frac{1}{1 + x^2} dx = \frac{1}{4} \]
            \[\frac{1}{\pi} \arctan x \vert_{x=-\infty}^{x_{\frac{1}{4}}} = \frac{1}{4} \]
            \[\frac{1}{\pi} arctan x_{\frac{1}{4}} + \frac{1}{2} = \frac{1}{4}\]
            \[arctan x_{\frac{1}{4}} = \frac{1}{4} \implies x_{\frac{1}{4}} = -1\]
            \[\xRightarrow{\text{simetrija}} x_{\frac{3}{4}} = 1, s = 1\]
    \end{itemize}
\end{ex}

\subsection{Rodovne funkcije}

\begin{defn}
    Naj bo X slu"cajna spremenljivka z vrednostmi v $\N \cup \{0\}: p_k = P(X = k) k = 0, 1, 2 \cdots \;
    p_k \geq 0, \sum_{k = 0}^{\infty} = 1$ \\
    Rodovna funkcija sku"cajne spremenljivke X je
    \[G_X(s) = p_0 + p_1 s + p_2 s^2 + \cdots = \sum_{k = 0}^{\infty} p_k \cdots s^k\]
    za $\forall s \in \R$, za katere vrsta absolutno konvergira.
\end{defn}

O"citno je $G_X(0) = p_0, G_X(1) = \sum_{k = 0}^{\infty} p_k = 1$ \\
Ker je $s^X: \begin{pmatrix}s^0 & s^1 & s^2 & \cdots \\ p_0 & p_1 & p_2 & \cdots\end{pmatrix}$, je $G_X(s) = E(s^X)$ \\
Za $s \in [-1,1]$ velja $|p_k \cdot s^k| \leq P_k$ in $\sum_{k = 0}^{\infty} p_k = 1$. Zato je vrsta
konvergentna, "ce je $|s| \leq 1$. Torej je konvergen"cni radij vrste vsaj 1

\begin{ex} \text{} \\
    \begin{itemize}
        \item $X \sim geo(p)$, $p \in (0,1)$
            \begin{align*}
                &p_k = P(X = k) = p \cdot q^{k-1} \; k = 1,2,3 \cdots \\
                &G_X(s) = \sum_{k = 1}^{\infty} p \cdot q^{k - 1} s^k = ps \sum_{k = 0}^{\infty} (qs)^{k-1} \\
                &= ps \frac{1}{1 - qs}
            \end{align*}
            konvergira, ko $|qs| < 1 \Leftrightarrow |s| < \frac{1}{|q|} =: R$
        \item $p_k = P(X = k) = \frac{\lambda^k}{k!} e^{-\lambda}$
            \[G_X(s) = \sum_{k = 0}^{\infty} \frac{\lambda^k}{k!} e^{-\lambda} s^k =
            e^{-\lambda} \sum_{k = 0}^{\infty} \frac{(\lambda s)^k}{k!} = \]
            \[= e^{-\lambda} \cdot e^{\lambda s} = e^{\lambda(s - 1)} \]
            $R = \infty \; \forall s \in \R$
    \end{itemize}
\end{ex}

Iz teorije Taylorjevih vrst sledi

\begin{theorem}[O enili"cnosti]
    Naj imata X in Y rodovni funkciji $G_X$ in $G_Y$. Potem je $G_X(s) = G_Y(s)$ za $\forall s \in [-1,1] \leftrightarrow
    P(X = k) = P(Y = k)$ za vse $k = 0, 1, 2 \cdots$ \\
    Tedaj velja $P(X = k) = \frac{1}{k!} G_X^{k}(0)$
\end{theorem}

$G_X(s) = \sum_{k = 0}^{\infty} p_k s^k$, $p_k = P(X = k)$ \\
Naj ima rodovna funkcija $G_X$ slu"cajne spremenljivke X konvergen"cni radij R > 1. Potem za $\forall s \in (-R,R)$ velja
$G_X^{'}(s) = \sum_{k = 1}^{\infty} k \cdot p_k s^{k-1}$ \\
"Ce postavimo $s=1$, dobimo $G^{'}(1) = \sum_{k = 1}^{\infty} k \cdot p_k = E(X)$

\begin{theorem}
    Naj ima X rodovno funkcijo $G_X(s)$ in naj bo $n \in \N$. Potem je
    \[G_X^{n}(1-) \equiv \lim_{s \nearrow 1} G_X^{n}(s) = E(X (X-1) (X-2) \cdots (X-N+1))\]
\end{theorem}

\begin{proof}
    Za $\forall s \in [0,1)$ je $G_X^{n}(s) = \sum_{k = n}^{\infty} k(k-1)(k-2) \cdots (k-n+1) p_k s^{k-n+1} =$
    \[= E(X(X-1)(X-2) \cdots (X-n+1) \cdot s^{X-n}) \]
    Ko gre $s \uparrow 1$, z uporabo Abelove leme dobimo
    \[\lim_{s \nearrow 1} G_X^{n}(s) = \lim_{s \nearrow 1} \sum_{k = n}^{\infty} k(k-1) \cdot (k-n+1) =\]
    \[\stackrel{\text{Abelova lema}}{=} \sum_{k = n}^{\infty} lim_{s \nearrow 1} k(k-1) \cdot (k-n+1) =
    \sum_{k = n}^{\infty} k(k-1) \cdot (k-n+1) p_k = E(X(X-1) \cdots (X-n+1))\]
\end{proof}

\begin{conseq}
    \[E(X) = G_{X}^{'}(1-)\]
    \[D(X) = E(X^2) - (E(X))^2 = E(X(X-1)) + E(X) - (E(X))^2 = G_X^{(2)}(1-) + G_X^{(1)}(1) - (G_X^{(1)}(1-))^2\]
\end{conseq}

\begin{theorem}
    Naj bosta X in Y neodvisni slu"cajni spremenljivki z rodovnima funkcijama $G_X$ in $G_Y$. Potem je $G_{X+Y}(s) =
    G_X(s) \cdot G_Y(s)$ za $s \in [-1,1]$
\end{theorem}

\begin{proof}
    $G_{X+Y}(s) = E(s^{X+Y}) = E(s^X \cdot s^Y) \stackrel{\text{izrek}}{=} E(s^X) \cdot E(s^Y) = G_X(s) \cdot G_Y(s)$,
    saj sta $s^X$ in $s^Y$ neodvisni slu"cajni spremenljivki
\end{proof}

\begin{general}
    "Ce so $X_1, X_2 \cdots X_n$ neodvisne slu"cajne spremenljivke, potem je za vse $s \in [-1,1] G_{X_1 + \cdots + X_n}(s) =
    G_{X_1}(s) \cdot \cdots \cdot G_{X_n}(s).$ \\
    "Ce so $X_1, X_2 \cdots X_n$ enako porazdeljene in neodvisne, potem je
    \begin{align*}
        G_{X_1 + \cdots + X_n}(s) = (G_X(s))^n    % (*)
    \end{align*}
\end{general}

\begin{theorem}
    Naj bodo za $\forall n \in \N$ slu"cajne spremenljivke $N, X_1, X_2 \cdots X_n$ neodvisne. Naj ima N rodovno
    funkcijo $G_N, X_n$ pa rodovno funkcijo $G_X$. Potem ima slu"cajna spemenljivka $S := X_1 + X_2 + \cdots + X_n$
    rodovno funkcijo enako $G_S = G_N \circ G_X$ oz. $G_S(s) = G_N(G_X(s))$ za $s \in [-1,1]$
\end{theorem}

To je posplo"sitev formule dd: $P(N = n) = 1, G_N(s) = 1 \cdot s^n = s^n$%(*)

\begin{proof}
    Zaradi neodvisnosti imamo $P(S = k) = \sum_{n=0}^{\infty} P(S = k, N = n) =$
    \[= \sum_{n=0}^{\infty} P(N = n, X_1 + \cdots + X_n = k) \stackrel{\text{neodvisnost}}{=}
    \sum_{n=0}^{\infty} P(N = n) \cdot P(X_1 + \cdots + X_n = k)\]
    Zato je
    \[G_S(s) = \sum_{k=0}^{\infty} P(S = k) \cdot s^k =
    \sum_{k=0}^{\infty} \sum_{n=1}^{\infty} P(N = n) \cdot P(X_1 + \cdots + X_n = k) \cdot s^k =\]
    \[= \sum_{n=1}^{\infty} P(N = n) (\sum_{k=0}^{\infty} P(X_1 + \cdots + X_n = k) \cdot s^k) =\]
    \[\stackrel{G_{X_1 + \cdots + X_n}(s) \stackrel{\text{neodvisnost}}{\*} (G_X(s)^n)}{=}
    \sum_{n=1}^{\infty} P(N = n) \cdot (G_X(s))^n = G_N(G_X(s))\]
    za vse $s \in [-1,1]$
\end{proof}



% 17. predavanje: 28.2.

\begin{conseq}
	Pri predpostavkah iz izreka velja Waldova enakost: \[E(S) = E(N) \cdot E(X)\]
\end{conseq}

\begin{proof}
    \begin{align}
    &G_S(s) = G_N(G_X(s)) \forall s \in [-1,1] \\
    &E(S) = G_s^{'}(1-) = G_N^{'}(G_X(1-)) \cdot G_X^{'}(1-) = E(N) \cdot E(X)
    \end{align}
\end{proof}

\begin{ex}
    Koko"s, jajca, pi"s"canci \\
    N jajc, $N \sim Poi(\lambda)$ \\
    K je "stevilo pi"s"cancev \\
    Definiramo $X_i = 1$ dogodek, da se iz i-tega jajca izvali pi"s"canec, sicer $X_i = 0$. Potem je
    $X_i: \begin{pmatrix}
        0 & 1 \\
        q & p
    \end{pmatrix}, q = 1 - p$ in $X_i$ so neodvisne slu"cajne spremenljivke. \\
    O"citno je $K = X_1 + X_2 + \cdots + X_n$ \\
    Ker je $G_N(s) = e^{\lambda(s-1)}$ in $G_X(s) = q \cdot s^0 + p \cdot s = q + ps$, je po
    izreku $G_K(s) = G_N(G_X(s)) = e^{\lambda(q + ps - 1)} = e^{\lambda(ps - p)} = e^{\lambda p(s-1)} \forall s \in [-1,1]$,
    zato je $K \sim Poi(\lambda p)$
\end{ex}

\subsection{Momentno rodovna funkcija}

\begin{defn}[Momentno rodovna funkcija]
    Momentno rodovna funkcija je $M_X(t) = E(e^{tX})$ za $t \in \R$, za katere obstaja matemati"cno upanje
\end{defn}

V primeru zvezne porazdelitve je $M_X(t) = \int_{-\infty}^{\infty} e^{tx} p_X(x) dx$ \\
To je Laplaceova transformacija funkcije $p_X$ \\
V diskretnem primeru $X: \begin{pmatrix}x_1 & x_2 & \cdots \\ p_1 & p_2 & \cdots \end{pmatrix}$ je
$M_X(t) = \sum_i e^{tx} p_i$ \\

V posebnem primeru, ko ima X nenegative celo"stevilske vrednosti, je $M_X(t) = \sum_{i=0}^{\infty} e^{it} p_i =$
\[= \sum_{i=0}^{\infty} p_i (e^{t})^{i} = G_X(e^t) \; (M_X(t) = E((e^t)^X) = G_X(e^t))\]
\[G_X(s) = E(s^X)\]

O"citno je $M_X(0) = E(e^0) = E(1) = 1$

\begin{ex}
    \[X \sim N(0,1)\]
    \[M_X(t) = \int_{-\infty}^{\infty} e^{tx} \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}} dx =\]
    \[= \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} e^{-\frac{(x-t)^2}{2}} dx \cdot e^{-\frac{t^2}{2}} =\]
    \[= e^{\frac{t^2}{2}} \forall t \in \R\]
    ker je $\frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} e^{-\frac{(x-t)^2}{2}}$ gostota za $N(0,1)$
\end{ex}

\begin{theorem}
    Naj bo $M_X(t) < \infty$ (obstaja, $< \infty$ zato, ker je $e^t > 0$) za $\forall t \in (-\delta, \delta)$ pri
    nekem $\delta > 0$. Potem je porazdelitev za X natanko dolo"cena z $M_X$, vsi za"cetni momenti obstajajo,
    $z_k = E(X^k) = M_X^{k}(0)$ za $\forall k \in \N$ in velja $M_X(t) = \sum_{k=0}^{\infty} \frac{z_k}{k!} t^k$ % M_X^{(k)} ?? 
    za $\forall t \in (-\delta, \delta)$
\end{theorem}

\begin{proof} (bistvo)
    \[M_X(t) = E(e^{t \cdot X}) = E(\sum_{k=0}^{\infty} t^k \frac{x^k}{k!}) =\]
    \[\sum_{k=0}^{\infty} \frac{E(X^k)}{k!} t^k = \sum_{k=0}^{\infty} \frac{z^k}{k!} t^k\]
\end{proof}

\begin{claim}
    $M_{aX+b}(t) = e^{bt} M_X(at), a \neq 0, b \in R$
\end{claim}

\begin{proof}
    $M_{aX+b}(t) = E(e^{t(aX+b)}) = E(e^{(at)X} \cdot e^{bt}) = e^{bt} M_X(at)$
\end{proof}

\begin{theorem}
    "Ce sta X in Y neodvisni slu"cajni spremenljivki, potem je $M_{X+Y}(t) = M_X(t) \cdot M_Y(t)$
\end{theorem}

\begin{proof}
    $M_{X+Y}(t) = E(e^{t(X+Y)}) = E(e^{t^X} \cdot e^{tY}) \stackrel{\text{$e^{tX}$, $e^{tY}$ neodvisni}}{=} $ \\
    $= E(e^{t^X}) \cdot E(e^{tY}) = M_X(t) \cdot M_Y(t)$
\end{proof}

\begin{claim}
    Naj bosta X in Y neodvisni slu"cajni spremenljivki in $X \sim N(\mu_x, \sigma_x), Y \sim N(\mu_y, \sigma_y)$.
    Potem je $X + Y \sim N(\mu_x + \mu_y, \sqrt{\sigma_x^2 + \sigma_y^2})$
\end{claim}

\begin{proof}
    Ker je
    \[U := \frac{X-\mu_x}{\sigma_x} = \frac{X-E(X)}{\sigma(X)} \sim N(0,1)\]
    (standardizacija), je
    \[X = \sigma_x \cdot U + \mu_x\]
    in zato je
    \[M_X(t) = e^{\mu_x t} \cdot M_U(\sigma_x t)\]
    po zadnji trditvi. Potem je
    \[M_U(t) = e^{\frac{t^2}{2}}\]
    je
    \[M_X(t) = e^{\mu_x t} \cdot e^{\frac{\sigma_x^2 t^2}{2}} = e^{\frac{\sigma_x^2 t^2}{2} + \mu_x t} \; \forall t \ in \R\]
    za $Y$ velja podobno. Po zadnjem izreku je
    \[M_{X+Y}(t) = M_X(t) \cdot M_Y(t) = e^{\frac{\sigma_x^2 t^2}{2} + \mu_x t} \cdot e^{\frac{\sigma_y^2 t^2}{2} + \mu_y t} =\]
    \[= e^{\frac{(\sigma_x^2 + \sigma_y^2) t^2}{2} + (\mu_x + \mu_y) t}\]
    Po izreku je
    \[X + Y \sim N(\mu_x+\mu_y, \sqrt{\sigma_x^2 + \sigma_y^2})\]
\end{proof}

\begin{rem}
    Če bi vedeli, da je $X + Y$ porazdeljena normalno, bi ``samo'' izra"cunali parametra %integral?
\end{rem}

\begin{ex}
    \[X \sim N(0,1), M_X(t) = e^{\frac{t^2}{2}} = \sum_{k=0}^{\infty} \frac{(\frac{t^2}{2})^k}{k!} =
    \sum_{k=0}^{\infty} \frac{1}{2^k \cdot k!} t^{2k}\]
    Po drugi strani je $M_X(t) = \sum_{j=0}^{\infty} \frac{z_j}{j!} t^j \; \forall t \in \R$ \\
    Primerjamo koeficiente:
    \begin{itemize}
        \item lihi koeficienti: $z_{2k-1} = 0 \; k \in \N$
        \item sodi koeficienti:
        \[\frac{z_{2k}}{(2k)!} = \frac{1}{k! 2^k} \implies z_{2k} = \frac{(2k)!}{k! 2^k} =\]
        \[= \frac{1 \cdot 2 \cdot 3 \cdot \cdots \cdot (2k)}{2 \cdot 4 \cdot 5 \cdot \cdots \cdot (2k)} =
        1 \cdot 3 \cdot 5 \cdot \cdots \cdot (2k-1) = (2k-1)!! \; k \in \N\]
    \end{itemize}
\end{ex}

\subsection{"Sibki in krepki zakon velikih "stevil}

\begin{defn}[Verjetnostna konvergenca]
    Zaporedje slu"cajnih spremenljivk $\{X_n\}_{n \in \N}$ verjetnostno konvergira proti sku"cajni spremenljivki
    X, "ce za $\forall \epsilon > 0$ velja $\lim_{n \to \infty} P(|X_n-X| \geq \epsilon) = 0$ \\
    oz. $\lim_{n \to \infty} P(|X_n-X| < \epsilon) = 1$
\end{defn}

\begin{defn}[Skoraj gotova konvergenca]     % izraz ?
    Zaporedje slu"cajnih spremenljivk $\{X_n\}_{n \in \N}$ skoraj gotovo konvergira proti sku"cajni spremenljivki
    X, "ce velja P(p $\lim_{n \to \infty} X_n = X) = 1$ \\
    Tukaj je $(\lim_{n \to \infty} X_n = X) = \{\omega \in \Omega: \lim_{n \to \infty} X_n(\omega) = X(\omega)\} =$
    \[= \{\omega \in \Omega: \forall k (\in \N) \exists m \in \N \forall n \geq m: |X_n(\omega) - X(\omega)| < \frac{1}{k}\} =\]
    \begin{align}
        = \{\cap_{k \in \N} \cup_{m \in \N} \cap_{n \geq m} \omega \in \Omega: |X_n(\omega) - X(\omega)| < \frac{1}{k}\} \*
    \end{align}
\end{defn}

\begin{rem}
    Števne unije in preseki $\implies$ smo v $\sigma-$algebri, torej je to res dogodek
\end{rem}

\begin{claim}
    Če $X_n \xrightarrow[]{n \to \infty} X$ skoraj gotovo, potem za $\forall \epsilon > 0
    \lim_{n \to \infty} P(|X_n - X| < \epsilon \text{ za } n \geq m) = 1$
\end{claim}

\begin{proof}
    Ozna"cimo $c_m := (|X_n - X| < \epsilon \text{ za } n \geq m) = \cap_{n=m}^{\infty} (|x_n - X| < \epsilon)$. \\
    Potem je $c_1 \subseteq c_2 \subseteq \cdots$ \\
    \* je $c_m$ za $\epsilon = \frac{1}{k}$ in $(\lim_{n \to \infty} X_n = X) \subseteq \cup_{n=1}^{\infty} c_m$ (presek) \\
    Torej je $1 = P(\lim_{n \to \infty} X_n = X) \subseteq =(\cup_{m=1}^{\infty} c_m) = \lim_{m \to \infty} P(c_m)$ \\
    Od tod sledi $\lim_{m \to \infty} P(c_m) = 1$
\end{proof}

\begin{conseq}
    "Ce $X_n \xrightarrow[]{n \to \infty} X$ skoraj gotovo, potem $X_n \xrightarrow[]{n \to \infty} X$ verjetnostno konvergira.
\end{conseq}

\begin{proof}
    Izberemo $\epsilon > 0$. Potem velja
    \[P(|X_n - X| < \epsilon \text{ za } \forall n \geq m) \leq P(|X_m - X| < \epsilon)\]
    "Ce uporabimo trditev, dobimo $\lim_{n \to \infty} P(|X_n - X| < \epsilon) = 1$ (leva stran). % verjetnostna konvergenca \\
\end{proof}

\begin{rem}
    Obratna implikacija ne velja
\end{rem}



% 18. predavanje: 7.3.

\begin{defn}
    Naj bo $X_1, X_2, X_3 \cdots$ zaporedje slu"cajnih spremenljivk, ki imajo matemati"cno upanje.
    Definirajmo $Y_n = \frac{S_n - E(S_n)}{n} = \frac{X_1 + \cdots + X_n}{n} - \frac{E(X_1) + \cdots + E(X_n)}{n}$ \\
    Potem je $E(Y_n) = 0$ \\
    Za $\{Y_n\}_{n \in \N}$ velja "sibki zakon velikih "stevil ("SZV"S), kadar $Y_n \stackrel{n \to \infty}{\rightarrow} 0$
    verjetnostno, torej za $\forall \epsilon > 0 \lim_{n \to \infty} (|y| < \epsilon) = 1 =
    \lim_{n \to \infty} (|\frac{S_n - E(S_n)}{n}| < \epsilon)$
    Za $\{Y_n\}_{n \in \N}$ velja krepki zakon velikih "stevil (KZV"S), kadar $Y_n \stackrel{n \to \infty}{\rightarrow} 0$
    skoraj gotovo, torej $P(\lim_{n \to \infty} \frac{S_n - E(S_n)}{n} = 0) = 1$ \\
    "Ce velja KVZ"S, potem velja "SVZ"S
\end{defn}

\begin{ex}
    Me"cemo kocko, $X_k$ je $\#$ pik v k-tem metu. Potem je $E(X_k) = \frac{7}{2}$ in $Y_n = \frac{X_1 + \cdots + X_n}{n} - \frac{7}{2}$ \\
    Ali konvergira $\frac{X_1 + \cdots + X_n}{n} \stackrel{n \to \infty}{\rightarrow} \frac{7}{2}$ skoraj gotovo? (Da)
\end{ex}

\begin{theorem} \text{} \\
    \begin{enumerate}[label=\alph*]
        \item Neenakost Markova: "ce slu"cajna spremenljivka X ima matemati"cno upanje, potem je
            $P(|X| \geq a) \leq \frac{E(|X|)}{a}$ za $\forall a > 0$
        \item Neenakost "Cebi"seva: "ce slu"cajna spremenljivka X ima disperzijo, potem je
            $P(|X - E(X)| \geq a \cdot \sigma(x)) \leq \frac{1}{a^2}$ za $\forall a > 0$ (pomembno za
            $a \geq 1$, ker je verjetnost $\leq 1$) \\
            oz. "ce pi"semo $\epsilon = a \cdot \sigma(x) \implies P(|X - E(X)| \geq \epsilon) \leq \frac{D(X)}{\epsilon^2}$
            za $\forall \epsilon > 0$
    \end{enumerate}
\end{theorem}

\begin{proof}
    (samo zvezni primer)
    \begin{enumerate}[label=\alph*]
        \item \[E(X) = \int_{-\infty}^{\infty} |x| p_x(x) dx \geq \int_{\{x: |x| \geq a\}} |x| p_x(x) dx \geq\]
            \[|a| \int_{\{x: |x| \geq a\}} p_x(x) dx = a \cdot P(|X| \geq a)\]
        \item \[P((X - E(X)) \geq \epsilon) = P((X - E(X))^2 \geq \epsilon^2)
            \stackrel{\text{(a) za X-E(X)}}{\leq} \frac{E((X-E(X))^2)}{\epsilon^2} = \frac{D(X)}{\epsilon^2}\]
    \end{enumerate}
\end{proof}

\begin{theorem}[Markov]
    "Ce za zaporedje slu"cajnih spremenljivk $\{X_n\}_{n \in \N}$ velja $\frac{D(S_n)}{n^2} \stackrel{n \to \infty}{\rightarrow} 0$,
    potem velja "SZV"S. Tukaj je $S_n := X_1 + \cdots + X_n$
\end{theorem}

\begin{proof}
    V neenakosti "Cebi"seva vzamemo $X = \frac{S_n}{n}$
    \[P(\frac{|S_n - E(S_n)|}{n} \geq \epsilon) \leq \frac{P(S_n)}{n^2 \epsilon^2} \stackrel{n \to \infty}{\rightarrow} 0\]
    "Ce vzamemo $Y_n = \frac{|S_n - E(S_n)|}{n}$, je $P(|Y_n| \geq \epsilon) \stackrel{n \to \infty}{\rightarrow} 0$ \\
    oz. $P(|Y_n| < \epsilon) \stackrel{n \to \infty}{\rightarrow} 1$ \\
    Zato $Y_n \stackrel{n \to \infty}{\rightarrow} 0$ verjetnostno, torej velja "SZV"S za zaporedje $\{X_n\}_{n \in \N}$
\end{proof}

\begin{conseq}[Izrek "Cebi"sev]
    "Ce so $X_1, X_2 \cdots X_n$ paroma nekorelirane slu"cajne spremenljivke in $\sup_{n \in \N} D(X_n) < \infty$, potem
    za $\{X_n\}_{n \in \infty}$ velja "SVZ"S
\end{conseq}

\begin{proof}
    Ker je $D(S_n) = D(X_1) + \cdots + D(X_n) \leq n \cdot c$, je $\frac{D(S_n)}{n^2} \leq \frac{n \cdot c}{n^2}
    = \frac{c}{n} \stackrel{n \to \infty}{\rightarrow} 0$, zato po izreku Markova velja "SZV"S
\end{proof}

\begin{ex}
    $X_n: \begin{pmatrix}0 & 1 \\ q & p\end{pmatrix}$ neodvisne slu"cajne spremenljivke, $D(X_n) = pq, E(X_n) = p,
    E(S_n) = n \cdot p$ \\
    Po izreku "Cebi"seva velja "SZV"S: $P(\frac{|S_n - E(S_n)|}{n} \geq \epsilon) \stackrel{n \to \infty}{\rightarrow} 0$
    \[\implies P(|\frac{S_n}{n} - p| \geq \epsilon) \stackrel{n \to \infty}{\rightarrow} 0\]
    $S_n$ je frekvenca dogodka, $\frac{S_n}{n}$ je relativna frekvenca, $\frac{S_n}{n} = \frac{X_1 + \cdots + X_n}{n}
    \stackrel{n \to \infty}{\rightarrow} p$ verjetnostno \\
    To je Bernoulijev zakon velikih "stevil iz 1713
\end{ex}

\begin{theorem}[Kolmogorov]
    "Ce za neodvisne slu"cajne spremenljivke $\{X_n\}_{n \in \N}$ velja $\sum_{n=1}^{\infty} \frac{D_n}{n^2} < \infty$,
    potem velja KZV"S, t.j. $P(\lim_{n \to \infty} \frac{S-n - E(S_n)}{n} = 0) = 1$. \\
    Posebej je pogoj za vrsto izpolnjen, "ce je $\sup_n D(X_n) < \infty$ 
\end{theorem}

\begin{ex}
    $X_n: \begin{pmatrix}0 & 1 \\ q & p\end{pmatrix}$ neodvisne slu"cajne spremenljivke, $D(X_n) = pq$ \\
    Po izreku Kolmogorova velja KVZ"S, t.j. $\frac{S_n}{n} = \frac{X_1 + \cdots + X_n}{n}
    \stackrel{n \to \infty}{\rightarrow} p$ skoraj gotovo. \\
    To posplo"suje Bernoullijev zakon
\end{ex}

\subsection{Centralni limitni izrek}

\begin{defn}
    Naj bo $\{X_n\}_{n \in \N}$ zaporedje slu"cajnih spremenljivk s kon"cnimi disperzijami. Definiramo
    $S_n := X_1 + \cdots + X_n$ in standardizirajmo: $Z_n = \frac{S_n - E(S_n)}{\sigma(S_n)}$, torej
    $E(Z_n) = 0, D(Z_n) = 1$ \\
    Za $\{X_n\}_{n \in \N}$ velja centralni limitni izrek, "ce je $F_{Z_n}(x) = P(Z_n \leq x)
    \stackrel{n \to \infty}{\rightarrow} F_{N(0,1)} \forall x \in \R$, t.j.
    \[P(\frac{S_n - E(S_n)}{\sigma(S_n)} \leq x) \stackrel{n \to \infty}{\rightarrow}
    \frac{1}{2 \pi} \int_{-\infty}^x e^{-\frac{t^2}{2}} dx \text{ za } \forall x \in \R\]
    Pracimo, da $\{Z_n\}_{n \in \N}$ po porazdelitvi konvergira proti standardizirani normalni porazdelitvi.
\end{defn}

\begin{theorem}[Centralni limitni izrek (CLI, osnovna verzija)]
    Naj bodo $X_1, X_2 \cdots$ neodvisne in enako porazdeljene slu"cajne spremenljivke. Potem zanje velja centralni limitni
    zakon, t.j
    \[P(\frac{S_n - E(S_n)}{\sigma(S_n)} \leq x) \stackrel{n \to \infty}{\rightarrow}
    \int_{-\infty}^x e^{\frac{t^2}{2}} dx \text{ za } \forall x \in \R\]
\end{theorem}

Dokazal je Ljapunov (1900), s tem je posplo"sil Laplaceov izrek iz leta 1812. V dokazu bomo uporabili

\begin{theorem}[O zveznosti rodovne funkcije]
    Naj za zaporedje $\{Z_n\}_{n \in \N}$ slu"cajnih spremenljivk velja: \\
    $M_{Z_n}(t) \rightarrow M_{N(0,1)}(t) = e^{\frac{t^2}{2}}$ za vse $t \in (-\delta,\delta)$ pri nekem $\delta > 0$ \\
    Potem $F_{Z_n}(x) \rightarrow F_{N(0,1)}(x)$ za $\forall x \in \R$
\end{theorem}

\begin{proof}
    CLI v primeru, ko $X_n$ imajo momentno rodovno funkcijo \\
    $M_X(t) = E(e^{t X_n})$ na neki okolici to"cke 0 \\
    Naj bo $E(X_n) = \mu, D(X_n) = \sigma^2$ in $U_n := X_n - \mu = X_n - E(X_n)$. Torej je $E(U_n) = 0$ in
    $D(U_n) = \sigma^2$ ter $M_{U}(t) = 1 + t E(U_n) + \frac{t^2}{2!} E(U_n^2) + o(t^2) =$ \\
    $= 1 + \frac{t^2}{2} \sigma^2 + o(t^2)$ ($\lim_{n \to \infty} \frac{o(n)}{n} = 0$) \\
    Ker je $D(S_n) \stackrel{\text{neodvisne}}{=} D(X_1) + \cdots + D(X_n) = n \cdot \sigma^2$ in
    $E(S_n) = n \cdot \mu = E(X_1) + \cdots + E(X_n)$, je $Z_n = \frac{S_n - E(S_n)}{\sigma(S_n)} =$ \\
    $= \frac{1}{\sigma \sqrt{n}}$ ($\sum_{n=0}^{n} U_i$) \\
    Potem je $M_{Z_n}(t) = E(e^{t Z_n}) = E(e^{\frac{t}{\sigma \sqrt{n}}(U_1 + \cdots + U_n)}) =
    E(e^{\frac{t}{\sigma \sqrt{n}} U_1}) \cdot \cdots \cdot E(e^{\frac{t}{\sigma \sqrt{n}} U_n}) =$ \\
    $\stackrel{\text{enaki}}{=} (M_U(\frac{t}{\sigma \sqrt{n}}))^n = (1 + \frac{t^2}{2n} + o(\frac{1}{n}))^n$ \\
    $\stackrel{n \to \infty \equiv o(\frac{1}{n} \to 0)}{\rightarrow} e^{\frac{t^2}{2}}$
    \begin{lemma}
        "Ce $X_n \to X$, potem $(1 + \frac{X_n}{n})^n \stackrel{n \to \infty}{\rightarrow} e^x$
    \end{lemma}
    Po prej"snjem izreku: $F_{Z_n}(x) \stackrel{n \to \infty}{\rightarrow} F_{N(0,1)}(x)$



% 19. predavanje: 14.3.

    \begin{equation*}
        \epsilon > 0: x - \epsilon \leq x_n \leq x + \epsilon \text{ za dovolj velik n}
    \end{equation*}

    \begin{align*}
        &\implies (1 + \frac{x-\epsilon}{n})^n \leq (1 + \frac{x_n}{n})^n \leq (1 + \frac{x+\epsilon}{n})^n \\
        &\implies (1 + \frac{x-\epsilon}{n})^n \to e^{x-\epsilon} \\
        &\implies (1 + \frac{x_n}{n})^n \to e^{x} \\
        &\implies (1 + \frac{x+\epsilon}{n})^n \to e^{x+\epsilon} \\
    \end{align*}
\end{proof}

V splo"snem se CLI doka"ze s pomo"cjo karakteristi"cnih funkcij: \\
naj bo X slu"cajna spremenljivka, $\ell_X(t) := E(e^{itX}) = E(cos(tX)) + iE(sin(tX)) t \in \R$ \\
za razliko od momentno rodovnih funkcij karakteristi"cne funkcije vedno odstajajo \\
v zveznem primeru je $\int_{-\infty}^{\infty} e^{itx}p(x)dx$ - Fourierova transformacija funkcije $p_X(x)$ \\
$X_1, X_2 \cdots X_n$ neodvisne, enako porazdeljene

\begin{align*}
    &\mu := E(X_n), \sigma := \sigma(X_n) \\
    &E(S_n) \stackrel{\text{neodvisnost}}{=} E(X_1) + \cdots + E(X_n) = n \mu \\
    &D(S_n) \stackrel{\text{neodvisnost}}{=} D(X_1) + \cdots + D(X_n) = n \sigma^2
\end{align*}

$X_1, X_2 \cdots X_n$ neodvisne slu"cajne spremenljivke

\begin{align*}
    &Z_n = \frac{S_n - E(S_n)}{\sigma(S_n)} = \frac{S_n - n \mu}{\sqrt{n} \sigma} =
        \frac{\frac{S_n}{n} - \mu}{\frac{\sigma}{\sqrt{n}}} \\
    &\overline{Z_n} := \frac{S_n}{n} = \frac{X_1 + \cdots + X_n}{n} \implies
        Z_n = \frac{\overline{X} - \mu}{\frac{\sigma}{\sqrt{n}}}
\end{align*}

Po CLI za velike n velja $Z_n \approx N(0,1)$, zato je $\overline{X} \approx N(\mu, \frac{\sigma}{\sqrt{n}})$ oz.
$S_n \approx N(n \mu, \sigma \sqrt{n})$ \\
"Ce so $X_1, X_2 \cdots$ porazdeljene normalno $N(\mu, \sigma)$, potem je $Z_n \sim N(0,1)$, torej
$F_{Z_n}(x) = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{x} e^{-\frac{t^2}{2}} dt$ \\

\begin{ex}
    Laplaceova formula je poseben primer CLI: \\
    $X_n: \begin{pmatrix}0 & 1 \\ q & p\end{pmatrix}, X_n = 1$ je dogodek, da se dogodek A (s $P(A) = p$) zgodi
    v n-ti ponovitvi poskusa, sicer je $X_n = 0$ \\
    $E(X_n) = p, S_n = X_1 + \cdots + X_n$ frekvenca dogodka A v prvih n ponovitvah \\
    $S_n \sim Bin(n,p), E(S_n) = np, D(S_n) = npq$, ker je $D(X_1) = pq$ \\
    $Z_n = \frac{S_n - E(S_n)}{\sigma(S_n)} = \frac{S_n - np}{\sqrt{npq}} \stackrel{\text{CLI}}{\approx} N(0,1)$,
    "ce je n velik \\
    \begin{align*}
        &P(S_n \leq X) = P(\frac{S_n - np}{\sqrt{npq}} \leq \frac{X - np}{\sqrt{npq}}) \approx \\
        &\approx \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\frac{x-np}{\sqrt{npq}}} e^{-\frac{t^2}{2}} dt = \\
        &= \frac{1}{2} + \Phi(\frac{x-np}{\sqrt{npq}})
    \end{align*}
    kjer je
    \begin{equation*}
        \Phi(x) := \frac{1}{\sqrt{2\pi}} \int_0^x e^{-\frac{t^2}{2}} dt
    \end{equation*}
    verjetnostni integral \\
    \begin{align*}
        &P(\alpha < S_n \leq \beta) = \\
        &= P(S_n \leq \beta) - P(S_n \leq \alpha) \approx \\
        &\approx \frac{1}{2} + \Phi(\frac{\beta - np}{\sqrt{npq}}) - \frac{1}{2} - \Phi(\frac{\alpha - no}{\sqrt{npq}}) = \\
        &= \Phi(\frac{\beta - np}{\sqrt{npq}}) - \Phi(\frac{\alpha - np}{\sqrt{npq}})
    \end{align*}
    Laplaceova aproksimacijska formula
\end{ex}

\begin{ex}
    Te"za vre"cke kostanja je porazdeljena pribli"zno normalno, saj je vsota te"z posameznih kostanjev, ki so neodvisne,
    enako porazdeljene slu"cajne spremenljivke \\
    $X_n \cdots$ te"za n-tega kostanja, $S_n = X_1 + \cdots + X_n \approx$ normalno - aditiven efekt
\end{ex}

\begin{ex}
    \begin{align*}
        &p_{X_n}(x) = \begin{cases}
            \frac{1}{2}; x \in [-1,1] \\
            0 \text{ sicer}
        \end{cases} \\
        &E(X_1) = 0, D(X_1) = \frac{(b-a)^2}{12} = \frac{1}{3} \\
        &S_1 = X_1, Z_1 = \frac{X_1 - E(X_1)}{\sigma(X_1)} = \frac{X_1}{\sqrt{\frac{1}{3}}} = x_1 \sqrt{3} \\
        &S_2 = X_1 + X_2 , Z_2 = \frac{S_2 - E(S_2)}{\sigma(S_2)} =
            \frac{X_1 + X_2 - E(X_1 + X_2)}{\sigma(X_1 + X_2)} \\
        &S_3 = X_1 + X_2 + X_3, Z_3 = \frac{S_3 - E(S_3)}{\sigma(S_3)}
    \end{align*}
\end{ex}

\section{Statistika}

\subsection{Osnovni pojmi}

Kot vedo statistiko razdelimo na:
\begin{enumerate}
    \item opisno statistiko: zbiranje, razvr"s"canje, prikazovanje podatkov, ra"cunanje osnovnih koli"cin
    \item analiti"cno statistiko: upraba podatkov pri sklepanju glede zakonitosti danega podro"cja
\end{enumerate}

\begin{defn}[Populacija]
    Populacija je kon"cna ali neskon"cna mno"zica elementov, pri katerih merimo ali opazujemo neko koli"cino
\end{defn}

\begin{ex} \text{} \\
    \begin{enumerate}[label=(\alph*)]
        \item kontrole kvalitete: populacija je mno"zica (serija) izdelka, npr. dnevna proizvodnja, merimo
            lastnosti izdelkov, npr. "zivljensko dobo
        \item testiranje seb: populacija je mno"zica vseh zaposlenih v dr"zavi, merimo npr. starost,
            vi"sino place $\cdots$
    \end{enumerate}
\end{ex}

Matemati"cni pogled: na verjetnostnem prostoru $(\Omega, \mathcal{F})$ imamo slu"cajno spremenljivko X. \\
Praviloma ne moremo izmeriti cele populacije, ampak meritve opravimo na relativno majhnem delu populacije,
na vzorcu. Le-ta mora biti reprezentativen, izbran nepristransko in dovolj velik. \\
Matemati"cni pogled: vzorec velikosti n je slu"cajni vektor $(x_1 \cdots x_n)$, kjer so komponente enako porazdeljene
kot slu"cajna spremenljivka X in med seboj neodvisne. \\
Vrednost tega slu"cajnega vektorja pri enem naboru n meritev je realizacija vzorca: $(x_1 \cdots x_n)$: to so
konkretni podatki, ki jih analiziramo. Pri opisni statistiki predstavimo in obdelamo te podatke. \\
Iz teh vzor"cnih podatkov "zelimo oceniti nekatere lastnosti populacije, kot sta:

\begin{enumerate}
    \item sredina populacije $\mu$, t.i. matemati"cno upanje slu"cajne spremenljivke X
    \item povpre"cni odklon $\sigma$ od sredine populacije, t.i. Standardna deviacija slu"cajne spremenljivke X
\end{enumerate}



% 20. predavanje: 21.3.

Ocene za $\mu$ so:

\begin{itemize}
    \item vzor"cno povpre"cje: $\overline{x} = \frac{x_1 + \cdots + x_n}{n}$
    \item vzor"cni modus: najpogostej"sa vrednost v vzorcu
    \item vzor"cna mediana: srednja vrednost v vzorcu, urejenem po velikosti
\end{itemize}

Ocene za $\sigma$ so:

\begin{itemize}
    \item vzor"cni razmak: razlika med najve"cjo in najmanj"so vrednostjo v vzorcu
    \item vzor"cna disperzija: $s_0^2 ? \frac{1}{n} \sum_{i=1}^n (x_i - \overline{x})^2$
    \item popravljena vzor"cna disperzija: $s^2 ? \frac{1}{n-1} \sum_{i=1}^n (x_i - \overline{x})^2 =
        \frac{n}{n-1} s_0^2$
\end{itemize}

\subsection{Vzor"cne statistike in cenilke}

\begin{defn}[Vzor"cna statistika]
    Naj bo $(X_1, X_2 \cdots X_n)$ vzorec t.i. slu"cajni vektor, kjer so $X_1 \cdots X_n$ enako porazdeljene
    kot slucajna spremenljivka $X$ in med seboj neodvisne. \\
    Vzor"cna statistika je simetri"cna funkcija vzorca $y = g(X_1, X_2 \cdots X_n)$, kjer je $g$ simetricna
    funkcije n spremenljivk
\end{defn}

Praviloma vzor"cna statistika ocenjuje vrednost nekega parametra $\xi$. Tedaj je y cenilka za parameter. \\
y je odvisna od n, zato pi"semo tudi $y_n = g(X_1 \cdots X_n)$. \\

\begin{defn}[Nepristranskost, doslednost]
    "Ce je $E(Y) = \xi$, je $Y$ nepristranska cenilka za parameter $xi$ \\
    Cenilka $Y=Y_n$ je dosledna, "ce $Y_n \stackrel{n \to \infty}{\to} \xi$
    verjetnostno, t.i. $\forall \epsilon > 0$ je $\lim_{n \to \infty} P(|Y_n - \xi| \geq \epsilon) = 0$ oz.
    $\lim_{n \to \infty} P(|Y_n - \xi| < \epsilon) = 1$
\end{defn}

\begin{defn}[Standardna napaka]
    Standardna napaka vzor"cne statistike $Y$ je standardna deviacija slu"cajne spremenljivke $Y$:
    $SE(Y) := \sigma(Y)$
\end{defn}

\begin{defn}[Vzor"cno povprecje]
    Naj bo $X$ slu"cajna spremenljivka na populaciji, ki ima matemati"cno upanje $E(X) = \mu$ in standardno
    deviacijo $\sigma(X) = \sigma$. Naj bo $(X_1 \cdots X_n)$ vzorec. Definirajmo vzor"cno povprecje
    \begin{equation*}
        \overline{X} = \frac{X_1 + \cdots + X_n}{n}
    \end{equation*}
    ki je vzor"cna statistika. \\
\end{defn}

Je cenilka za $\overline{X}$, ki je nepristranska:

\begin{equation*}
    E(\overline{X}) = \frac{1}{n} (E(X_1) + \cdots + E(X_n)) = \frac{1}{n} n \cdot \mu = \mu
\end{equation*}

Po "SZV"S (izreku "Cebi"seva) je to dosledna cenilka za $\mu$. \\
Ker je
\begin{equation*}
    D(\overline{X}) \stackrel{\text{neodv}}{=} \frac{1}{n^2} \sum_{i=1}^n D(X_i) =
    \frac{1}{n^2} n \cdot \sigma^2 = \frac{\sigma^2}{n}
\end{equation*}

je standardna napaka

\begin{equation*}
    SE(Y) = \frac{\sigma}{\sqrt{n}}
\end{equation*}

- "cim vecji n, bolje oceni parameter $\mu$ \\
Po CLI je pri velikem n slu"cajna spremenljivka $Z_n := \frac{S - n \mu}{\sigma \sqrt{n}} =
\frac{\overline{X} - \mu}{\frac{\sigma}{\sqrt{n}}} = \frac{\overline{X} - \mu}{\sigma} \sqrt{n}$
porazdeljena pribli"zno $N(0,1)$ oz. $\overline{X}$ je porazdeljen pribli"zno $N(\mu, \frac{\sigma}{\sqrt{n}})$ \\
"Ce je $X$ normalno porazdeljena $N(\mu, \sigma)$, potem je $\overline{X}$ porazdeljen
$N(\mu, \frac{\sigma}{\sqrt{n}})$ za vsak $n$

\begin{claim}
    Naj bo $Y_n$ cenilka za $\xi$. "Ce je $E(Y_n) \stackrel{n \to \infty}{\to} \xi$ in
    $D(Y_n) \stackrel{n \to \infty}{\to} 0$, potem je $Y=Y_n$ dosledna cenilka za $\xi$
\end{claim}

\begin{proof}
    Fiksirajmo $\epsilon > 0$. Dokazati moramo $\lim_{n \to \infty} P(|Y_n - \xi| \geq \epsilon) = 0$ \\
    Ker je $E(Y_n) \stackrel{n \to \infty}{\xi}$, obstaja $n_0 \in \N$: $|E(Y_n) - \xi| < \frac{\epsilon}{2}$
    zato je dogodek
    \begin{align*}
        &(|Y_n - \xi| \geq \epsilon) \subseteq (|Y_n - E(Y_n)| + |E(Y_n) - \xi| \geq \epsilon) \text{ za }
            \forall n \subseteq\\
        &\stackrel{n \geq n_0}{\subseteq} (|Y_{n_0} - E(Y_{n_0})| + |E(Y_{n_0}) - \xi| \geq \epsilon)
    \end{align*}
    Torej je za $n \geq n_0$
    \begin{equation*}
        P(|Y_n - \xi| \geq \epsilon) \leq P(|Y_n - E(Y_n)| \geq  \frac{\epsilon}{2})
        \leq \frac{D(Y_n)}{\epsilon^2} \cdot 4 \stackrel{n \to \infty}{\to} 0 \text{ (doslednost)}
    \end{equation*}
    Neenakost "Cebi"seva: $P(|X - E(X)| \geq \epsilon) \leq \frac{D(X)}{\epsilon^2}$ \\
    Tako imamo doslednost cenilke: $P(|Y_n - \xi| \geq \epsilon) \stackrel{n \to \infty}{\to} 0$
\end{proof}

\begin{ex}
    Porazdelitev $\chi^2$, n "stevilo prostorskih stopenj \\
    \begin{equation*}
        p(X) = \begin{cases}
            \frac{1}{2^{\frac{n}{2}} \gamma(\frac{n}{2})} x^{\frac{n}{2}-1} e^{-\frac{x}{2}} x > 0 \\
            0 \text{ sicer}
        \end{cases}
    \end{equation*}
    Modus = $n-2$, $E(X) = n$, $D(X) = 2n$ \\
    Mediana $\approx n \cdot (1 - \frac{2}{9n})^3$
\end{ex}

\begin{defn}[Vzorcna disperzija]
    Naj bo X slu"cajna spremenljivka na populaciji. Vzor"cna disperzija je definirana s
    \begin{equation*}
        s_0^2 = \frac{1}{n} \sum_{i=1}^n (x_i - \overline{x})^2
    \end{equation*}
    popravljena vzor"cna disperzija pa je
    \begin{equation*}
        s^2 = \frac{1}{n-1} \sum_{i=1}^{n} (x_i - \overline{x})^2
    \end{equation*}
\end{defn}

Kako sta porazdeljeni, "ce je $X \sim N(\mu, \sigma)$? \\
Raje vzemimo vzor"cno statistiko: $\chi^2 := \frac{1}{\sigma^2} \sum_{i=1}^n (x_i - \overline{x})^2 =
\frac{n}{\sigma^2} s_0^2 = \frac{n-1}{\sigma^2} s^2$ \\
Ni lahko izra"cunati, da je $\chi^2 \sim \chi^2(n-1)$ \\
Ideja izpeljave je $\chi^2 = Z_1^2 + \cdots + Z_{n-1}^2$ za $Z_i \sim N(0,1)$ in med seboj neodvisne.
Potem uporabimo trditev iz verjetnosti: $Z_i^2 \sim \chi^2(1)$, torej $E(\chi^2) = n-1$, $D(\chi^2) = 2(n-1)$.
Od tod sledi
\begin{equation*}
    E(s_0^2) = E(\frac{\sigma^2}{n} \chi^2) = \frac{\sigma^2}{n} E(\chi^2) = \frac{n-1}{n} \sigma^2
\end{equation*}
torej $s_0^2$ ni nepristranska za $\sigma^2$, je pa asimptoti"cno nepristranska, t.i.
$E(s_0^2) \stackrel{n \to \infty}{\to} \sigma^2$ \\
Podobno je $E(s^2) = \frac{\sigma^2}{n-1} E(\chi^2) = \sigma^2$, torej je $s^2$ nepristranska cenilka za
$\sigma^2$ \\
Ker je $D(s_0^2) = \frac{\sigma^4}{n^2} D(\chi^2) = \frac{\sigma^4 2(n-1)}{n^4} \stackrel{n \to \infty}{\to} 0$
in $D(s^2) = \frac{2 \sigma^4}{(n-1)^2} \stackrel{n \to \infty}{\to} 0$, iz trditve sledi, da sta $s_0^2$ in
$s^2$ dosledni cenilki za $\sigma^2$



% 21. predavanje: 28.3.

\subsubsection*{Studentova t-porazdelitev}

\begin{equation*}
    p(x) = \frac{1}{\sqrt{n} B(\frac{n}{2},\frac{1}{2})} (1 + \frac{x^2}{n})^{-\frac{n+1}{2}}
\end{equation*}
kjer je $B(p,q) = \frac{\Gamma(p) \Gamma(q)}{\Gamma(p+q)}$ Beta funkcija

\begin{align*}
    n = 1: &\quad \frac{1}{\pi} (1+x^2)^{-1} = \frac{1}{\pi (1+x^2)} \text{Cauchyjeva porazdelitev} \\
    &\text{ko gre } n \to \infty, \text{ gre } \sqrt{n} B(\frac{n}{2},\frac{1}{n}) \to \sqrt{2 \pi}
        \text{ in } (1 + \frac{x^2}{n})^{-\frac{n-1}{2}} = ((1 + \frac{x^2}{n})^n)^{-\frac{n+1}{2n}}
        \to e^{-\frac{x^2}{2}} \\
    &\text{torej je pri velikih n gostota pribli"zno } N(0,1) \\
    n = 2: &\quad \frac{1}{\sqrt{2} B(1,\frac{1}{2})} (1 + \frac{x^2}{2})^{-\frac{3}{2}} \\
    &\text{za } n \geq 2 \text{ je } E(X) = 0 \\
    n = 3: &\quad c \cdot (1 + \frac{x^2}{2})^{-2} \approx \frac{1}{x^4} \text{ za velike } x \\
    &\text{za } n \geq 3 \text{ je } D(X) = \frac{n}{n-2} > 1
\end{align*}

Leta 1908 jo je odkril W.S. Gosset, statistik v pivovarni guiness v Dublinu. Student je njegov prevdonim.

\subsubsection*{}

Pri normalni porazdelitvi slu"cajne spremenljivke $X \sim N(\mu, \sigma)$ je vzor"cno povpre"cje  $\overline{X}$
porazdeljeno $N(\mu, \frac{\sigma}{\sqrt{n}}), \overline{X} = \frac{X_1 + \cdots + X_n}{n}$, torej je
$Z := \frac{\overline{X} - \mu}{\frac{\sigma}{\sqrt{n}}} = \frac{\overline{X} - \mu}{\sigma} \sqrt{n}$
porazdeljena N(0,1). "Ce poznamo $\sigma$, potem bomo znali povedati, kako dobra ocena za $\mu$ je $\overline{X}$
($\to$ intervali zaupanja). \\

Kako ravnati, "ce $\sigma$ ne poznamo? \\
Lahko jo ocenimo s $s = \sqrt{\frac{1}{n-1} \sum_{i=1}^n (X_i - \overline{X})^2}$, tako da potem vzor"cna
statistika $T = \frac{\overline{X} - \mu}{s} \sqrt{n}$ ni ve"c porazdeljena po $N(0,1)$, niti pribli"zno
normalna, razen "ce je n velik in je s potem skoraj konstanta $\sigma$. \\

Kako je porazdeljena vzor"cna statistika T? \\
Ker je $\chi^2 = \frac{1}{\sigma^2} \sum_{i=1}^n (X_i - \overline{X})^2 = \frac{(n-1) S^2}{\sigma^2}$, je
$\frac{Z}{T} = \frac{S}{\sigma} = \sqrt{\frac{\chi^2}{n-1}}$, torej je $T = \frac{Z}{\sqrt{\frac{\chi^2}{n-1}}}$ \\
Izka"ze se, da sta $Z \sim N(0,1)$ in $\chi^2 \sim \chi^2(n-1)$ neodvisni slu"cajni spremenljivki. Od tod lahko
izra"cunamo, da ima $T$ Studentovo porazdelitev z $n-1$ prostorskimi stopnjami:
\begin{equation*}
    p_T(t) = \frac{1}{(n-1) B(\frac{n-1}{2}, \frac{1}{2})} \cdot \frac{1}{(1 + \frac{x^2}{n-1})^{\frac{n}{2}}}
\end{equation*}

\subsection{Metode za pridobivanje cenilk}

\subsubsection{Metoda momentov}

\begin{defn}[Vzro"cni moment]
    Naj bo $(X_1, X_2 \cdots X_n)$ vzorec velikosti n, torej $X_1 \cdots X_n$ neodvisne slu"cajne spremenljivke,
    porazdeljene kot slu"cajna spremenljivka $X$. Zacetni moment reda $k$ je $z_k = E(X^k)$. Definiramo k-ti
    vzro"cni moment $z_k := \frac{X_1^k + \cdots + X_n^k}{n}$. Le ta je nepristranska cenilka za $z_k:
    E(Z_k) = \frac{1}{n} (E(X_1^k) + \cdots + E(X_n^k)) = z_k$. $Z_k$ je tudi dosledna cenilka za $z_k$.
\end{defn}

Naj bo gostota slu"cajne spremenljivke $X$ odvisna od parametrov $\xi_1 \cdots \xi_n: p(X; \xi_1 \cdots \xi_m)$.
Naj odstajajo za"cetni momenti $z_k = E(X^k) = \int_{-\infty}^{\infty} p(x; \xi_1 \cdots \xi_n) dx, k = 1, 2 \cdots m$.
Denimo, da iz teh m ena"cb lahko izrazimo parametre: $\xi_k = \phi_k(z_1, z_2 \cdots z_m), k = 1 \cdots m$ za neko
funkcijo $\phi_k$. Potem je $c_k := \phi_k(z_1 \cdots z_m)$ cenilka za parameter $\xi_k, k = 1 \cdots n$

\begin{ex}
    Naj bo $X \sim N(\mu, \sigma)$, kjer sta $\mu$ in $\sigma$ neznana parametra. Potem je $z_1 = E(X) = \mu,
    z_2 = E(X^2) = E(X^2) - (E(X))^2 + (E(X))^2 = D(X) + (E(X))^2 = \sigma^2 + \mu^2$ (m = 2) \\
    Iz teh dveh ena"cb izrazimo parametra $\mu$ in $\sigma$: $\mu = z_1, \sigma^2 = z_2 - \mu^2 = z_2 - z_1^2$. \\
    Cenilka za $\mu$ je $Z_1 = \overline{X} = \frac{X_1 + \cdots + X_n}{n}$, cenilka za $\sigma^2$ je
    $Z_2 - Z_1^2 = \frac{X_1^2 + \cdots + X_n^2}{n} - \overline{X}^2$. To je enako
    \begin{align*}
        &S_0^2 = \frac{1}{n} \sum_{i=1}^n (X_i - \overline{X})^2 = \\
        &= \frac{1}{n} \sum_{i=1}^n (X_i^2 - 2 X_i \overline{X} + \overline{X}^2) = \\
        &= \frac{1}{n} \sum_{i=1}^n X_i^2 - 2 \overline{X} \overline{X} + \overline{X}^2 = \\
        &= \frac{1}{n} \sum_{i=1}^2 X_i^2 - \overline{X}^2
    \end{align*}
    Torej bodimo "ze znani cenilki za parametra $\mu$ in $\sigma^2$
\end{ex}

\begin{ex}
    Naj bo $X$ porazdeljena enakomerno na $[a,b]$, kjer sta $a$ in $b$ neznana parametra. I"s"cemo cenilki za $a$
    in $b$. Po metodi momentov moramo izra"cunati 2 za"cetna momenta
    \begin{align*}
        &z_1 = E(X) = \frac{a+b}{2} \\
        &z_2 = E(X^2) = \int_{-\infty}^{\infty} x^2 p(x; a, b) dx = \frac{1}{b-a} \int_{a}^{b} x^2 dx = \\
        &= \frac{1}{b-a} \frac{x^3}{3} \vert_a^b = \frac{b^3-a^3}{3(b-a)} = \frac{b^2 + ab + a^2}{3}
    \end{align*}
    Iz 1. ena"cbe dobimo $b = 2z_1 - a$, kar vstavimo v 2. ena"cbo
    \begin{align*}
        &3z_2^2 = b^2 + ab + a^2 = 4z_1^2 - 4z_1 a + a^2 + 2a z_1 - a^2 + a^2 \\
        &\implies 3z_2 = 4z_1^2 - 2z_1 a + a^2 \\
        &a^2 - 2a z_1 + (4z_1^2 - 3z_2) = 0 \\
        &\quad D = 4z_1^2 - 4(4z_1^2 - 3z_2) = 12(z_2 - z_1^2) \\
        &a_{1,2} = \frac{1}{2} (2z_1 \pm \sqrt{D}) = z_1 \pm \frac{1}{2} 2\sqrt{3} \sqrt{z_2 - z_1^2} =
            z_1 \pm \sqrt{3} \sqrt{z_2 - z_1^2}
    \end{align*}
    Ker je $a < b$, je torej
    \begin{align*}
        &a = z_1 - \frac{1}{2} 2\sqrt{3} \sqrt{z_2 - z_1^2} \\
        &b = z_1 + \frac{1}{2} 2\sqrt{3} \sqrt{z_2 - z_1^2}
    \end{align*}
    Cenilka za a je
    \begin{align*}
        &A := Z_1 \pm \frac{1}{2} 2\sqrt{3} \sqrt{Z_2 - Z_1^2}
        &A := Z_1 \pm \frac{1}{2} 2\sqrt{3} \sqrt{Z_2 - Z_1^2} = Z_1 - S_0 \sqrt{3} \text{ po prej"snjem primeru}
            = \overline{X} - S_0 \sqrt{3}
    \end{align*}
    Cenilka za b je $B = \overline{X} + S_0 \sqrt{3}$
    Denimo da imamo konkreten vzorec $-2, 0, 1, 2, 4 (n=5)$ \\
    $\overline{X} = \frac{-2 + 0 + 1 + 2 + 4}{5} = 1$ \\
    $S_0^2 = \frac{1}{n} \sum_{i=1}^n (X_i - \overline{X})^2 = \frac{1}{5} ((-3)^2 + (-1)^2 + 0^2 + 1^2 + 3^2) = 4$
    Vzor"cna vrednost za $A$ je $\overline{X} - S_0 \sqrt{3} = 1 - 2\sqrt{3} = \doteq -2.46$, vzor"cna
    vrednost z $B$ je $\overline{X} + S_0 \sqrt{3} = 1 + 2\sqrt{3} \doteq 4.46$
\end{ex}



% 22. predavanje: 4.4.

\subsubsection{Metoda maksimalne zanesljivosti (oz. najve"cjega verjetja)}

\begin{defn}[Funkcija zanesljivosti]
    Naj bo gostota slu"cajne spremenljivke $X$ odvisna od parametra $\xi$, torej $p(x; \xi)$. Funkcija
    zanesljivosti (likelihood function) je
    \begin{equation*}
        L(x_1 \cdots x_n; \xi) = p(x_1; \xi) \cdot \cdots \cdot p(x_n; \xi)
    \end{equation*}
\end{defn}

Pri danih $x_1 \cdots x_n$ izberimo tak $\xi_{max}$, da ima $L$ tam maksimum. Ta vrednost parametra je odvisna od
$x_1 \cdots x_n$, torej $\xi_{max} = \phi(x_1, x_2 \cdots x_n)$ za neko funkcijo $\phi$. Tako dobimo cenilko
$c := \phi(x_1 \cdots x_n)$ za parameter $\xi$

\begin{ex}
    \begin{equation*}
        p(x; \lambda) := \begin{cases}
            \lambda e^{-\lambda x} \; x > 0 \\
            0 \qquad x < 0
        \end{cases}
    \end{equation*}
    $\lambda$ je neznan parameter, ki ga ocenjujemo
    \begin{equation*}
        L(x_1 \cdots x_n; \lambda) = \lambda e^{-\lambda x_1} \cdot \cdots \cdot \lambda e^{-\lambda x_n} =
        \lambda^n e^{-(x_1 + \cdots + x_n)}
    \end{equation*}
    Poiskati moramo $\lambda_{max}$, pri katerem je dose"zen maksimum funkcije $L$ (oz. maksimum funkcije $\ln(L)$)
    \begin{align*}
        &\ln L(x_1 \cdots x_n; \lambda) = n \cdot \ln \lambda - \lambda \sum_{i=1}^{n} x_i \\
        &\frac{\partial}{\partial \lambda} (\ln L(x_1 \cdots x_n; \lambda)) = \frac{n}{\lambda} -
            \sum_{i=1}^{n} x_i = 0 \\
        &\quad \implies \lambda_{max} = \frac{n}{\sum_{i=1}^n x_i} = \frac{1}{\overline{x}}
    \end{align*}
    Ker je $\frac{\partial^2}{\partial \lambda^2} \ln L(x_1 \cdots x_n; \lambda) = -\frac{n}{\lambda^2} < 0$,
    je v $\lambda_{max}$ maksimum. \\
    Cenilka za $\lambda$ je $c := \frac{1}{\overline{X}}$ \\
    Isto cenilko dobimo z metodo momentov:
    \begin{align*}
        &z_1 = E(X) = \frac{0}{\infty} x \lambda e^{-\lambda x} dx = \stackrel{\text{D.N.}}{\cdots} \frac{1}{\lambda} &
        &\implies \lambda = \frac{1}{z_1} = \frac{1}{\overline{x}}
    \end{align*}
    cenilka za $\lambda$ je $c := \frac{1}{\overline{X}}$
\end{ex}

\begin{ex}
    $X \sim N(\mu, \sigma)$, $\mu, \sigma$ neznana parametra, ki ju ocenjujemo
    \begin{align*}
        &L(x_1 \cdots x_n; \mu, \sigma) := \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2} (\frac{x_1-\mu}{\sigma})^2} \cdot
            \cdots \cdot \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2} (\frac{x_n-\mu}{\sigma})^2} = \\
        &= \frac{1}{(2\pi)^{\frac{n}{2}}} \cdot \frac{1}{\sigma^n} e^{-\frac{1}{2 \sigma^2}
            (x_1 - \mu)^2 + \cdots + (x_n - \mu)^2} \\
        &\ln L = -\frac{n}{2} \ln 2\pi - n \cdot \ln \sigma - \frac{1}{2 \sigma^2} ((x_1 - \mu)^2 + \cdots +
            (x_n - \mu)^2) \\
        &\frac{\partial}{\partial \mu} \ln L = -\frac{1}{2 \sigma^2} (2(x_1 - \mu)(-1) + \cdots +
            2(x_1 - \mu)(-1)) = \frac{1}{\sigma^2} (x_1 - \mu + \cdots + x_n - \mu) = 0 \\
        &x_1 + \cdots + x_n - n\mu = 0 \implies \mu = \frac{x_1 + \cdots + x_n}{n} = \overline{x} \\
        &\frac{\partial}{\partial \sigma} \ln L = -\frac{n}{\sigma} + \frac{1}{\sigma^3}
            ((x_1 - \mu)^2 + \cdots + (x_n - \mu)^2) = 0 \\
        &\implies \sigma^2 = \frac{1}{n} ((x_1 - \mu)^2 + \cdots + (x_n - \mu)^2) = \\
        &= \frac{1}{n} ((x_1 - \overline{x})^2 + \cdots + (x_n - \overline{x})^2) = s_0^2
    \end{align*}
    Cenilka za $\mu$ je $\overline{X}$, cenilka za $\sigma^2$ je $S_0^2 =
    \frac{1}{n} \sum_{i=1}^{n} (X_i - \overline{X})^2$
\end{ex}

\begin{ex}
    $Bin(1,p) = Ber(p), X: \begin{pmatrix}0 & 1 \\ q & p\end{pmatrix} q = 1-p, p$ neznan parameter
    \begin{align*}
        &P(X=x) = p^x (1-p)^{1-x} x \in \{0, 1\} \\
        &L(x_1 \cdots x_n; p) = p^{x_1} (1-p)^{1-x_1} \cdot \cdots \cdot p^{x_n} (1-p)^{1-x_n} = \\
        &= p^{x_1 + \cdots + x_n} (1-p)^{n - (x_1 + \cdots + x_n)} \\
        &x := x_1 + \cdots + x_n \implies L(x_1 \cdots x_n; p) = p^x (1-p)^{1-x} x \in \{0, 1 \cdots n\} \\
        &\ln L = x \ln p + (n-x) \ln(1-p) \\
        &\frac{\partial}{\partial p} \ln L = \frac{x}{p} - \frac{n-x}{1-p} = 0 \\
        &\implies x(1-p) = (n-x)p \implies x - xp = np - xp \implies p = \frac{x}{n} = \overline{x}
    \end{align*}
    Cenilka za $p$ je $P := \overline{X} = \frac{X_1 + \cdots + X_n}{n}$ \\
    Ker je
    \begin{equation*}
        E(P) = \frac{1}{n} (E(X_1) + \cdots + E(X_n)) = p
    \end{equation*}
    je $P$ nepristranska cenilka \\
    Ker je
    \begin{equation*}
        D(P) = \frac{1}{n^2} (D(X_1 + \cdots + D(X_n))) = \frac{1}{n^2} n D(X_1) =
        \frac{1}{n} D(X_1) \stackrel{n \to \infty}{\to} 0
    \end{equation*}
    po trditvi sledi, da je $\overline{X}$ dosledna cenilka za $P$
\end{ex}

\subsection{Intervalsko ocenjevanje parametrov}

\begin{defn}[Interval zaupanja]
    Naj bo gostota slu"cajne spremenljivke $X$ odvisna od parametra $\xi$. Interval $[A,B]$ (odvisen le od
    $(x_1 \cdots x_n)$ in ne do $\xi$) je interval zaupanja za parameter $\xi$, pri stopnji tveganja
    $\alpha \in (0,1)$, "ce je
    \begin{equation*}
        P(\xi \in [A,B]) = 1-\alpha \text{ oz. } P(\xi \notin [A,B]) = \alpha
    \end{equation*}
\end{defn}

Za $\alpha$ obi"cajno vzamemo vrednost $0.05$ (ali $0.01$) \\
$A$ in $B$ sta vzor"cni statistiki, $1 - \alpha$ je stopnja zaupanja

\begin{ex}
    $X \sim N(\mu, \sigma), \sigma$ poznamo, $\mu$ pa je neznan parameter. \\
    Slu"cajna spremenljivka $Z := \frac{\overline{X} - \mu}{\sigma} \sqrt{n} \sim N(0,1)$ \\
    Pri dani stopnji tveganja $\alpha$ najdemo $z_{\frac{\alpha}{2}} > 0$, da je
    $P(-z_{\frac{\alpha}{2}} < Z < z_{\frac{\alpha}{2}}) = 1-\alpha$ oz.
    $P(|Z| > z_{\frac{\alpha}{2}}) = \alpha$ oz. $P(Z > z_{\frac{\alpha}{2}}) = \frac{\alpha}{2}$ \\
    Pogoj $|Z| < z_{\frac{\alpha}{2}}$ pomeni: $|\overline{X} - \mu| < z_{\frac{\alpha}{2}} \cdot
    \frac{\sigma}{\sqrt{n}}$ \\
    \begin{align*}
        &A := \overline{X} - Z_{\frac{\alpha}{2}} \frac{\sigma}{\sqrt{n}} < \mu <\\
        &< \overline{X} + Z_{\frac{\alpha}{2}} \frac{\sigma}{\sqrt{n}} =: B
    \end{align*}
    $[A,B]$ je interval zaupranja za $\mu$ pri stopnji tveganja $\alpha$ \\
    Konkreten zgled: imejmo vzorec velikosti $n=36$, za katerega izra"cunamo $\overline{x} = 2.6$ in
    $s = \sqrt{\frac{1}{n-1} \sum_{i=1}^{n} (x_i - \overline{x})^2} = 0.3$. Predpostavimo, da imamo
    $X \sim N(\mu, \sigma)$ in predpostavimo, da je $\sigma := s = 0.3$ (kar pogosto naredimo, "ce je
    n razmeroma velik). Vzemimo $\alpha = 0.05$. Iz tabele razberemo $z_{\frac{\alpha}{2}} = 1.96$, torej
    $P(Z > z_{\frac{\alpha}{2}}) = \frac{\alpha}{2}$. Tedaj je vzor"cna vrednost za $A$ enaka
    \begin{equation*}
        \overline{x} - z_{\frac{\alpha}{2}} \frac{\sigma}{\sqrt{n}} = 2.6 - 1.96 \frac{0.3}{\sqrt{36}} = 2.5
    \end{equation*}
    vzor"cna vrednost za $B$ je $\overline{x} - z_{\frac{\alpha}{2}} \frac{\sigma}{\sqrt{n}} = 2.7$ \\
    Interval zaupanja za $\mu$ je $[2.5, 2.7]$, t.j.
    \begin{equation*}
        P(\mu \in [2.5, 2.7]) = 1 - \alpha = 0.95
    \end{equation*}
\end{ex}

\begin{ex}
    $X \sim N(\mu, \sigma)$, $\mu$ in $\sigma$ sta neznana. \\
    I"s"cemo interval zaupanja za $\mu$. \\
    Slu"cajna spremenljivka $T := \frac{\overline{X} - \mu}{\sigma} \sqrt{n} \sim Student(n-1)$ \\
    Pri danem tveganju $\alpha$ izberemo $t_{\frac{\alpha}{2}} > 0$, da je $P(|T| < t_{\frac{\alpha}{2}}) =
    1 - \alpha$ oz. $P(T > t_{\frac{\alpha}{2}}) = \frac{\alpha}{2}$ \\
    Sedaj imamo podobno situacijo kot v primeru 1. \\
    Pogoj $|T| < t_{\frac{\alpha}{2}}$ pomeni
    \begin{equation*}
        A := \overline{X} - t_{\frac{\alpha}{2}} \frac{\sigma}{\sqrt{n}} < \mu <
        \overline{X} + t_{\frac{\alpha}{2}} \frac{\sigma}{\sqrt{n}} =: B
    \end{equation*}
    Konkreten zgled: "zivljenska doba "zarnic v vzorcu je $9.8, 10.2, 10.4, 9.8, 10.0, 10.2, 9.6$ (v dneh),
    $n=7$. Predpostavimo normalni model $N(\mu, \sigma)$ z neznanima parametroma $\mu$ in $\sigma$
    \begin{align*}
        &\overline{x} = \frac{1}{n} \sum_{i=1}^n x_i = 10.0 \\
        &s := \sqrt{\frac{1}{n-1} \sum_{i=1}^n (x_i - \overline{x})^2} = 0.283
    \end{align*}
    Vzemimo $\alpha = 0.05$, iz tabele za $Student(5)$ razberemo $t_{\frac{\alpha}{2}} = 2.45$ \\
    Vzor"cna vrednost za $A$ je $a = \overline{x} - t_{\frac{\alpha}{2}} \cdot \frac{s}{\sqrt{n}} = 9.74$ \\
    Vzor"cna vrednost za $B$ je $b = \overline{x} + t_{\frac{\alpha}{2}} \cdot \frac{s}{\sqrt{n}} = 10.26$ \\
    Interval zaupanja za $\mu$ je $[9.74, 10.26]$, kar zapi"semo kot $\mu = 10.0 \pm 0.26$, Verjetnost, da
    je $\mu \in [9.74, 10.26]$ je 0.95
\end{ex}



% 23. predavanje: 11.4.

\begin{ex}
    Pri normalni porazdelitvi $N(\mu, \sigma)$ ocenjujemo parameter $\sigma$. Vzor"cna statistika

    \begin{equation*}
        \chi^2 := \frac{1}{\sigma^2} \sum_{i=1}^{n} (x_i - \overline{x})^2 =
        \frac{n-1}{\sigma^2} \sum_{i=1}^{n} (x_i - \overline{x})^2S 
    \end{equation*}

    je porazdeljena po $\chi^2(n-1)$ \\
    Izberimo $c_1$ in $c_2$ da je

    \begin{equation*}
        P(\chi^2 < c_1) = \frac{\alpha}{2} = P(\chi^2 > c_2)
    \end{equation*}

    oz.

    \begin{equation*}
        P(c_1 < \chi^2 < c_2) = 1 - \alpha
    \end{equation*}

    Pogoj $c_1 < \chi^2 < c_2$ pomeni

    \begin{align*}
        &c_1 < \frac{n-1}{\sigma^2} s^2 < c_2 \iff \\
        &\iff \frac{1}{c_1} > \frac{\sigma^2}{(n-1)s^2} > \frac{1}{c_2} \iff \\
        &\iff B := \frac{(n-1)s^2}{c_1} > \sigma^2 > \frac{(n-1)s^2}{c_1} =: A 
    \end{align*}

    $[A,B]$ je interval zaupanja za $\sigma^2$ pri stopnji tveganja $\alpha$

    \begin{align*}
        &A = \frac{1}{c_2} \sum_{i=1}^{n} (x_i - \overline{x})^2, \\
        &B = \frac{1}{c_1} \sum_{i=1}^{n} (x_i - \overline{x})^2
    \end{align*}

    \begin{ex}
        "Zarnice iz prej"snjega primera: \\
        $n=7, (n-1)s^2 = \sum_{i=1}^{n} (x_i - \overline{x})^2 = 0.481, \alpha = 0.05$9

        \begin{align*}
            &\chi^2(6): c_1 = 1.24, c_2 = 14.45 \\
            &a = \frac{1}{14.45} 0.481 = 0.033, b = \frac{1}{1.24} 0.481 = 0.388 \\
            &\implies P(0.033 < \sigma^2 < 0.388) = 0.95 \\
            &P(0.182 < \sigma < 0.623) = 0.95
        \end{align*}

        $[0.182, 0.623]$ je interval zaupanja za $\sigma$ pri stopnji tveganja $0.05$
    \end{ex}
\end{ex}

\begin{ex}
    $X: \begin{pmatrix}
        0 & 1 \\
        q & p
    \end{pmatrix}, q = 1-p, p$ neznan parameter, ki ga ocenjujemo \\
    $(x_1 \cdots x_n)$ vzorec. Potem je $S_n = X_1 + \cdots + X_n \sim Bin(n,p)$ in $\overline{X} = \frac{S_n}{n}$
    je nepristranska in dosledna cenilka za $p$. Po CLI (Laplaceovi formuli) je pri velikih n
    $Z := \frac{S_n - np}{\sqrt{npq}} \sim N(0,1)$ oz. $Z = \frac{\overline{X} - p}{\sqrt{pq}} \sqrt{n} \sim N(0,1)$
    oz. $\overline{X} \sim N(p, \sqrt{\frac{pq}{n}})$ \\
    Pri danem $\alpha > 0$ izberimo $z_{\frac{\alpha}{2}} > 0$, da je $P(|Z| < z_{\frac{\alpha}{2}}) = 1 - \alpha$ \\P
    Pogoj $|Z| < z_{\frac{\alpha}{2}}$ pomeni $|S - np| < z_{\frac{\alpha}{2}} \sqrt{npq}$ oz.
    $|\overline{X} - p| < z_{\frac{\alpha}{2}} \cdot \sqrt{\frac{pq}{n}}$ \\
    "Ce na desni strani naredimo aproksimacijo $\overline{X} \approx p$, dobimo pogoj

    \begin{equation*}
        |\overline{X} - p| < z_{\frac{\alpha}{2}} \sqrt{\frac{\overline{X} (1-\overline{X})}{n}}
    \end{equation*}

    od koder dobimo interval zaupanja za $p$:

    \begin{align*}
        &A := \overline{X} - z_{\frac{\alpha}{2}} \sqrt{\frac{\overline{X} (1-\overline{X})}{n}} \\
        &B := \overline{X} + z_{\frac{\alpha}{2}} \sqrt{\frac{\overline{X} (1-\overline{X})}{n}} \\
        &A < p < B
    \end{align*}

    \begin{ex}
        Predsedni"ske volitve v ZDA leta 2000: \\
        Anketa na $2207$ volivcev: $n=2207, \alpha = 0.05 \implies z_{\frac{\alpha}{2}} = 1.96$ \\
        George Bush: $47\%$, Algore: $44\%$, Ralph Nader: $2\%$ \\
        Dolo"cimo intervale zaupanja

        \begin{equation*}
            p_{Bush} = 0.47 \pm 1.96 \sqrt{\frac{0.47 (1-0.47)}{2207}} \doteq 0.47 \pm 0.02
        \end{equation*}

        Interval zaupanja za $p_{Bush}$ je $[0.45, 0.49]$

        \begin{equation*}
            p_{Algore} = 0.44 \pm 1.96 \sqrt{\frac{0.44 \cdot 0.56}{2207}} \doteq 0.44 \pm 0.02
        \end{equation*}

        Interval zaupanja za $p_{Algore}$ je $[0.42, 0.46]$

        \begin{equation*}
            p_{Nader} = 0.02 \pm 1.96 \sqrt{\frac{0.02 \cdot 0.98}{2207}} \doteq 0.02 \pm 0.006
        \end{equation*}

        Odstopanje:

        \begin{align*}
            &z_{\frac{\alpha}{2}} \sqrt{\frac{\overline{x} (1-\overline{x})}{n}} < 
                2 \sqrt{\frac{\frac{1}{4}}{n}} = \frac{1}{\sqrt{n}} \\
            &x(1-x) \leq \frac{1}{4} \iff x-x^2 \leq \frac{1}{4} \iff \\
            &\iff 0 \leq x^2 - x + \frac{1}{4} = (x-\frac{1}{2})^2
        \end{align*}
    \end{ex}
\end{ex}

\subsection{Preizku"sanje statisti"cnih hipotez}

\begin{defn}[Statisti"cna hipoteza]
    Statisti"cna hipoteza je vsaka domneva o porazdelitvi slu"cajne spremenljivke $X$ na populaciji
\end{defn}

\begin{defn}[Enostavnost hipoteze]
    Hipoteza je enostavna, "ce natanko dolo"ca porazdelitev, sicer je sestavljena
\end{defn}

\begin{ex}
    $X \sim N(\mu, \sigma), \sigma$ poznamo, $\mu$ je neznan parameter \\
    $H(\mu = 0)$ je primer enostavne hipoteze. "Ce $\sigma$ ne poznamo, je to sestavljena hipoteza
\end{ex}

Vedno preizku"samo eno ni"celno hipotezo $H_0$ nasproti alternativni hipotezi $H_1$

\begin{ex}
    $X \sim N(\mu, \sigma), \sigma$ poznamo \\
    $H_0(\mu = 0): H_1(\mu \neq 0)$
\end{ex}

Za $H_0$ obi"cajno vzamemo enostavno hipotezo, za katero upamo, da jo bomo zavrnili \\
Hipoteza je lahko pravilna ali nepravilna. Ideal je sprejeti pravilno in zavrniti nepravilno. Odlo"citi se
moramo na osnovi vzorca. "Ce vzor"cni podatki preve"c odstopajo od hipoteze, potem niso konsistentni z njo oz.
so razlike zna"cilne (signifikantne); tedaj hipotezo zavrnemo \\
Vnaprej dolo"cimo stopnjo zna"cilnosti $\alpha \in [0,1]$, to je verjetnost, da zavrnemo pravilo hipotezo.
Obi"cajno je $\alpha = 0.05$ ali $\alpha = 0.01$. Take teste imenujemo testi zna"cilnosti

Primeri testov znacilnosti

\subsubsection{test $Z$}

$X \sim N(\mu, \sigma), \sigma$ znan parameter \\
Ni"celna domneva je $H_=(\mu = \mu_0)$, kjer je $\mu_0$ damo realno "stevilo \\
Pri predpostavki $H_0(\mu = \mu_0)$ je $Z := \frac{\overline{X} - \mu}{\sigma} \sqrt{n}$ porazdeljena
$N(0,1)$, saj je $\overline{X} \sim N(\mu_0, \frac{\sigma}{\sqrt{n}})$ \\
Vzemimo $H_1(\mu \neq \mu_0)$. Tedaj $H_0$ zavrnemo, "ce vzor"cna vrednost za $Z$ le"zi na kriticnem obmocju

\begin{equation*}
    K_{\alpha} = (-\infty, -z_{\frac{\alpha}{2}}] \cup [z_{\frac{\alpha}{2}}, \infty)
\end{equation*}

kjer je $\alpha$ stopnja zna"cilnosti in $P(Z > z_{\frac{\alpha}{2}}) = \frac{\alpha}{2}$



% 24. predavanje: 18.4.

$Z \dots$ testna statistika \\
Pri stopnji zna"cilnosti $\alpha$ dolo"cimo $z_{\frac{\alpha}{2}} > 0$, da je

\begin{equation*}
    P(Z > z_{\frac{\alpha}{2}}) = \frac{\alpha}{2}
\end{equation*}

$K_{\alpha}$ kriti"cno obmo"cje \\
"Ce je vzor"cna vrednost za $Z$ na $K_{\alpha}$, hipotezo $H_0$ zavrnemo

\begin{ex}
    Izdelovalec vrvic trdi, da je povpre"cna sila, pri kateri se vrvica strga 150N s standardno deviacijo
    5N. Na vzorcu 50 vrvic ($n=50$) je bila popvre"cna sila 148N. Privzamemo normalno porazdelitev $N(\mu,5)$.
    Pri stopnji zna"cilnosti $\alpha = 0.01$ testiramo hipotezo

    \begin{equation*}
        H_0(\mu = 150) : H_1(\mu \neq 150)
    \end{equation*}

    Iz tabel razberemo $z_{\frac{\alpha}{2}} = 2.58$ \\
    Kriti"cno obmo"cje $K_{\alpha} = (-\infty, -2.58] \cup [2.58, \infty)$ \\
    Testna statistika $Z = \frac{\overline{X}-150}{5} \sqrt{50} = (\overline{X}-150) \sqrt{2}$, njena vzor"cna
    vrednost je

    \begin{equation*}
        z = (148 - 150) \sqrt{2} = -2 \sqrt{2} = -2.82
    \end{equation*}

    Ker je $z = -2.82 \in K_{\alpha}, H_0$ zavrnemo; razlike so zna"cilne (signifikantne) \\
    Ker gledamo odstopanja v obe smeri, je to dvostranski test $Z$. \\
    Smiselen bi bil enostranski test $Z$: $H_0(\mu = 150) : H_1(\mu < 150)$ \\
    $K_{\alpha} = (-\infty, -z_{\alpha})$, pri $\alpha = 0.01$ je $z_{\alpha} = 2.33$ \\
    Ker $z = -2.82 \in K_{\alpha}, H_0$ zavrnemo tudi sedaj
\end{ex}

\subsubsection{test T: $X \sim N(\mu, \sigma)$, $\mu$ neznan}

$H_0(\mu = \mu_0) : H_1(\mu \neq \mu_0), \mu_0$ je dano "stevilo \\
Testna statistika

\begin{equation*}
    T = \frac{\overline{X} - \mu_0}{S} \sqrt{n}
\end{equation*}

$S$ je vzor"cna deviacija \\
Pri predpostavki $H_0$ je porazdeljena po Student(n-1) \\
$K_{\alpha} = (-\infty, -t_{\frac{\alpha}{2}}] \cup [t_{\frac{\alpha}{2}}, \infty)$ \\
"Ce vzor"cna vrednost za $T$ le"zi na $K_{\alpha}$, hipotezo zavrnemo

\begin{ex}
    Nadaljevanje prej"snjega primera \\
    Privzamemo deviacijo 5N izra"cunano iz vzorca, torej

    \begin{equation*}
        S = 5N, s^2 = \frac{1}{n-1} \sum_{i=1}^{n} (x_i - \overline{x})^2
    \end{equation*}

    Kot prej je $\overline{x} = 148N$ \\
    Iz tabel razberemo, da je $t_{\frac{\alpha}{2}} = 2.68$ (pri 49 prostorskih stopnjah in $\alpha = 0.01$) \\
    $K_{\alpha} = (-\infty, -2.68] \cup [2.68, \infty)$, vzor"cna vrednost za $T$ je (tako kot prej) $t = -2.82$ \\
    Ker $t \in K_{\alpha}, H_0$ zavrnemo
\end{ex}

\begin{ex}
    V klini"cnem poskusu testirajo zdravila za zni"zevanje krvnega tlaka so 10 bolnikom izmerili sistoli"cno krvni
    tlak pred in po zdravljenju. Razlike \"pred-po\" so $-8,0,2,4,9,14,19,22,32,35 mmHg$. Predpostavimo normalni model
    $N(\mu, \sigma)$ z neznanima $\mu$ in $\sigma$. Testiramo hipotezo $H_0(\mu = 0) : H_1(\mu > 0)$. Naredimo
    test zna"cilnosti (parametri"cni test zna"cilnosti)
    
    \begin{enumerate}
        \item enostranski test $T$ $H_0(\mu = 0) : H_1(\mu > 0), H_0 :=$ zdravilo ne u"cinkuje
        \item stopnja zna"cilnosti $\alpha = 0.05$
        \item testna statistika $T = \frac{\overline{X}}{S} \sqrt{10}$, $n=10$
        \item kriti"cno obmo"cje $K_{\alpha} = [t_{\alpha}, \infty)$ \\
            $Student(9) \implies t_{\alpha} = 1.83$
        \item vzor"cna vrednost za $T$ je
            \begin{equation*}
                t = \frac{12.9}{14.1} \sqrt{10} = 2.89
            \end{equation*}
    \end{enumerate}

    Sklep: ker $t = 2.89 \in K_{\alpha}$, hipotezo $H_0$ zavrnemo
\end{ex}

\begin{defn}[P-vrednost]
    P-vrednost je najmanj"sa stopnja zna"cilnosti, pri kateri "se lahko zavrnemo hipotezo (pro danih vzor"cnih
    podatkih)
\end{defn}

V na"sem primeru je $P = 0.89\% = 0.0089$

\subsubsection{Studentov primerjalni test}

Imejmo 2 neodvisna vzorca velikosti $m$ in $n$. Prvi je vzet iz populacije, na kateri ime slu"cajna spremenljivka
$X \sim N(\mu_x, \sigma)$, druga pa iz populacije, na kateri imamo $Y \sim N(\mu_x, \sigma)$. Predpostavljamo torej
enakost disperzij. "Ce sta $s_x^2$ in $s_y^2$ povpre"cni vzor"cni disperziji

\begin{align*}
    &S_X^2 = \frac{1}{m-1} \sum_{i=1}^{m} (x_i - \overline{x})^2 \\
    &S_Y^2 = \frac{1}{n-1} \sum_{i=1}^{n} (y_i - \overline{y})^2
\end{align*}

potem definiramo skupno vzor"cno varianco

\begin{equation*}
    S^2 = \frac{(m-1) S_X^2 + (n-1) S_Y^2}{m+n-2} = \frac{1}{m+n-1} (\sum_{i=1}^{m}
    (x_i - \overline{x})^2 + \sum_{i=1}^{n} (y_i - \overline{y})^2)
\end{equation*}

Testiramo hipotezo $H_0(\mu_x = \mu_y) : H_1(\mu_x \neq \mu_y)$. Testna statistika

\begin{equation*}
    T = \frac{\overline{X} - \overline{Y}}{S} \sqrt{\frac{mn}{m+n}}
\end{equation*}

Potem je

\begin{align*}
    &\overline{X} - \overline{Y} \sim N(0, \sqrt{(\frac{\sigma}{\sqrt{m}})^2 + (\frac{\sigma}{\sqrt{n}})^2}) = \\
    &= N(0, \sigma \sqrt{\frac{1}{m} + \frac{1}{n}}) = N(0, \sigma \sqrt{\frac{m+n}{mn}})
\end{align*}

Zato ima spremenljivka

\begin{equation*}
    Z = \frac{\overline{X} - \overline{Y}}{S} \sqrt{\frac{mn}{m+n}} \sim N(0,1)
\end{equation*}

Ker je spremenljivka

\begin{equation*}
    U = \frac{(m+n-2) S^2}{\sigma^2} \sim \chi^2(m+n-2)
\end{equation*}

je

\begin{equation*}
    T = \frac{Z}{\sqrt{\frac{U}{m+n-2}}} \sim Student(m+n-2)
\end{equation*}



% 25. predavanje: 25.4.

\begin{ex}
    2 zdravili proti nespe"cnosti preizkusijo na vzorcih velikosti $m=n=10$. \\
    Dodatno "stevilo ur pri prvem zdravilu: $1.9, 0.8, 1.1, 0.1, -0.1, 4.4, 5.5, 1.6, 4.6, 3.4$ \\
    Dodatno "stevilo ur pri drugem zdravilu pa $0.7, -1.6, -0.2, -1.2, -0.1, 3.4, 3.7, 0.8, 0.0, 2.0$

    \begin{enumerate}
        \item dvostranski standardni primerjalni test $H_0(\mu_x = \mu_y) : H_1(\mu_x \neq \mu_y)$
        \item stopnja zna"cilnosti $\alpha = 0.05$
        \item testna statistika $T = \frac{\overline{X} - \overline{Y}}{S} \sqrt{\frac{mn}{m+n}} \sim Student(m+n-2)
            = Student(18)$, "ce velja hipoteza $H_0$
        \item kriti"cno obmo"cje $K_{\alpha} = (-\infty, -t_{\frac{\alpha}{2}}] \cup [t_{\frac{\alpha}{2}}, \infty),
            t_{\frac{\alpha}{2}} = 2.10$ (iz tabele, 18 prostorskih stopenj)
        \item vzor"cna vrednost za $T$ je 
            \begin{equation*}
                t = \frac{\overline{x} - \overline{y}}{s} \sqrt{5} = \frac{2.33 - 0.75}{\sqrt{3.70}} \sqrt{5} = 1.84
            \end{equation*}
    \end{enumerate}

    Sklep: ker $1.84 \notin K_{\alpha}$, hipoteze $H_0$ ne moremo zavrniti pri stopnji zna"cilnosti $5\%$ \\
    $P$-vrednost pride 0.079
\end{ex}

Poleg populacijskega povpre"cja $\mu$ lahko testiramo tudi druge koli"cine:

\begin{itemize}
    \item standardna deviacija $\sigma: H_0(\sigma = \sigma_0), \sigma_0$ je dano "stevilo
    \item tip porazdelitvenega zakona: $H_0(F = F_0)$
    \item neodvisnost dveh spremenljivk
    \item korelacijski koeficient
\end{itemize}

\subsubsection{Test hi-kvadrat}

(Pearson) \\
Preizkus domneve o tipu porazdelitvenega zakona, torej $H_0(F = F_0) : H_1(F \neq F_0)$, kjer je $F_0$ dana
porazdelitvena funkcija. Zalogo vrednosti slu"cajne spremenljivke $X$ razdelimo na $r$ razredov (disjunktno)
$S_1, S_2 \cdots S_r$, da je

\begin{equation*}
    p_k = P(X \in S_k \mid H_k) > 0 \; \forall k = 1, 2 \cdots r
\end{equation*}

Potem je $\sum_{k=1}^{r} p_k = 1, \sum_{k=1}^{r} N_k = n$ ter $N_k \sim Bin(n, p_k)$ in $E(N_k) = p \cdot n_k$
kar je pri"cakovana vrednost za $k$-ti razred. \\
Pri velikem $n$ ima testna statistika $\chi^2 = \sum_{k=1}^{r} \frac{(N_k - n p_k)^2}{n p_k}$ pribli"zno
porazdelitev $\chi^2(r-1)$ \\
"Ce $\chi^2$ zavzame preveliko vrednost, hipotezo $H_0$ zavrnemo \\
$K_{\alpha} = [c_{\alpha}, \infty), P(\chi^2 > c_{\alpha}) = \alpha$

\begin{ex}
    Preizku"samo \"po"stenost\" kocke \\
    v $n=120$ dobimo za "stevilo pik

    \begin{center}
        \begin{tabular}{ c c c c c c c c}
            "stevilo pik & 1 & 2 & 3 & 4 & 5 & 6 & \\
            opazovane frekvence & 20 & 22 & 17 & 18 & 19 & 24 & $N_k$ \\
            pri"cakovane frekvence & 20 & 20 & 20 & 20 & 20 & 20 & $n p_k$
            \end{tabular}
    \end{center}

    $r = 6, p_k = \frac{1}{6}$

    \begin{equation*}
        \chi^2 = \frac{(20-20)^2}{20} + \frac{(22-20)^2}{20} + \cdots + \frac{(24-20)^2}{20} = \frac{34}{20} = 1.7
    \end{equation*}

    Pri $\alpha = 0.05$ in 5 prostorskih stopnjah je $c_{\alpha} = 11.1$ \\
    Ker $1.7 \notin K_{\alpha}$, hipoetzo $H_0$, da gre za enakomerno porazdelitev na 6 to"ckah ne moremo zavrniti
\end{ex}

\begin{ex}
    Podatki o "stevilo mo"cnih potresov (vsaj 8 stopnje po Richterjevi lestvici) v obdobju 1969-2001, $n=33$

    \begin{center}
        \begin{tabular}{c c c c c c c c}
            "stevilo potresov & 0 & 1 & 2 & 3 & 4 & 5 & $\cdots$ \\
            "stevilo let s toliko potresiv & 15 & 13 & 4 & 1 & 0 & 0 & $\cdots$
        \end{tabular}
    \end{center}

    Hipoteza $H_0:$ podatki so porazdeljeni po $Poisson(1.5): p_k = e^{-1.5} \frac{(1.5)^k}{k!} k = 0, 1, 2 \cdots$ \\
    Pri"cakovane frekvence so

    \begin{center}
        \begin{tabular}{c c c c c c c}
            "stevilo potresov & 0 & 1 & 2 & 3 & 4 & $\cdots$ \\
            "stevilo let s toliko potresiv & 7.4 & 11.0 & 8.3 & 4.1 & 1.6 & $\cdots$
        \end{tabular}
    \end{center}

    Vsota za od (vklju"cno z) 3 naprej je 6.3 \\
    Teorija priporo"ca, da so pri"cakovane frekvence vsaj 5, zato vpeljemo razred ``$\geq 3$"

    \begin{center}
        \begin{tabular}{c c c c c}
            razred & 0 & 1 & 2 & 3 \\
            opazovane frekvence & 15 & 13 & 4 & 1 \\
            pri"cakovane frekvence & 7.4 & 11.0 & 8.3 & 6.3
        \end{tabular}
    \end{center}

    $r=4$

    \begin{equation*}
        \chi^2 = \frac{(15-7.4)^2}{7.4} + \frac{(13-11)^2}{11} + \frac{(4-8.3)^2}{8.3} + \frac{(1-6.3)^2}{6.3} = 14.9
    \end{equation*}

    Pri $\chi^2(3)$ je $c_{\alpha} = 7.82$ \\
    Ker $14.9 \in K_{\alpha} = [7.82, \infty)$, hipotezo $H_0$ zavrnemo \\
    $P$-vrednost je 0.002
\end{ex}

\begin{rem}
    "Ce so v testu $\chi^2$ frekvence $p_k$ odvedljivo odvisne od parametra $\theta$, torej $p_k(\theta)$, potem ima
    statistika $\chi^2 = \sum_{k=1}^{r} \frac{(N_k - n p_k(\hat{\theta}))^2}{n p_k(\hat{\theta})}$ pribli"zno
    porazdelitev $\chi^2(r-2)$, kjer je $\hat{\theta}$ cenilka za parameter $\theta$ po metodi maksimalne zanesljivosti
\end{rem}

\begin{ex}
    Potresi (od prej), $H_0$: podatki imajo Poissonovo porazdelitev \\
    Cenilka za $\lambda$ je 
    
    \begin{equation*}
        \hat{\lambda} = \overline{X} = \frac{0 \cdot 15 + 1 \cdot 13 + 2 \cdot 4 + 3 \cdot 1}{33} = \frac{8}{11} = 0.73
    \end{equation*}

    (ista cenilka tako po metodi momentov kot po moetodi najve"cjega verjetja) \\
    Torej

    \begin{equation*}
        p_k(\hat{\lambda}) = e^{-0.73} \frac{(0.73)^k}{k!} \; k = 0, 1 \cdots
    \end{equation*}

    \begin{center}
        \begin{tabular}{c c c c c}
            razred & 0 & 1 & $\geq 2$ & k \\
            opa"zene frekvence & 15 & 13 & 5 & $N_k$ \\
            pri"cakovane frekvence & 15.9 & 11.6 & 5.5 & $n \cdot p_k(\hat{\lambda})$
        \end{tabular}
    \end{center}

    $r=3$

    \begin{equation*}
        \chi^2 = \frac{(15-15.9)^2}{15.9} + \frac{(13-11.6)^2}{11.6} + \frac{(5-5.5)^2}{5.5} = 0.27
    \end{equation*}

    Za $\chi^2(r-2) = \chi^2(1)$ in $\alpha = 0.05$ je $c_{\alpha} = 3.84, K_{\alpha} = [3.84, \infty)$ \\
    Ker $0.27 \notin K_{\alpha}$, hipoteze $H_0$ ne moremo zavrniti
\end{ex}

\begin{rem}
    Ra"cuni bi bili druga"cni "ce bi imeli $\lambda = 0.73$ podan na za"cetku \\
    ($\chi^2(r-1)$ vs. $\chi^2(r-2)$)
\end{rem}


\end{document}